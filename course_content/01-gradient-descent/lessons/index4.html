<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Introducing Stochastic Gradient Descent (SGD)</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          max-width: 800px;
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          font-size: 150%;
      }
      section {
          margin-bottom: 20px;
          padding: 20px;
          background-color: #ffffff;
          display: none;
          opacity: 0;
          transition: opacity 0.5s ease-in;
      }
      h1, h2, h3, h4 {
          color: #333;
          margin-top: 20px;
      }
      p, li {
          line-height: 1.6;
          color: #444;
          margin-bottom: 20px;
      }
      ul, ol {
          padding-left: 20px;
      }
      .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think, .visual-aid-placeholder {
          text-align: left;
      }
      .image-placeholder img, .interactive-placeholder img, .visual-aid-placeholder img {
          max-width: 100%;
          height: auto;
          border-radius: 5px;
      }
      .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          padding: 20px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .vocab-section {
          background-color: #f0f8ff;
      }
      .vocab-section h3 {
          color: #1e90ff;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .vocab-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .vocab-term {
          font-weight: bold;
          color: #1e90ff;
      }
      .why-it-matters {
          background-color: #ffe6f0;
      }
      .why-it-matters h3 {
          color: #d81b60;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .stop-and-think {
          background-color: #e6e6ff;
      }
      .stop-and-think h3 {
          color: #4b0082;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .continue-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #007bff;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .reveal-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #4b0082;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .test-your-knowledge {
          background-color: #e6ffe6; /* Light green background */
      }
      .test-your-knowledge h3 {
          color: #28a745; /* Dark green heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .test-your-knowledge h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .test-your-knowledge p {
          margin-bottom: 15px;
      }
      .check-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #28a745; /* Green background */
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
          border: none;
          font-size: 1em;
      }
      .option {
          margin-bottom: 10px;
          padding: 10px;
          border: 1px solid #ddd;
          border-radius: 5px;
          cursor: pointer;
      }
      .option:hover {
          background-color: #f5f5f5;
      }
      .option.selected {
          background-color: #e6ffe6;
          border-color: #28a745;
      }
      .option-explanation {
          display: none;
          margin-top: 10px;
          padding: 10px;
          background-color: #f9f9f9;
          border-radius: 5px;
      }
      .visual-aid-placeholder {
          background-color: #f9f9f9;
          padding: 15px;
          border-radius: 5px;
          margin: 15px 0;
      }
      .visual-aid-placeholder .caption {
          font-style: italic;
          font-size: 0.9em;
          color: #666;
          margin-top: 10px;
      }
      .interactive-placeholder {
          background-color: #f0f8ff;
          padding: 15px;
          border-radius: 5px;
          margin: 15px 0;
      }
      .interactive-placeholder .caption {
          font-style: italic;
          font-size: 0.9em;
          color: #666;
          margin-top: 10px;
      }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <section id="section1">
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A visual metaphor: A large, heavy truck labeled 'Batch GD' slowly climbing a hill, contrasted with a nimble dirt bike labeled 'SGD' quickly zipping up a similar hill, perhaps taking a slightly bumpier path.">
      </div>
      <h1>Introducing Stochastic Gradient Descent (SGD)</h1>
      <p>Welcome back! We've dived deep into what we call <strong>Batch Gradient Descent</strong>. The 'Batch' part means that to calculate the gradient (the direction to step) in <em>each iteration</em>, we use our <em>entire</em> training dataset. Think about the summation term $$\sum_{i=1}^{n}$$ in our gradient formulas – that sum goes over all $$n$$ data points.</p>
      <div class="continue-button" onclick="showNextSection(2)">Continue</div>
  </section>

  <section id="section2">
      <h2>The Bottleneck of 'Batch' GD</h2>
      <p>Using the whole dataset for each gradient calculation makes each step very accurate – we're getting the 'true' gradient based on all available information. However, what happens when $$n$$ (our number of training examples) is enormous? Millions, or even billions, of data points?</p>
      <p>Calculating that sum over every single data point for <em>every single step</em> of Gradient Descent can become incredibly computationally expensive and very, very slow. Each step might take a long time, even if we're making good progress with each step.</p>
      <div class="visual-aid-placeholder">
          <img src="/placeholder.svg?height=200&width=600" alt="An animation showing a CPU meter. When 'Batch GD Step' is active, the meter spikes to 100% and stays there for a noticeable duration. Then it drops, and the process repeats slowly for the next step.">
          <div class="caption">Each step of Batch GD can be computationally intensive with large datasets.</div>
      </div>
      <div class="continue-button" onclick="showNextSection(3)">Continue</div>
  </section>

  <section id="section3">
      <p>Imagine trying to figure out the average opinion of an entire country by meticulously polling every single citizen <em>before</em> you make any small adjustment to your political campaign strategy. It would be accurate, but painfully slow!</p>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=250&width=600" alt="The professor-like character from Slide 7 (or a similar cartoon figure) looking thoughtful and writing on a chalkboard: 'Batch GD: Accurate but SLOW on Big Data. Is there a faster way?'">
          <div class="caption">Seeking a more nimble approach for large datasets.</div>
      </div>
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>As datasets continue to grow, the scalability of our learning algorithms becomes paramount. Batch Gradient Descent's per-iteration cost can be a significant bottleneck, motivating the need for more efficient alternatives.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(4)">Continue</div>
  </section>

  <section id="section4">
      <h2>The 'Stochastic' Idea: One Sample at a Time</h2>
      <p>This is where <strong>Stochastic Gradient Descent (SGD)</strong> comes to the rescue! The word 'stochastic' essentially means involving randomness or probability. Instead of using all $$n$$ data points to calculate the gradient for an update, SGD does something radical: <strong>it estimates the gradient and updates the parameters using just <em>one</em> randomly chosen training example at a time!</strong></p>
      <p>Let's compare the conceptual update for a parameter $$\beta_j$$ (or $$\theta_j$$):</p>
      <p><strong>Batch GD Update (using all $$n$$ samples):</strong></p>
      <p>\[ \beta_j := \beta_j - \alpha \underbrace{\left( \frac{1}{n} \sum_{i=1}^{n} \text{gradient\_contribution\_from\_sample_i} \right)}_{\text{Average gradient over all samples}} \]</p>
      <p><strong>SGD Update (using one random sample $$k$$):</strong> 
      <br>First, shuffle your dataset. Then, for each sample $$k$$ in your shuffled dataset (one by one):</p>
      <p>\[ \beta_j := \beta_j - \alpha \underbrace{\left( \text{gradient\_contribution\_from\_just\_sample_k} \right)}_{\text{Gradient from a single sample}} \]</p>
      <p>So, for each step in SGD, we pick one data point, calculate the gradient <em>as if that one point were our entire dataset</em>, and immediately update our parameters. Then we pick the next point, and so on.</p>
      <div class="vocab-section">
          <h3>Build Your Vocab</h3>
          <h4 class="vocab-term">Stochastic</h4>
          <p>In this context, 'stochastic' means that each step of the algorithm involves an element of randomness (due to picking one random sample, or a random mini-batch of samples) to approximate the true gradient.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(5)">Continue</div>
  </section>

  <section id="section5">
      <p>Think about the implications:</p>
      <ul>
          <li><strong>Pros:</strong>
              <ul>
                  <li><strong>Speed per update:</strong> Each parameter update is incredibly fast to compute because it only involves one data point!</li>
                  <li><strong>Faster initial progress:</strong> You can make many, many more updates in the same amount of time it would take Batch GD to make one. This often leads to faster convergence towards a good solution, especially in the early stages of training and on very large (redundant) datasets.</li>
                  <li><strong>Escaping shallow local minima:</strong> The 'noisy' updates (since each step is based on a rough gradient estimate) can sometimes help the algorithm 'bounce out' of shallow local minima or navigate saddle points more effectively than the smooth path of Batch GD.</li>
              </ul>
          </li>
      </ul>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=200&width=600" alt="A cartoon of a GD algorithm character easily jumping out of a small dip in the cost landscape, helped by a 'noisy step' arrow.">
      </div>
      <div class="continue-button" onclick="showNextSection(6)">Continue</div>
  </section>

  <section id="section6">
      <ul>
          <li><strong>Cons:</strong>
              <ul>
                  <li><strong>Noisy path:</strong> Because each update is based on a very rough estimate of the true gradient, the path SGD takes towards the minimum will be much 'noisier' or 'zig-zaggy' compared to the smoother path of Batch GD.</li>
                  <li><strong>Oscillation near minimum:</strong> SGD typically doesn't converge precisely <em>to</em> the minimum. Instead, it tends to oscillate or bounce around in the vicinity of the minimum. To mitigate this, the learning rate $$\alpha$$ is often gradually decreased over time.</li>
              </ul>
          </li>
      </ul>
      <div class="visual-aid-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="Two side-by-side contour plots of a simple 2D cost function (like an oval bowl). Left: 'Batch GD Path' - A smooth, direct red line starting from an initial point and moving straight to the center (minimum). Right: 'SGD Path' - A blue line starting from the same initial point but taking a much more erratic, zig-zagging path, eventually getting close to the center but then oscillating around it.">
          <div class="caption">Batch GD takes a smooth path; SGD takes a noisy, faster (per update) path.</div>
      </div>
      <div class="stop-and-think">
          <h3>Stop and Think</h3>
          <h4>If SGD uses only one sample per update, how many updates does it make to go through the entire dataset once (one 'epoch') compared to Batch GD?</h4>
          <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
          <p id="stop-and-think-1" style="display: none;">SGD makes 'n' updates (where n is the number of samples) in one epoch. Batch GD makes only 1 update in one epoch (as it processes all 'n' samples for that single update).</p>
      </div>
      <div class="continue-button" onclick="showNextSection(7)">Continue</div>
  </section>

  <section id="section7">
      <h2>Mini-Batch GD: The Best of Both Worlds?</h2>
      <p>Pure SGD (one sample at a time) can be quite noisy. Batch GD (all samples at a time) can be quite slow. Is there a happy medium? Yes! It's called <strong>Mini-Batch Gradient Descent</strong>.</p>
      <p>In Mini-Batch GD, instead of using all $$n$$ samples or just 1 sample, you use a small 'mini-batch' of samples (e.g., 32, 64, 128, 256 samples – typically powers of 2 for hardware efficiency) to estimate the gradient at each step.</p>
      <p>\[ \text{Mini-Batch GD Update (using a mini-batch of } m_b \text{ samples):} \]</p>
      <p>\[ \beta_j := \beta_j - \alpha \underbrace{\left( \frac{1}{m_b} \sum_{k=1}^{m_b} \text{gradient\_contribution\_from\_sample_k\_in\_batch} \right)}_{\text{Average gradient over the mini-batch}} \]</p>
      <p>First, you'd shuffle your entire dataset. Then you'd divide it into these mini-batches. For each mini-batch, you calculate the average gradient over the samples in that batch and then update your parameters.</p>
      <div class="visual-aid-placeholder">
          <img src="/placeholder.svg?height=250&width=600" alt="A diagram showing a large dataset being divided into several smaller chunks labeled 'Mini-Batch 1', 'Mini-Batch 2', etc. An arrow shows GD processing these mini-batches one by one.">
          <div class="caption">Mini-Batch GD processes the data in small, manageable chunks.</div>
      </div>
      <div class="continue-button" onclick="showNextSection(8)">Continue</div>
  </section>

  <section id="section8">
      <p>Mini-Batch GD offers a good balance:</p>
      <ul>
          <li><strong>Reduces noise:</strong> The gradient estimate is more stable than pure SGD because it's averaged over a small batch.</li>
          <li><strong>Computationally efficient:</strong> Still much faster per update than Batch GD for large datasets.</li>
          <li><strong>Hardware utilization:</strong> Modern hardware like GPUs are optimized for parallel computations on small batches of data, making mini-batch processing very efficient.</li>
      </ul>
      <p>For these reasons, Mini-Batch Gradient Descent is often the <strong>most common approach</strong> used in practice, especially for training deep neural networks.</p>
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>Choosing the right batch size (1 for SGD, 'n' for Batch GD, or a value like 32-256 for Mini-Batch GD) is another important hyperparameter. It affects training speed, stability, and even the quality of the final model.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(9)">Continue</div>
  </section>

  <section id="section9">
      <h2>Exploring with an Interactive Tool</h2>
      <p>The best way to get a feel for how Batch GD, SGD, and Mini-Batch GD behave is to see them in action on the same problem. The <code>deeplearning.ai</code> website has an excellent interactive 'Optimization' explainer tool (as shown on Slide 8 of the course materials) that lets you do just this!</p>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A screenshot or a simplified mock-up of the deeplearning.ai optimization tool interface (from Slide 8), highlighting the 'Select a batch size' option with m_b=1, m_b=24 (example), m_b=m.">
      </div>
      <p>Imagine using such a tool:</p>
      <ol>
          <li><strong>Generate a Dataset:</strong> You'd start by creating some data points for a simple linear regression problem (e.g., predicting $$\hat{y} = wx + b$$).</li>
          <li><strong>View Cost Landscape:</strong> The tool would show you the 'cost hill' – how the cost changes as you vary the parameters $$w$$ (slope) and $$b$$ (intercept). A red dot would show your current parameter values on this landscape.</li>
      </ol>
      <div class="visual-aid-placeholder">
          <img src="/placeholder.svg?height=250&width=600" alt="A simplified 2D contour plot of a cost function (axes: 'b (intercept)' and 'W (slope)'). A red dot indicates the current parameter position. A separate plot shows 'Cost vs. Iteration'.">
          <div class="caption">Visualizing the cost landscape and tracking progress.</div>
      </div>
      <div class="continue-button" onclick="showNextSection(10)">Continue</div>
  </section>

  <section id="section10">
      <ol start="3">
          <li><strong>Optimize with Different Batch Sizes:</strong> This is the fun part! You could select:
              <ul>
                  <li><strong>Learning Rate ($$\alpha$$):</strong> Small, Medium, or Large.</li>
                  <li><strong>Batch Size ($$m_b$$):</strong>
                      <ul>
                          <li>If you set <code>batch size = m</code> (total number of samples), you're running <strong>Batch GD</strong>.</li>
                          <li>If you set <code>batch size = 1</code>, you're running <strong>Stochastic GD (SGD)</strong>.</li>
                          <li>If you set <code>1 < batch size < m</code> (e.g., 16, 32, etc.), you're running <strong>Mini-Batch GD</strong>.</li>
                      </ul>
                  </li>
              </ul>
          </li>
      </ol>
      <div class="continue-button" onclick="showNextSection(11)">Continue</div>
  </section>

  <section id="section11">
      <p>Then you'd hit 'Train Model' and watch how the red dot (your parameters) moves across the cost landscape and how the 'Cost vs. Iteration' plot changes for each method.</p>
      <p><strong>You'd likely observe:</strong></p>
      <ul>
          <li><strong>Batch GD:</strong> Smooth path, fewer updates per 'pass' through the data (epoch), high cost per update.</li>
          <li><strong>SGD:</strong> Very noisy path, many updates per epoch, low cost per update, might oscillate around the minimum.</li>
          <li><strong>Mini-Batch GD:</strong> A path that's less noisy than SGD but makes more progress per epoch than Batch GD.</li>
      </ul>
      <div class="interactive-placeholder">
          <img src="/placeholder.svg?height=350&width=600" alt="A conceptual interactive element for this lesson: A 2D contour plot (cost landscape for parameters w, b). Controls: Slider for 'Learning Rate', Radio buttons for 'Batch Size': ['Full Batch (m)', 'Mini-Batch (e.g., m/10)', 'Stochastic (1)']. A 'Run One Epoch' button.">
          <div class="caption">Experimenting with batch sizes reveals different optimization dynamics.</div>
      </div>
      <p>Tools like this are invaluable for building intuition! I highly recommend checking out the actual tool at <code>https://www.deeplearning.ai/ai-notes/optimization</code>.</p>
      <div class="test-your-knowledge">
          <h3>Test Your Knowledge</h3>
          <h4>If you want the fastest updates per second (assuming a very large dataset) but can tolerate a noisy convergence path, which method would you likely choose?</h4>
          <div class="option" onclick="selectOption(this, 0)">Batch Gradient Descent</div>
          <div class="option-explanation" id="explanation-0">Batch GD has very slow updates per second on large datasets because each update requires processing all data.</div>
          
          <div class="option" onclick="selectOption(this, 1)">Stochastic Gradient Descent (SGD)</div>
          <div class="option-explanation" id="explanation-1">Correct! SGD processes only one sample per update, making each update extremely fast, though the path is noisy.</div>
          
          <div class="option" onclick="selectOption(this, 2)">Mini-Batch Gradient Descent with a very large mini-batch size.</div>
          <div class="option-explanation" id="explanation-2">A very large mini-batch size starts to resemble Batch GD in terms of update speed.</div>
          
          <button class="check-button" onclick="checkAnswer(1)">Check Answer</button>
      </div>
      <div class="continue-button" onclick="showNextSection(12)">Continue</div>
  </section>

  <section id="section12">
      <h2>Wrapping Up Lesson 4</h2>
      <p>So, we've now added Stochastic Gradient Descent (and its popular cousin, Mini-Batch GD) to our optimization toolkit!</p>
      <p><strong>Key Takeaways:</strong></p>
      <ul>
          <li><strong>Batch GD:</strong> Uses all data for each gradient calculation. Accurate steps, but slow on large datasets.</li>
          <li><strong>Stochastic GD (SGD):</strong> Uses one random sample per update. Very fast updates, noisy path, can escape shallow local minima, but may oscillate near the true minimum.</li>
          <li><strong>Mini-Batch GD:</strong> Uses a small batch of samples per update. A good balance of speed, stability, and hardware efficiency. Often the preferred method in practice.</li>
      </ul>
      <p>Understanding these variants allows you to choose the most appropriate optimization strategy depending on your dataset size, model complexity, and computational resources.</p>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A fork in the road with three paths: 'Batch GD (Slow & Steady)', 'SGD (Fast & Bumpy)', 'Mini-Batch GD (Balanced Ride)'. A machine learning practitioner character is pondering which path to take.">
      </div>
      <p>This concludes our foundational exploration of Gradient Descent and its main variants! You've learned how these algorithms work to minimize cost functions and train machine learning models. This knowledge is a critical building block for understanding almost all modern machine learning and deep learning. Well done on getting this far!</p>
  </section>

  <script>
      // Show the first section initially
      document.getElementById("section1").style.display = "block";
      document.getElementById("section1").style.opacity = "1";

      function showNextSection(nextSectionId) {
          const currentButton = event.target;
          const nextSection = document.getElementById("section" + nextSectionId);
          
          currentButton.style.display = "none";
          
          nextSection.style.display = "block";
          setTimeout(() => {
              nextSection.style.opacity = "1";
          }, 10);

          setTimeout(() => {
              nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }, 500);
      }

      function revealAnswer(id) {
          const revealText = document.getElementById(id);
          const revealButton = event.target;
          
          revealText.style.display = "block";
          revealButton.style.display = "none";
      }

      let selectedOption = null;

      function selectOption(element, index) {
          // Clear previous selection
          const options = document.querySelectorAll('.option');
          options.forEach(opt => opt.classList.remove('selected'));
          
          // Select current option
          element.classList.add('selected');
          selectedOption = index;
      }

      function checkAnswer(correctIndex) {
          // Hide all explanations first
          const explanations = document.querySelectorAll('.option-explanation');
          explanations.forEach(exp => exp.style.display = 'none');
          
          // If an option is selected, show its explanation
          if (selectedOption !== null) {
              document.getElementById(`explanation-${selectedOption}`).style.display = 'block';
              
              // Highlight correct/incorrect
              const options = document.querySelectorAll('.option');
              options.forEach((opt, index) => {
                  if (index === correctIndex) {
                      opt.style.border = '2px solid #28a745';
                  } else if (index === selectedOption && selectedOption !== correctIndex) {
                      opt.style.border = '2px solid #dc3545';
                  }
              });
          }
      }
  </script>
</body>
</html>