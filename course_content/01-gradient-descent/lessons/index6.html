<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Workout Time: Gradient Descent Calculations</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          max-width: 800px;
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          font-size: 150%;
      }
      section {
          margin-bottom: 20px;
          padding: 20px;
          background-color: #ffffff;
          display: none;
          opacity: 0;
          transition: opacity 0.5s ease-in;
      }
      h1, h2, h3, h4 {
          color: #333;
          margin-top: 20px;
      }
      p, li {
          line-height: 1.6;
          color: #444;
          margin-bottom: 20px;
      }
      ul, ol {
          padding-left: 20px;
      }
      .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          text-align: left;
      }
      .image-placeholder img, .interactive-placeholder img {
          max-width: 100%;
          height: auto;
          border-radius: 5px;
      }
      .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          padding: 20px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .vocab-section {
          background-color: #f0f8ff;
      }
      .vocab-section h3 {
          color: #1e90ff;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .vocab-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .vocab-term {
          font-weight: bold;
          color: #1e90ff;
      }
      .why-it-matters {
          background-color: #ffe6f0;
      }
      .why-it-matters h3 {
          color: #d81b60;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .stop-and-think {
          background-color: #e6e6ff;
      }
      .stop-and-think h3 {
          color: #4b0082;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .continue-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #007bff;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .reveal-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #4b0082;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .test-your-knowledge {
          background-color: #e6ffe6; /* Light green background */
      }
      .test-your-knowledge h3 {
          color: #28a745; /* Dark green heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .test-your-knowledge h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .test-your-knowledge p {
          margin-bottom: 15px;
      }
      .check-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #28a745; /* Green background */
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
          border: none;
          font-size: 1em;
      }
      .faq-section {
          background-color: #fffbea; /* Light yellow background */
      }
      .faq-section h3 {
          color: #ffcc00; /* Bright yellow heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .faq-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      table {
          width: 100%;
          border-collapse: collapse;
          margin: 20px 0;
      }
      table, th, td {
          border: 1px solid #ddd;
      }
      th, td {
          padding: 12px;
          text-align: left;
      }
      th {
          background-color: #f2f2f2;
      }
      .option {
          margin-bottom: 10px;
          padding: 10px;
          border: 1px solid #ddd;
          border-radius: 5px;
          cursor: pointer;
      }
      .option:hover {
          background-color: #f5f5f5;
      }
      .option.selected {
          background-color: #e6ffe6;
          border-color: #28a745;
      }
      .option-feedback {
          margin-top: 10px;
          padding: 10px;
          border-radius: 5px;
          display: none;
      }
      .correct-feedback {
          background-color: #d4edda;
          color: #155724;
      }
      .incorrect-feedback {
          background-color: #f8d7da;
          color: #721c24;
      }
      .solution-section {
          background-color: #f8f9fa;
          padding: 15px;
          border-radius: 5px;
          margin-top: 20px;
          border-left: 4px solid #007bff;
      }
      .solution-toggle {
          cursor: pointer;
          color: #007bff;
          font-weight: bold;
      }
      .solution-content {
          display: none;
          margin-top: 10px;
      }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <section id="section1">
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A cartoon character flexing its bicep, which is shaped like a Beta symbol (β), with a calculator in its other hand. Math symbols float around.">
      </div>
      <h1>Workout Time: Gradient Descent Calculations</h1>
      <p>Alright, team! You've seen Gradient Descent in action visually, and you know the theory behind its application to Linear Regression from Lesson 2. Now, it's time to get our hands dirty with some actual numbers and flex those calculation muscles!</p>
      <p>In this exercise session, we're going to manually step through the Gradient Descent updates for a simple linear regression problem. This will really help solidify your understanding of how those formulas translate into changing parameter values.</p>
      <div class="continue-button" onclick="showNextSection(2)">Continue</div>
  </section>

  <section id="section2">
      <h2>The Scenario: Predicting Exam Scores</h2>
      <p>Imagine we're trying to build a very simple model to predict a student's exam score (let's call this $$y$$) based on the number of hours they studied (our single feature, $$x$$).</p>
      <p>Our linear model (or hypothesis) is:</p>
      <p>\[ \text{Score} = f_\beta(x) = \beta_0 + \beta_1 \cdot \text{Hours} \]</p>
      <p><strong>Here's what we're given for our problem:</strong></p>
      <p>\[\text{Learning Rate (α)} = 0.05\]</p>
      <p>\[\text{Initial Parameters: } \beta_0 = 2, \quad \beta_1 = 3\]</p>
      <p>And our tiny dataset consists of just two students (so, $$n=2$$):</p>
      <div class="interactive-placeholder">
          <table>
              <thead>
                  <tr>
                      <th>Student</th>
                      <th>Hours Studied (x<sup>(i)</sup>)</th>
                      <th>Actual Score (y<sup>(i)</sup>)</th>
                  </tr>
              </thead>
              <tbody>
                  <tr>
                      <td>1</td>
                      <td>2</td>
                      <td>10</td>
                  </tr>
                  <tr>
                      <td>2</td>
                      <td>4</td>
                      <td>15</td>
                  </tr>
              </tbody>
          </table>
          <p><em>Our training data for this exercise.</em></p>
      </div>
      <div class="continue-button" onclick="showNextSection(3)">Continue</div>
  </section>

  <section id="section3">
      <p><strong>Quick Formula Recap (from Lesson 2):</strong><br>
      Remember these? We'll need them!</p>
      <ul>
          <li><strong>Prediction:</strong> $$f_\beta(x^{(i)}) = \beta_0 + \beta_1 x^{(i)}$$</li>
          <li><strong>Error Term (for gradients):</strong> $$(f_\beta(x^{(i)}) - y^{(i)})$$</li>
          <li><strong>Gradient for $$\beta_0$$:</strong> $$\frac{\partial C}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)})$$</li>
          <li><strong>Gradient for $$\beta_1$$:</strong> $$\frac{\partial C}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$$</li>
          <li><strong>Update Rule:</strong> $$\beta_{\text{new}} = \beta_{\text{old}} - \alpha \cdot \text{Gradient}$$</li>
          <li><strong>(Optional) Cost Function (MSE):</strong> $$C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)})^2$$ (useful to see if we're improving!)</li>
      </ul>
      <div class="vocab-section">
          <h3>Build Your Vocab</h3>
          <h4 class="vocab-term">Epoch (in this context)</h4>
          <p>When performing Batch Gradient Descent (which we are simulating here by using all n=2 samples for each gradient calculation), one full pass through the entire training dataset to calculate the gradients and make one update to the parameters is effectively one step or one iteration. The term 'epoch' becomes more distinct with Mini-Batch or Stochastic GD where multiple updates happen within one pass through the data.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(4)">Continue</div>
  </section>

  <section id="section4">
      <h2>Exercise Tasks: Let's Calculate!</h2>
      <p>We've got three levels of challenge for you. Start with Level 1 and see how far you can go! Show your work for each step – it's all about the process.</p>
      <div class="continue-button" onclick="showNextSection(5)">Continue</div>
  </section>

  <section id="section5">
      <p><strong>Difficulty Level 1: One Step Forward</strong><br>
      Your mission is to perform <em>one full update step</em> of Gradient Descent.</p>
      <p>\[\textbf{Tasks for Level 1:}\]</p>
      <ol>
          <li><strong>Calculate Predictions:</strong> Using the initial $$\beta_0 = 2$$ and $$\beta_1 = 3$$, calculate the predicted score $$f_\beta(x^{(i)})$$ for Student 1 and Student 2.</li>
          <li><strong>Calculate Error Terms:</strong> For each student, find the error term $$(f_\beta(x^{(i)}) - y^{(i)})$$.</li>
          <li><strong>(Optional Bonus):</strong> Calculate the initial Cost $$C(\beta)$$ using the MSE formula with your initial predictions. What's our starting 'badness' score?</li>
          <li><strong>Calculate Gradients:</strong> Compute the gradient $$\frac{\partial C}{\partial \beta_0}$$ and the gradient $$\frac{\partial C}{\partial \beta_1}$$. Remember $$n=2$$.</li>
          <li><strong>Update Parameters:</strong> Using the learning rate $$\alpha = 0.05$$ and your calculated gradients, find the new values for $$\beta_0$$ and $$\beta_1$$ after this single update step.</li>
      </ol>
      <div class="interactive-placeholder">
          <div style="border: 1px solid #ddd; padding: 20px; border-radius: 5px;">
              <h3>Level 1 Worksheet</h3>
              <p>Show your work for each step in Level 1!</p>
              <div class="solution-section">
                  <div class="solution-toggle" onclick="toggleSolution('level1-solution')">Click to reveal solution for Level 1</div>
                  <div id="level1-solution" class="solution-content">
                      <h4>Step 1: Calculate Predictions</h4>
                      <p>For Student 1: $$f_\beta(x^{(1)}) = \beta_0 + \beta_1 \cdot x^{(1)} = 2 + 3 \cdot 2 = 2 + 6 = 8$$</p>
                      <p>For Student 2: $$f_\beta(x^{(2)}) = \beta_0 + \beta_1 \cdot x^{(2)} = 2 + 3 \cdot 4 = 2 + 12 = 14$$</p>
                      
                      <h4>Step 2: Calculate Error Terms</h4>
                      <p>For Student 1: $$f_\beta(x^{(1)}) - y^{(1)} = 8 - 10 = -2$$</p>
                      <p>For Student 2: $$f_\beta(x^{(2)}) - y^{(2)} = 14 - 15 = -1$$</p>
                      
                      <h4>Step 3: (Optional) Calculate Initial Cost</h4>
                      <p>$$C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)})^2$$</p>
                      <p>$$C(\beta) = \frac{1}{2 \cdot 2} [(8 - 10)^2 + (14 - 15)^2] = \frac{1}{4} [4 + 1] = \frac{1}{4} \cdot 5 = 1.25$$</p>
                      
                      <h4>Step 4: Calculate Gradients</h4>
                      <p>For $$\beta_0$$: $$\frac{\partial C}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)})$$</p>
                      <p>$$\frac{\partial C}{\partial \beta_0} = \frac{1}{2} [(-2) + (-1)] = \frac{1}{2} \cdot (-3) = -1.5$$</p>
                      
                      <p>For $$\beta_1$$: $$\frac{\partial C}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$$</p>
                      <p>$$\frac{\partial C}{\partial \beta_1} = \frac{1}{2} [(-2) \cdot 2 + (-1) \cdot 4] = \frac{1}{2} [(-4) + (-4)] = \frac{1}{2} \cdot (-8) = -4$$</p>
                      
                      <h4>Step 5: Update Parameters</h4>
                      <p>$$\beta_{0,new} = \beta_{0,old} - \alpha \cdot \frac{\partial C}{\partial \beta_0} = 2 - 0.05 \cdot (-1.5) = 2 + 0.075 = 2.075$$</p>
                      <p>$$\beta_{1,new} = \beta_{1,old} - \alpha \cdot \frac{\partial C}{\partial \beta_1} = 3 - 0.05 \cdot (-4) = 3 + 0.2 = 3.2$$</p>
                  </div>
              </div>
          </div>
      </div>
      <div class="continue-button" onclick="showNextSection(6)">Continue</div>
  </section>

  <section id="section6">
      <p><strong>Difficulty Level 2: Two Full Iterations</strong><br>
      Ready for more? Let's take another step!</p>
      <p>\[\textbf{Tasks for Level 2:}\]</p>
      <ol>
          <li><strong>Complete all tasks from Level 1.</strong> Make sure your updated $$\beta_0$$ and $$\beta_1$$ from Level 1 are correct, as you'll need them here.</li>
          <li><strong>Perform a Second Iteration:</strong> Now, treat the $$\beta_0$$ and $$\beta_1$$ values you calculated at the end of Level 1 as your <em>new starting parameters</em>. Repeat the entire process:
              <ul>
                  <li>Calculate new predictions for Student 1 and Student 2 using these <em>updated</em> $$\beta$$s.</li>
                  <li>Calculate the new error terms.</li>
                  <li><strong>(Optional Bonus):</strong> Calculate the new Cost $$C(\beta)$$ using these new predictions. Did the cost go down compared to the initial cost (if you calculated it)? It should!</li>
                  <li>Calculate the new gradients $$\frac{\partial C}{\partial \beta_0}$$ and $$\frac{\partial C}{\partial \beta_1}$$.</li>
                  <li>Calculate the final $$\beta_0$$ and $$\beta_1$$ values after this <em>second</em> update step.</li>
              </ul>
          </li>
      </ol>
      <div class="stop-and-think">
          <h3>Stop and Think</h3>
          <h4>If Gradient Descent is working correctly, what do you expect to happen to the Cost C(β) after each successful iteration (assuming a reasonable learning rate)?</h4>
          <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
          <p id="stop-and-think-1" style="display: none;">The Cost C(β) should decrease! Each step is meant to take us to a point of lower cost on the cost function landscape.</p>
      </div>
      <div class="interactive-placeholder">
          <div style="border: 1px solid #ddd; padding: 20px; border-radius: 5px;">
              <h3>Level 2 Worksheet</h3>
              <p>Continue your calculations for the second iteration!</p>
              <div class="solution-section">
                  <div class="solution-toggle" onclick="toggleSolution('level2-solution')">Click to reveal solution for Level 2</div>
                  <div id="level2-solution" class="solution-content">
                      <p>Starting with our updated parameters from Level 1: $$\beta_0 = 2.075$$ and $$\beta_1 = 3.2$$</p>
                      
                      <h4>Step 1: Calculate New Predictions</h4>
                      <p>For Student 1: $$f_\beta(x^{(1)}) = \beta_0 + \beta_1 \cdot x^{(1)} = 2.075 + 3.2 \cdot 2 = 2.075 + 6.4 = 8.475$$</p>
                      <p>For Student 2: $$f_\beta(x^{(2)}) = \beta_0 + \beta_1 \cdot x^{(2)} = 2.075 + 3.2 \cdot 4 = 2.075 + 12.8 = 14.875$$</p>
                      
                      <h4>Step 2: Calculate New Error Terms</h4>
                      <p>For Student 1: $$f_\beta(x^{(1)}) - y^{(1)} = 8.475 - 10 = -1.525$$</p>
                      <p>For Student 2: $$f_\beta(x^{(2)}) - y^{(2)} = 14.875 - 15 = -0.125$$</p>
                      
                      <h4>Step 3: (Optional) Calculate New Cost</h4>
                      <p>$$C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)})^2$$</p>
                      <p>$$C(\beta) = \frac{1}{2 \cdot 2} [(8.475 - 10)^2 + (14.875 - 15)^2]$$</p>
                      <p>$$C(\beta) = \frac{1}{4} [(-1.525)^2 + (-0.125)^2] = \frac{1}{4} [2.325625 + 0.015625] = \frac{1}{4} \cdot 2.34125 = 0.5853125$$</p>
                      <p>Yes, the cost has decreased from 1.25 to approximately 0.585, which is good!</p>
                      
                      <h4>Step 4: Calculate New Gradients</h4>
                      <p>For $$\beta_0$$: $$\frac{\partial C}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)})$$</p>
                      <p>$$\frac{\partial C}{\partial \beta_0} = \frac{1}{2} [(-1.525) + (-0.125)] = \frac{1}{2} \cdot (-1.65) = -0.825$$</p>
                      
                      <p>For $$\beta_1$$: $$\frac{\partial C}{\partial \beta_1} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)}) \cdot x^{(i)}$$</p>
                      <p>$$\frac{\partial C}{\partial \beta_1} = \frac{1}{2} [(-1.525) \cdot 2 + (-0.125) \cdot 4] = \frac{1}{2} [(-3.05) + (-0.5)] = \frac{1}{2} \cdot (-3.55) = -1.775$$</p>
                      
                      <h4>Step 5: Update Parameters Again</h4>
                      <p>$$\beta_{0,new} = \beta_{0,old} - \alpha \cdot \frac{\partial C}{\partial \beta_0} = 2.075 - 0.05 \cdot (-0.825) = 2.075 + 0.04125 = 2.11625$$</p>
                      <p>$$\beta_{1,new} = \beta_{1,old} - \alpha \cdot \frac{\partial C}{\partial \beta_1} = 3.2 - 0.05 \cdot (-1.775) = 3.2 + 0.08875 = 3.28875$$</p>
                  </div>
              </div>
          </div>
      </div>
      <div class="continue-button" onclick="showNextSection(7)">Continue</div>
  </section>

  <section id="section7">
      <p><strong>Difficulty Level 3: Parameter Sensitivity & What-If Scenarios</strong><br>
      Time to explore how changes can impact the outcome! For these tasks, always start from the <em>original initial parameters</em> ($$\beta_0 = 2, \beta_1 = 3$$) unless specified otherwise.</p>
      <p>\[\textbf{Tasks for Level 3:}\]</p>
      <ol>
          <li><strong>Complete all tasks from Level 1</strong> (so you have a baseline).</li>
          <li><strong>What-If Scenario 1 (Learning Rate Impact):</strong>
              <ul>
                  <li>Go back to the <em>original initial parameters</em> ($$\beta_0 = 2, \beta_1 = 3$$).</li>
                  <li>Now, imagine the learning rate $$\alpha$$ was much larger, say $$\alpha = 0.5$$ (ten times our original!).</li>
                  <li>Recalculate only the <em>first update step</em> (find the new $$\beta_0$$ and $$\beta_1$$).</li>
                  <li><strong>Observe & Explain:</strong> What happens to the new parameter values? Do they seem to be moving in a sensible direction, or are the changes very drastic? Could this be a problem if it continued?</li>
              </ul>
          </li>
          <li><strong>What-If Scenario 2 (Outlier Data Impact):</strong>
              <ul>
                  <li>Go back to the <em>original initial parameters</em> ($$\beta_0 = 2, \beta_1 = 3$$) and the <em>original learning rate</em> ($$\alpha = 0.05$$).</li>
                  <li>Now, imagine Student 2's data was different: Hours (x<sup>(2)</sup>) = 4, but their actual Score (y<sup>(2)</sup>) was only 5 (perhaps they had a bad day – this is an outlier!). Student 1's data remains the same.</li>
                  <li>Recalculate only the <em>first update step</em> for $$\beta_0$$ and $$\beta_1$$ using this modified dataset.</li>
                  <li><strong>Observe & Explain:</strong> How does this single outlier data point affect the calculated gradients and the updated parameter values compared to your original Level 1 calculation? Does the outlier pull the parameters in a particular direction?</li>
              </ul>
          </li>
      </ol>
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>Level 3 explores hyperparameter sensitivity (like the learning rate) and data sensitivity (like outliers). Understanding these helps in diagnosing issues when training real machine learning models.</p>
      </div>
      <div class="interactive-placeholder">
          <div style="border: 1px solid #ddd; padding: 20px; border-radius: 5px;">
              <h3>Level 3 Worksheet</h3>
              <p>Explore the what-if scenarios!</p>
              <div class="solution-section">
                  <div class="solution-toggle" onclick="toggleSolution('level3-solution')">Click to reveal solution for Level 3</div>
                  <div id="level3-solution" class="solution-content">
                      <h4>What-If Scenario 1: Learning Rate Impact</h4>
                      <p>Using the original parameters $$\beta_0 = 2$$ and $$\beta_1 = 3$$ but with learning rate $$\alpha = 0.5$$</p>
                      <p>From Level 1, we calculated:</p>
                      <p>$$\frac{\partial C}{\partial \beta_0} = -1.5$$ and $$\frac{\partial C}{\partial \beta_1} = -4$$</p>
                      <p>Now with $$\alpha = 0.5$$:</p>
                      <p>$$\beta_{0,new} = \beta_{0,old} - \alpha \cdot \frac{\partial C}{\partial \beta_0} = 2 - 0.5 \cdot (-1.5) = 2 + 0.75 = 2.75$$</p>
                      <p>$$\beta_{1,new} = \beta_{1,old} - \alpha \cdot \frac{\partial C}{\partial \beta_1} = 3 - 0.5 \cdot (-4) = 3 + 2 = 5$$</p>
                      
                      <p><strong>Observation:</strong> The changes are much more drastic! $$\beta_0$$ increased by 0.75 (compared to 0.075 with $$\alpha = 0.05$$) and $$\beta_1$$ increased by 2 (compared to 0.2). While the direction is still correct (both parameters are increasing), the magnitude of change is 10 times larger.</p>
                      <p>This could be problematic if it continued because we might overshoot the minimum and start oscillating around it or even diverge completely. With such large steps, we might jump past the optimal values and never converge.</p>
                      
                      <h4>What-If Scenario 2: Outlier Data Impact</h4>
                      <p>Using the original parameters $$\beta_0 = 2$$ and $$\beta_1 = 3$$ with $$\alpha = 0.05$$, but Student 2's score is now 5 instead of 15.</p>
                      
                      <p>Step 1: Calculate Predictions (same as before)</p>
                      <p>For Student 1: $$f_\beta(x^{(1)}) = 2 + 3 \cdot 2 = 8$$</p>
                      <p>For Student 2: $$f_\beta(x^{(2)}) = 2 + 3 \cdot 4 = 14$$</p>
                      
                      <p>Step 2: Calculate Error Terms</p>
                      <p>For Student 1: $$f_\beta(x^{(1)}) - y^{(1)} = 8 - 10 = -2$$ (same as before)</p>
                      <p>For Student 2: $$f_\beta(x^{(2)}) - y^{(2)} = 14 - 5 = 9$$ (very different from before!)</p>
                      
                      <p>Step 3: Calculate Gradients</p>
                      <p>For $$\beta_0$$: $$\frac{\partial C}{\partial \beta_0} = \frac{1}{2} [(-2) + 9] = \frac{1}{2} \cdot 7 = 3.5$$</p>
                      <p>For $$\beta_1$$: $$\frac{\partial C}{\partial \beta_1} = \frac{1}{2} [(-2) \cdot 2 + 9 \cdot 4] = \frac{1}{2} [(-4) + 36] = \frac{1}{2} \cdot 32 = 16$$</p>
                      
                      <p>Step 4: Update Parameters</p>
                      <p>$$\beta_{0,new} = \beta_{0,old} - \alpha \cdot \frac{\partial C}{\partial \beta_0} = 2 - 0.05 \cdot 3.5 = 2 - 0.175 = 1.825$$</p>
                      <p>$$\beta_{1,new} = \beta_{1,old} - \alpha \cdot \frac{\partial C}{\partial \beta_1} = 3 - 0.05 \cdot 16 = 3 - 0.8 = 2.2$$</p>
                      
                      <p><strong>Observation:</strong> The outlier has a dramatic effect! Instead of both parameters increasing (as in Level 1), now both parameters are decreasing. The gradients have completely changed sign and magnitude. The outlier is pulling the model in the opposite direction, trying to make it fit this unusual data point. This demonstrates how sensitive gradient descent can be to outliers, especially in small datasets.</p>
                  </div>
              </div>
          </div>
      </div>
      <div class="continue-button" onclick="showNextSection(8)">Continue</div>
  </section>

  <section id="section8">
      <h2>Solutions & Self-Check</h2>
      <p>Okay, take your time with the calculations! Once you've given it your best shot, you can check your work against the solutions provided above. The goal isn't just to get the right numbers, but to understand <em>how</em> we get them.</p>
      <p>Don't worry if your numbers aren't perfect on the first try. The key is to identify where any differences occurred and understand why. Did you sum correctly? Did you use the right 'n'? Was the learning rate applied correctly?</p>
      <div class="faq-section">
          <h3>Frequently Asked Questions</h3>
          <h4>Why do we divide by 'n' (or '2n') in the cost and gradient formulas? What if I forgot?</h4>
          <p>Dividing by 'n' (the number of samples) in the gradient calculation gives us the <em>average</em> gradient contribution per sample. This helps keep the magnitude of the gradients somewhat consistent regardless of dataset size. If you forgot it, your gradients would be 'n' times larger, meaning your effective learning rate would also be 'n' times larger, likely causing your parameters to update too aggressively and possibly diverge! The '1/2' in '1/2n' for the MSE cost is a mathematical convenience that cancels out a '2' when differentiating a squared term, simplifying the gradient formula slightly.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(9)">Continue</div>
  </section>

  <section id="section9">
      <h2>Workout Cooldown: What We've Accomplished</h2>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=200&width=200" alt="A small graphic of a brain with gears turning, next to a checkmark, symbolizing understanding achieved through effort.">
      </div>
      <p>Excellent effort! By manually crunching these numbers, you've gained a much deeper, more tangible understanding of how Gradient Descent iteratively refines model parameters. You've seen predictions, errors, gradients, and updates all come together.</p>
      <p>This kind of hands-on calculation builds a strong foundation for when you start <em>coding</em> these algorithms. You'll know exactly what your code is supposed to be doing under the hood.</p>
      <p>Next up, get ready to translate this understanding into Python! We'll be implementing Batch Gradient Descent from scratch in our upcoming coding lesson.</p>
      <div class="test-your-knowledge">
          <h3>Test Your Knowledge</h3>
          <h4>If, after an update step, your new parameter β₁ became much larger and positive, what does that generally imply about the errors and feature values that contributed to its gradient?</h4>
          <div class="option" onclick="selectOption(1, false)">
              <p>The term Σ(f_β(x^(i)) - y^(i)) * x^(i) was largely positive, meaning on average, when x^(i) was positive, predictions f_β(x^(i)) were too high compared to y^(i).</p>
              <div id="feedback-1" class="option-feedback incorrect-feedback">
                  <p>Careful with the signs! If the gradient term (Σ(error)*x) was largely positive, then β_new = β_old - α*(positive gradient), which would make β_new <em>smaller</em> if β_old was positive, or more negative. For β₁ to become <em>larger</em> and positive, the gradient term itself must have been negative.</p>
              </div>
          </div>
          <div class="option" onclick="selectOption(2, true)">
              <p>The term Σ(f_β(x^(i)) - y^(i)) * x^(i) was largely negative. This means, for positive x^(i), predictions f_β(x^(i)) were often too low (f_β(x^(i)) - y^(i) < 0), or for negative x^(i), predictions were too high (f_β(x^(i)) - y^(i) > 0).</p>
              <div id="feedback-2" class="option-feedback correct-feedback">
                  <p>Spot on! If the gradient term Σ(error)*x is largely negative, the update β_new = β_old - α*(negative gradient) becomes β_new = β_old + α*(positive value). This would increase β₁.</p>
              </div>
          </div>
          <div class="option" onclick="selectOption(3, false)">
              <p>It means the learning rate α was too small.</p>
              <div id="feedback-3" class="option-feedback incorrect-feedback">
                  <p>The learning rate affects the <em>size</em> of the step, but the <em>direction</em> of the change in β₁ is determined by the sign of its gradient.</p>
              </div>
          </div>
      </div>
  </section>

  <script>
      // Show the first section initially
      document.getElementById("section1").style.display = "block";
      document.getElementById("section1").style.opacity = "1";

      function showNextSection(nextSectionId) {
          const currentButton = event.target;
          const nextSection = document.getElementById("section" + nextSectionId);
          
          currentButton.style.display = "none";
          
          nextSection.style.display = "block";
          setTimeout(() => {
              nextSection.style.opacity = "1";
          }, 10);

          setTimeout(() => {
              nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }, 500);
      }

      function revealAnswer(id) {
          const revealText = document.getElementById(id);
          const revealButton = event.target;
          
          revealText.style.display = "block";
          revealButton.style.display = "none";
      }

      function toggleSolution(id) {
          const solutionContent = document.getElementById(id);
          if (solutionContent.style.display === "block") {
              solutionContent.style.display = "none";
          } else {
              solutionContent.style.display = "block";
          }
      }

      function selectOption(optionNumber, isCorrect) {
          // Clear all selections first
          const options = document.querySelectorAll('.option');
          options.forEach(option => {
              option.classList.remove('selected');
          });
          
          // Hide all feedback
          const feedbacks = document.querySelectorAll('.option-feedback');
          feedbacks.forEach(feedback => {
              feedback.style.display = 'none';
          });
          
          // Select the clicked option and show its feedback
          const selectedOption = event.currentTarget;
          selectedOption.classList.add('selected');
          
          const feedback = document.getElementById('feedback-' + optionNumber);
          feedback.style.display = 'block';
      }
  </script>
</body>
</html>