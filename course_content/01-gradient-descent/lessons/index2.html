<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Gradient Descent for Linear Regression</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          max-width: 800px;
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          font-size: 150%;
      }
      section {
          margin-bottom: 20px;
          padding: 20px;
          background-color: #ffffff;
          display: none;
          opacity: 0;
          transition: opacity 0.5s ease-in;
      }
      h1, h2, h3, h4 {
          color: #333;
          margin-top: 20px;
      }
      p, li {
          line-height: 1.6;
          color: #444;
          margin-bottom: 20px;
      }
      ul {
          padding-left: 20px;
      }
      .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          text-align: left;
      }
      .image-placeholder img, .interactive-placeholder img {
          max-width: 100%;
          height: auto;
          border-radius: 5px;
      }
      .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          padding: 20px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .vocab-section {
          background-color: #f0f8ff;
      }
      .vocab-section h3 {
          color: #1e90ff;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .vocab-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .vocab-term {
          font-weight: bold;
          color: #1e90ff;
      }
      .why-it-matters {
          background-color: #ffe6f0;
      }
      .why-it-matters h3 {
          color: #d81b60;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .stop-and-think {
          background-color: #e6e6ff;
      }
      .stop-and-think h3 {
          color: #4b0082;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .continue-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #007bff;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .reveal-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #4b0082;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .test-your-knowledge {
          background-color: #e6ffe6; /* Light green background */
      }
      .test-your-knowledge h3 {
          color: #28a745; /* Dark green heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .test-your-knowledge h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .test-your-knowledge p {
          margin-bottom: 15px;
      }
      .check-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #28a745; /* Green background */
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
          border: none;
          font-size: 1em;
      }
      .option {
          margin-bottom: 10px;
          padding: 10px;
          border: 1px solid #ddd;
          border-radius: 5px;
          cursor: pointer;
      }
      .option:hover {
          background-color: #f5f5f5;
      }
      .option-explanation {
          margin-top: 10px;
          padding: 10px;
          background-color: #f9f9f9;
          border-radius: 5px;
          display: none;
      }
      .correct {
          border-color: #28a745;
          background-color: #d4edda;
      }
      .incorrect {
          border-color: #dc3545;
          background-color: #f8d7da;
      }
      .faq-section {
          background-color: #fffbea; /* Light yellow background */
      }
      .faq-section h3 {
          color: #ffcc00; /* Bright yellow heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .faq-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <section id="section1">
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A split image: Left side shows the general Gradient Descent formula (β_new = β_old - α∇C(β)). Right side shows a simple scatter plot with a line of best fit, hinting at linear regression.">
      </div>
      <h1>Gradient Descent for Linear Regression</h1>
      <p>Welcome back! In our last lesson, we got a solid grip on the general idea behind Gradient Descent. We learned its core update rule: <code>β_new = β_old - α * ∇C(β)</code>, which is our trusty guide for navigating down any cost function hill. Remember, $$\beta$$ are our model parameters, $$C(\beta)$$ is the cost we want to minimize, $$\nabla C(\beta)$$ is the gradient telling us the steepest uphill direction, and $$\alpha$$ is our step size, the learning rate.</p>
      <div class="continue-button" onclick="showNextSection(2)">Continue</div>
  </section>

  <section id="section2">
      <h2>Setting the Stage: Linear Regression</h2>
      <p>Now, let's get practical! We're going to apply Gradient Descent to one of the most fundamental machine learning models: <strong>Linear Regression</strong>.</p>
      <p>Imagine you have some data – say, the size of a house (our feature, $$x$$) and its price (our target, $$y$$). Linear regression tries to find a straight line that best predicts the price given the size.</p>
      <div class="continue-button" onclick="showNextSection(3)">Continue</div>
  </section>

  <section id="section3">
      <p>This line is defined by an equation. If we have just one feature $$x$$ (like house size), our model's prediction, which we'll call $$f_\beta(x)$$, looks like this:</p>
      <p>\[ f_\beta(x^{(i)}) = \beta_0 + \beta_1 x^{(i)} \]</p>
      <p>Here:</p>
      <ul>
          <li>$$x^{(i)}$$ is the feature value for our $$i$$-th data point (e.g., the size of the $$i$$-th house).</li>
          <li>$$\beta_0$$ is the <strong>intercept</strong> (or bias) – it's where the line crosses the y-axis (price axis when size is 0).</li>
          <li>$$\beta_1$$ is the <strong>coefficient</strong> (or weight) for our feature $$x$$ – it represents the slope of the line, telling us how much the price changes for a one-unit change in size.</li>
      </ul>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A simple 2D scatter plot with 'House Size (x)' on the horizontal axis and 'Price (y)' on the vertical axis. A few data points are plotted. A straight line (y = β₀ + β₁x) is drawn through the points. β₀ is marked where the line intersects the y-axis, and the slope β₁ is indicated.">
      </div>
      <p>Our linear model: trying to fit a line to the data.</p>
      <div class="continue-button" onclick="showNextSection(4)">Continue</div>
  </section>

  <section id="section4">
      <p>If we have <em>multiple</em> features (e.g., size, number of bedrooms, age of house), say $$m$$ features $$x_1, x_2, ..., x_m$$, our prediction equation becomes:</p>
      <p>\[ f_\beta(x^{(i)}) = \beta_0 + \beta_1 x_1^{(i)} + \beta_2 x_2^{(i)} + ... + \beta_m x_m^{(i)} \]</p>
      <p>Our goal with Gradient Descent will be to find the best values for $$\beta_0, \beta_1, ..., \beta_m$$ that make our line (or hyperplane in higher dimensions) fit the data as closely as possible.</p>
      <div class="vocab-section">
          <h3>Build Your Vocab</h3>
          <h4 class="vocab-term">Hypothesis (in Machine Learning)</h4>
          <p>The function $$f_\beta(x)$$ that our model uses to make predictions is often called the 'hypothesis'. It represents our model's guess about the relationship between features (x) and the target (y).</p>
      </div>
      <div class="continue-button" onclick="showNextSection(5)">Continue</div>
  </section>

  <section id="section5">
      <h2>Measuring 'Badness': The Cost Function (MSE)</h2>
      <p>To use Gradient Descent, we first need a way to measure how 'good' or 'bad' our current line (defined by our current $$\beta$$s) is at fitting the data. This is where the <strong>Cost Function</strong> comes in. For linear regression, a very popular choice is the <strong>Mean Squared Error (MSE)</strong>.</p>
      <div class="continue-button" onclick="showNextSection(6)">Continue</div>
  </section>

  <section id="section6">
      <p>Here's the idea behind MSE:</p>
      <ol>
          <li>For each data point $$(x^{(i)}, y^{(i)})$$ (e.g., i-th house size and its actual price), our model makes a prediction $$f_\beta(x^{(i)})$$.</li>
          <li>The <strong>error</strong> for that data point is the difference between the actual value $$y^{(i)}$$ and our predicted value: $$ (y^{(i)} - f_\beta(x^{(i)})) $$.</li>
          <li>We <strong>square</strong> this error: $$ (y^{(i)} - f_\beta(x^{(i)}))^2 $$. Why square it?
              <ul>
                  <li>It makes all errors positive (we don't want negative and positive errors canceling out).</li>
                  <li>It penalizes larger errors more heavily than smaller errors.</li>
              </ul>
          </li>
      </ol>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A graph showing a data point (y_actual) and a prediction point (y_predicted) from a line. A vertical line segment connecting them is labeled 'Error'. Another graph shows the y = x^2 curve, illustrating how squaring an error amplifies larger errors.">
      </div>
      <div class="continue-button" onclick="showNextSection(7)">Continue</div>
  </section>

  <section id="section7">
      <ol start="4">
          <li>We <strong>sum</strong> these squared errors over all our $$n$$ training examples.</li>
          <li>Finally, we take the <strong>average</strong> by dividing by $$n$$ (or $$2n$$ for mathematical convenience later when we take derivatives – the $$1/2$$ factor will cancel out a `2` that comes from differentiating the square).</li>
      </ol>
      <div class="continue-button" onclick="showNextSection(8)">Continue</div>
  </section>

  <section id="section8">
      <p>So, the Mean Squared Error cost function $$C(\beta)$$ is:</p>
      <p>\[ C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - f_\beta(x^{(i)}))^2 \]</p>
      <p>If we plug in our linear hypothesis $$f_\beta(x^{(i)}) = \beta_0 + \beta_1 x_1^{(i)} + ... + \beta_m x_m^{(i)}$$, it becomes:</p>
      <p>\[ C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - (\beta_0 + \beta_1 x_1^{(i)} + ... + \beta_m x_m^{(i)}))^2 \]</p>
      <p>Our goal for Gradient Descent is to find the $$\beta$$ values that make this $$C(\beta)$$ as small as possible!</p>
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>The MSE cost function for linear regression is convex, meaning it looks like a bowl (in higher dimensions). This is great because it guarantees that Gradient Descent will converge to the single global minimum, provided we choose an appropriate learning rate.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(9)">Continue</div>
  </section>

  <section id="section9">
      <h2>Finding the Way Down: The Gradients</h2>
      <p>Alright, we have our cost function $$C(\beta)$$. To use our Gradient Descent rule ($$\beta_{\text{new}} = \beta_{\text{old}} - \alpha \nabla C(\beta_{\text{old}})$$), we need to calculate the <strong>gradient</strong>, $$\nabla C(\beta)$$.</p>
      <p>The gradient is a vector of partial derivatives. This means we need to find how the cost $$C(\beta)$$ changes if we slightly 'wiggle' each parameter $$\beta_j$$ (like $$\beta_0$$, $$\beta_1$$, etc.) one at a time, while keeping the others fixed. This is denoted as $$\frac{\partial C}{\partial \beta_j}$$.</p>
      <div class="continue-button" onclick="showNextSection(10)">Continue</div>
  </section>

  <section id="section10">
      <p>Let's derive these partial derivatives. It involves a bit of calculus (specifically the chain rule), but we'll focus on the results.</p>
      <p>Let's define the error for the i-th example as <code>error^(i) = (f_β(x^(i)) - y^(i))</code>. Note that our cost function uses <code>(y^(i) - f_β(x^(i)))</code>. The square makes the sign irrelevant for the cost value itself, but it affects the sign during differentiation. Let's stick to <code>(y^(i) - f_β(x^(i)))</code> from the cost function for differentiation.</p>
      <p>\[ C(\beta) = \frac{1}{2n} \sum_{i=1}^{n} (y^{(i)} - f_\beta(x^{(i)}))^2 \]</p>
      <div class="continue-button" onclick="showNextSection(11)">Continue</div>
  </section>

  <section id="section11">
      <p><strong>Gradient for $$\beta_0$$ (the intercept/bias term):</strong></p>
      <p>When we differentiate $$C(\beta)$$ with respect to $$\beta_0$$, the term $$f_\beta(x^{(i)})$$ contains $$\beta_0$$ with a coefficient of 1. Applying the chain rule: <code>d/dβ₀ (y^(i) - (β₀ + β₁x₁^(i) + ...))^2</code> gives <code>2 * (y^(i) - f_β(x^(i))) * (-1)</code>.</p>
      <p>\[ \frac{\partial C}{\partial \beta_0} = \frac{1}{2n} \sum_{i=1}^{n} 2 \cdot (y^{(i)} - f_\beta(x^{(i)})) \cdot \frac{\partial}{\partial \beta_0}(y^{(i)} - f_\beta(x^{(i)})) \]</p>
      <p>\[ = \frac{1}{2n} \sum_{i=1}^{n} 2 \cdot (y^{(i)} - f_\beta(x^{(i)})) \cdot (-1) \]</p>
      <p>\[ = \frac{1}{n} \sum_{i=1}^{n} -(y^{(i)} - f_\beta(x^{(i)})) \]</p>
      <p>\[ \frac{\partial C}{\partial \beta_0} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)}) \]</p>
      <p>This tells us how much the cost changes for a small change in $$\beta_0$$. It's basically the average of the errors (prediction minus actual).</p>
      <div class="continue-button" onclick="showNextSection(12)">Continue</div>
  </section>

  <section id="section12">
      <p><strong>Gradient for $$\beta_j$$ (any other coefficient, where $$j > 0$$):</strong></p>
      <p>For any other parameter $$\beta_j$$ (e.g., $$\beta_1$$ for feature $$x_1$$), the term $$f_\beta(x^{(i)})$$ contains $$\beta_j$$ multiplied by $$x_j^{(i)}$$. Applying the chain rule: <code>d/dβⱼ (y^(i) - (β₀ + ... + βⱼxⱼ^(i) + ...))^2</code> gives <code>2 * (y^(i) - f_β(x^(i))) * (-x_j^{(i)})</code>.</p>
      <p>\[ \frac{\partial C}{\partial \beta_j} = \frac{1}{2n} \sum_{i=1}^{n} 2 \cdot (y^{(i)} - f_\beta(x^{(i)})) \cdot \frac{\partial}{\partial \beta_j}(y^{(i)} - f_\beta(x^{(i)})) \]</p>
      <p>\[ = \frac{1}{2n} \sum_{i=1}^{n} 2 \cdot (y^{(i)} - f_\beta(x^{(i)})) \cdot (-x_j^{(i)}) \]</p>
      <p>\[ = \frac{1}{n} \sum_{i=1}^{n} -(y^{(i)} - f_\beta(x^{(i)})) \cdot x_j^{(i)} \]</p>
      <p>\[ \frac{\partial C}{\partial \beta_j} = \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \]</p>
      <p>This is the average of the errors, each weighted by the corresponding feature value $$x_j^{(i)}$$. (Note: for $$\beta_0$$, you can think of $$x_0^{(i)}$$ as always being 1, so the formula is consistent.)</p>
      <div class="stop-and-think">
          <h3>Stop and Think</h3>
          <h4>Look at the two gradient formulas. What's the main difference between the calculation for β₀ and βⱼ (where j > 0)?</h4>
          <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
          <p id="stop-and-think-1" style="display: none;">The gradient for βⱼ includes an extra multiplication by the feature value xⱼ⁽ⁱ⁾ for each data point. The gradient for β₀ doesn't have this (or you can think of it as being multiplied by x₀⁽ⁱ⁾ which is always 1).</p>
      </div>
      <div class="continue-button" onclick="showNextSection(13)">Continue</div>
  </section>

  <section id="section13">
      <h2>The Update Rules in Action!</h2>
      <p>Phew! Now that we have the specific gradient expressions for each parameter $$\beta_j$$ in our linear regression model, we can write down the explicit update rules for Gradient Descent.</p>
      <p>Remember, the general rule is: Parameter_new = Parameter_old - LearningRate * Gradient_of_Parameter.</p>
      <div class="continue-button" onclick="showNextSection(14)">Continue</div>
  </section>

  <section id="section14">
      <p>So, for each iteration of Gradient Descent, we'll update $$\beta_0$$ and all other $$\beta_j$$s (for $$j=1, ..., m$$) simultaneously like this:</p>
      <p>\[ \text{Repeat until convergence } \{ \]</p>
      <p>\[ \quad \beta_0 := \beta_0 - \alpha \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)}) \]</p>
      <p>\[ \quad \beta_j := \beta_j - \alpha \frac{1}{n} \sum_{i=1}^{n} (f_\beta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \quad \text{ (for } j=1, ..., m) \]</p>
      <p>\[ \} \]</p>
      <p>A crucial point: when performing these updates in an iteration, you should calculate all the gradient terms $$\frac{\partial C}{\partial \beta_j}$$ using the <em>current</em> $$\beta$$ values from the <em>start</em> of that iteration. Then, update all $$\beta$$ values together. If you update $$\beta_0$$ and then immediately use that <em>new</em> $$\beta_0$$ to calculate the gradient for $$\beta_1$$ within the same iteration, it's not the standard 'batch' gradient descent. Often, we calculate all <code>temp_β</code> values first, then assign them back to the actual <code>β</code> values.</p>
      <div class="continue-button" onclick="showNextSection(15)">Continue</div>
  </section>

  <section id="section15">
      <p><strong>Let's walk through a mini-example calculation (one iteration):</strong></p>
      <p>Suppose we have a very simple dataset with just one feature $$x_1$$ and two data points (n=2):</p>
      <ul>
          <li>Point 1: $$x_1^{(1)} = 2, y^{(1)} = 5$$</li>
          <li>Point 2: $$x_1^{(2)} = 4, y^{(2)} = 9$$</li>
      </ul>
      <p>Let's say our current parameters are: $$\beta_0 = 1, \beta_1 = 1.5$$, and our learning rate $$\alpha = 0.1$$.</p>
      <p>Our hypothesis is $$f_\beta(x) = \beta_0 + \beta_1 x_1$$.</p>
      <div class="continue-button" onclick="showNextSection(16)">Continue</div>
  </section>

  <section id="section16">
      <p><strong>Step 1: Calculate predictions and errors for each point.</strong></p>
      <ul>
          <li>For point 1 (i=1):
              <ul>
                  <li>Prediction: $$f_\beta(x^{(1)}) = 1 + 1.5 \cdot 2 = 1 + 3 = 4$$</li>
                  <li>Error term $$(f_\beta(x^{(1)}) - y^{(1)})$$: $$4 - 5 = -1$$</li>
              </ul>
          </li>
          <li>For point 2 (i=2):
              <ul>
                  <li>Prediction: $$f_\beta(x^{(2)}) = 1 + 1.5 \cdot 4 = 1 + 6 = 7$$</li>
                  <li>Error term $$(f_\beta(x^{(2)}) - y^{(2)})$$: $$7 - 9 = -2$$</li>
              </ul>
          </li>
      </ul>
      <div class="continue-button" onclick="showNextSection(17)">Continue</div>
  </section>

  <section id="section17">
      <p><strong>Step 2: Calculate the gradients.</strong> (Remember n=2)</p>
      <ul>
          <li>For $$\beta_0$$: $$\frac{\partial C}{\partial \beta_0} = \frac{1}{2} [ (f_\beta(x^{(1)}) - y^{(1)}) + (f_\beta(x^{(2)}) - y^{(2)}) ] = \frac{1}{2} [(-1) + (-2)] = \frac{1}{2} [-3] = -1.5$$</li>
          <li>For $$\beta_1$$: $$\frac{\partial C}{\partial \beta_1} = \frac{1}{2} [ (f_\beta(x^{(1)}) - y^{(1)})x_1^{(1)} + (f_\beta(x^{(2)}) - y^{(2)})x_1^{(2)} ] = \frac{1}{2} [(-1)(2) + (-2)(4)] = \frac{1}{2} [-2 - 8] = \frac{1}{2} [-10] = -5$$</li>
      </ul>
      <div class="continue-button" onclick="showNextSection(18)">Continue</div>
  </section>

  <section id="section18">
      <p><strong>Step 3: Update the parameters.</strong></p>
      <ul>
          <li>New $$\beta_0 = \beta_0 - \alpha \cdot \frac{\partial C}{\partial \beta_0} = 1 - 0.1 \cdot (-1.5) = 1 + 0.15 = 1.15$$</li>
          <li>New $$\beta_1 = \beta_1 - \alpha \cdot \frac{\partial C}{\partial \beta_1} = 1.5 - 0.1 \cdot (-5) = 1.5 + 0.5 = 2.0$$</li>
      </ul>
      <p>So, after one iteration, our new parameters are $$\beta_0 = 1.15$$ and $$\beta_1 = 2.0$$. Our line has shifted slightly to better fit the data!</p>
      <div class="interactive-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A 'Calculation Check' widget. It shows the example data points, initial βs, and α. It has fields for the user to input their calculated predictions (f(x1), f(x2)), errors, gradients (∂C/∂β₀, ∂C/∂β₁), and finally the new β₀ and β₁. When the user clicks a 'Check My Work' button, it validates their inputs against the correct values derived in the text and provides feedback (e.g., green checkmark or red X with hint).">
      </div>
      <p>Try calculating these yourself to solidify your understanding!</p>
      <div class="test-your-knowledge">
          <h3>Test Your Knowledge</h3>
          <h4>In the gradient calculation for β₁, why is each error term `(f_β(x^(i)) - y^(i))` multiplied by `x₁^(i)`?</h4>
          <div id="options">
              <div class="option" onclick="checkAnswer(this, false)">It's just a mathematical convention without a deeper meaning.</div>
              <div class="option" onclick="checkAnswer(this, true)">Because β₁ is the coefficient for the feature x₁, and the chain rule of differentiation brings down x₁^(i).</div>
              <div class="option" onclick="checkAnswer(this, false)">To make the gradient larger and speed up convergence.</div>
          </div>
          <div id="explanation-1" class="option-explanation">Not quite! This term arises directly from the chain rule when differentiating the squared error with respect to β₁.</div>
          <div id="explanation-2" class="option-explanation">Exactly! When you differentiate `(y^(i) - (β₀ + β₁x₁^(i) + ...))^2` with respect to `β₁`, the `x₁^(i)` term acts as a 'coefficient' to `β₁` inside the squared term, and it comes out due to the chain rule.</div>
          <div id="explanation-3" class="option-explanation">While it affects the magnitude of the gradient, its presence is a direct result of the mathematical derivation, not an arbitrary choice for speed.</div>
      </div>
      <div class="continue-button" onclick="showNextSection(19)">Continue</div>
  </section>

  <section id="section19">
      <h2>Practice Makes Perfect!</h2>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="The cartoon Einstein-like character from Slide 3 with the speech bubble 'Exercise: Calculate the second iteration of our example and update the weights according to the gradients and learning rate.'">
      </div>
      <p>The slides for this course (Slide 3) include an exercise: 'Calculate the second iteration of our example and update the weights according to the gradients and learning rate.' This is a fantastic way to get hands-on with these formulas.</p>
      <p>To do that, you would take the new parameters we just calculated ($$\beta_0 = 1.15, \beta_1 = 2.0$$) as your 'old' parameters for the next iteration, and repeat the whole process: calculate new predictions, new errors, new gradients, and then new updated parameters. Keep doing this, and your parameters will slowly converge to values that minimize the Mean Squared Error!</p>
      <div class="continue-button" onclick="showNextSection(20)">Continue</div>
  </section>

  <section id="section20">
      <h2>Wrapping Up Lesson 2</h2>
      <p>Great job! You've now seen exactly how to tailor the general Gradient Descent algorithm for Linear Regression.</p>
      <p><strong>Key takeaways from this lesson:</strong></p>
      <ul>
          <li><strong>Linear Regression Model:</strong> $$f_\beta(x) = \beta_0 + \beta_1 x_1 + ... + \beta_m x_m$$</li>
          <li><strong>Cost Function (MSE):</strong> $$C(\beta) = \frac{1}{2n} \sum (y^{(i)} - f_\beta(x^{(i)}))^2$$, which measures how far off our predictions are.</li>
          <li><strong>Gradients:</strong> We derived the specific formulas for $$\frac{\partial C}{\partial \beta_0}$$ and $$\frac{\partial C}{\partial \beta_j}$$, which tell us how to adjust each parameter to reduce the cost.</li>
          <li><strong>Update Rules:</strong> We plugged these gradients into the Gradient Descent formula to get the iterative update steps for $$\beta_0$$ and $$\beta_j$$.</li>
      </ul>
      <p>This process of defining a model, a cost function, and then using Gradient Descent to find the best parameters is a cornerstone of many machine learning algorithms.</p>
      <p>In our next lesson, we'll take a step back to look at the Gradient Descent algorithm more formally, discuss some important practical details like when to stop iterating, and explore what happens when our cost function isn't a nice simple bowl shape. This will lead us to the interesting topic of local versus global minima. Stay tuned!</p>
  </section>

  <script>
      // Show the first section initially
      document.getElementById("section1").style.display = "block";
      document.getElementById("section1").style.opacity = "1";

      function showNextSection(nextSectionId) {
          const currentButton = event.target;
          const nextSection = document.getElementById("section" + nextSectionId);
          
          currentButton.style.display = "none";
          
          nextSection.style.display = "block";
          setTimeout(() => {
              nextSection.style.opacity = "1";
          }, 10);

          setTimeout(() => {
              nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }, 500);
      }

      function revealAnswer(id) {
          const revealText = document.getElementById(id);
          const revealButton = event.target;
          
          revealText.style.display = "block";
          revealButton.style.display = "none";
      }

      function checkAnswer(option, isCorrect) {
          // Reset all options
          const options = document.querySelectorAll('.option');
          options.forEach(opt => {
              opt.classList.remove('correct', 'incorrect');
          });
          
          // Hide all explanations
          const explanations = document.querySelectorAll('.option-explanation');
          explanations.forEach(exp => {
              exp.style.display = 'none';
          });
          
          // Mark the selected option
          if (isCorrect) {
              option.classList.add('correct');
              document.getElementById('explanation-2').style.display = 'block';
          } else {
              option.classList.add('incorrect');
              const index = Array.from(options).indexOf(option) + 1;
              document.getElementById('explanation-' + index).style.display = 'block';
          }
      }
  </script>
</body>
</html>