<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<link rel="stylesheet" href="../../styles/lesson.css">
<title>Refining the Pose (Top-Down Architectures)</title>
<script>
window.MathJax = {
    tex: { inlineMath: [['\\(','\\)'], ['$', '$']] },
    options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
<div class="progress-container"><div class="progress-bar" id="progressBar"></div></div>
<div class="lesson-container">

<section id="section1" class="visible">
        <div class="image-placeholder">
        <img src="images/1.jpg" alt="Comparison of early-stage pose prediction vs refined output correcting limb positions." loading="lazy">
    </div>
    <h1>Refining the Pose (Top-Down Architectures)</h1>
    <h2>Inside the Bounding Box</h2>
    <p>In the last lesson, we compared Top-Down and Bottom-Up strategies. We learned that Top-Down methods act like a sniper: first, they isolate the person using an object detector, and then they zoom in to find the skeleton.</p>

    <p>But what exactly happens inside that zoomed-in bounding box? It’s not always a one-shot guess. In this lesson, we are going to look at two heavyweights of the Top-Down world: <strong>Convolutional Pose Machines (CPM)</strong> and <strong>Stacked Hourglass Networks</strong>.</p>
    <div class="continue-button" onclick="showNextSection(2)">Continue</div>
</section>

<section id="section2">
    <p>Early pose estimation models tried to guess the joints immediately. But look at the image above—without context, a model might think an elbow is a knee, or twist an arm backward.</p>
    <div class="continue-button" onclick="showNextSection(3)">Continue</div>
</section>

<section id="section3">
    <p>To fix this, we need <strong>Refinement</strong>. This is the core philosophy of the Convolutional Pose Machine (CPM).</p>
    <div class="continue-button" onclick="showNextSection(4)">Continue</div>
</section>

<section id="section4">
    <h2>Convolutional Pose Machines (CPM)</h2>
    <p>The CPM doesn't just look at the image once. It operates in <strong>stages</strong>.</p>
    <p>Imagine you are trying to draw a stick figure of a friend while blinking rapidly:</p>
    <ul>
        <li><strong>Stage 1:</strong> You get a quick glimpse and draw a rough sketch. The arm is a bit off.</li>
        <li><strong>Stage 2:</strong> You open your eyes again, look at your drawing, and look at your friend. You realize the arm is wrong, so you erase and redraw it based on the body's position.</li>
        <li><strong>Stage 3:</strong> You refine the position of the wrist perfectly.</li>
    </ul>
    <div class="continue-button" onclick="showNextSection(5)">Continue</div>
</section>

<section id="section5">
    <p>Mathematically, the CPM works similarly. At stage \(t\), the network takes two things as input:</p>
    <ol>
        <li>The image features.</li>
        <li>The <strong>Belief Maps</strong> predicted by the previous stage (\(t-1\)).</li>
    </ol>
    <br>
    <p>This allows the network to learn spatial context. If Stage 1 finds a strong signal for a 'Shoulder' but a weak signal for an 'Elbow', Stage 2 uses that Shoulder location to guess where the Elbow <em>should</em> be.</p>

    <div class="continue-button" onclick="showNextSection(6)">Continue</div>
</section>

<section id="section6">
    <h2>The Receptive Field Problem</h2>
    <p>Here is the biggest headache in pose estimation: To identify a small part (like a wrist), you often need to see the big picture (the shoulder and body).</p>
    <p>If you look at the world through a thin straw, you have a <strong>small receptive field</strong>. You might see skin texture, but you won't know if it's an arm or a leg. You need to drop the straw and look through a wide window.</p>
    <div class="continue-button" onclick="showNextSection(7)">Continue</div>
</section>

<section id="section7">
    <p>CPM solves this 'straw' problem in two aggressive ways:</p>
    <ul>
        <li><strong>Aggressive Pooling:</strong> It shrinks the image resolution rapidly. If you shrink a \(256 \times 256\) image down to \(32 \times 32\), a single pixel in the small map represents a huge chunk of the original image.</li>
        <li><strong>Large Kernels:</strong> While most modern networks use tiny \(3 \times 3\) filters, CPM uses massive \(9 \times 9\) or even \(11 \times 11\) filters. This physically forces the network to look at a wider area of pixels at once.</li>
    </ul>
    <div class="continue-button" onclick="showNextSection(8)">Continue</div>
</section>

<section id="section8">
    <p>However, stacking all these stages makes the network incredibly deep. Deep networks suffer from the <strong>Vanishing Gradient</strong> problem—as the training signal travels back from the end to the start, it gets weaker and weaker, until the early layers stop learning.</p>
    
    <div class="check-your-knowledge">
        <h3>Stop And Think</h3>
        <h4>CPM has a clever trick to solve Vanishing Gradients. It adds a loss function after <em>every</em> stage, not just the final one. Why does this help?</h4>
        <div id="cuy-gradient-answer" style="display:none;" class="animate-in">
            <strong>Answer:</strong> This is called <strong>Intermediate Supervision</strong>. By calculating the error at Stage 1, Stage 2, etc., and backpropagating from <em>each</em> point, we inject a fresh, strong gradient signal directly into the middle and early parts of the network. It keeps the early layers 'awake' and learning.
        </div>
        <button class="reveal-button" onclick="revealAnswer('cuy-gradient-answer')">Reveal Answer</button>
    </div>

    <div class="vocab-section">
        <h3>Build Your Vocab</h3>
        <h4>Intermediate Supervision</h4>
        <p>A training technique where loss is calculated and backpropagated at multiple points within a deep neural network, rather than just at the output, to facilitate the training of earlier layers.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(9)">Continue</div>
</section>

<section id="section9">
    <h2>Stacked Hourglass Networks</h2>
    <p>While CPM uses brute force (large filters) to see the context, the <strong>Stacked Hourglass Network</strong> uses structure.</p>
    <p>The goal is the same: capture features at <strong>all scales</strong>. We need low resolution to understand the full body pose (global context) and high resolution to pinpoint exactly where the pixel of the wrist is (local precision).</p>
pl
    <div class="continue-button" onclick="showNextSection(10)">Continue</div>
</section>

<section id="section10">
    <p>The 'Hourglass' module looks like a bowtie or... well, an hourglass:</p>
    <div class="interactive-container" style="width: 100%; margin: 2rem 0; background: #f8fafc; border-radius: 12px; border: 1px solid #e2e8f0; overflow: hidden; box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);">
      <div style="padding: 15px; background: #fff; border-bottom: 1px solid #e2e8f0;">
          <h4 style="margin: 0; color: #2d3748; font-size: 1.1rem;">Visualizing the Hourglass & Skip Connections</h4>
          <p style="margin: 5px 0 0 0; color: #718096; font-size: 0.9rem;">Watch how high-res details (Skip Connections) merge with global context.</p>
      </div>
      
      <div style="position: relative; width: 100%; height: 300px; background: #ffffff;">
          <canvas id="hourglassCanvas" style="display: block; width: 100%; height: 100%;"></canvas>
          <div id="diagramLabel" style="position: absolute; bottom: 20px; left: 50%; transform: translateX(-50%); background: rgba(255, 255, 255, 0.9); padding: 8px 16px; border-radius: 20px; font-weight: 600; color: #4a5568; box-shadow: 0 2px 10px rgba(0,0,0,0.1); border: 1px solid #cbd5e1; white-space: nowrap; transition: all 0.2s ease;">Initializing...</div>
      </div>
  </div>
  
  <script>
  (function() {
      const canvas = document.getElementById('hourglassCanvas');
      const ctx = canvas.getContext('2d');
      const labelDiv = document.getElementById('diagramLabel');
      const section = canvas.closest('section');
      
      let width = 0, height = 0;
      let animationProgress = 0; // 0 to 1
      const speed = 0.005; // Animation speed
  
      // Configuration for the 7 layers (Nodes)
      // xPercent: horizontal position (0-1)
      // heightPercent: vertical size of the block (0-1)
      // type: 'enc' (encoder), 'bot' (bottleneck), 'dec' (decoder)
      const layers = [
          { x: 0.1, h: 0.8, color: '#fc8181', type: 'enc', id: 0 }, // Encoder 1 (Start)
          { x: 0.25, h: 0.5, color: '#f56565', type: 'enc', id: 1 }, // Encoder 2
          { x: 0.4, h: 0.25, color: '#e53e3e', type: 'enc', id: 2 }, // Encoder 3
          { x: 0.5, h: 0.15, color: '#9b2c2c', type: 'bot', id: 3 }, // Bottleneck
          { x: 0.6, h: 0.25, color: '#4299e1', type: 'dec', id: 4, skipFrom: 2 }, // Decoder 1
          { x: 0.75, h: 0.5, color: '#63b3ed', type: 'dec', id: 5, skipFrom: 1 }, // Decoder 2
          { x: 0.9, h: 0.8, color: '#90cdf4', type: 'dec', id: 6, skipFrom: 0 }  // Decoder 3 (End)
      ];
  
      function syncCanvasSize(force = false) {
          const parent = canvas.parentElement;
          if (!parent) return false;
          const nextWidth = parent.clientWidth;
          const nextHeight = parent.clientHeight;
          if (!nextWidth || !nextHeight) return false;
          if (!force && nextWidth === width && nextHeight === height) return true;
          canvas.width = nextWidth;
          canvas.height = nextHeight;
          width = nextWidth;
          height = nextHeight;
          return true;
      }
  
      window.addEventListener('resize', () => syncCanvasSize(true));
      syncCanvasSize();
  
      if (section && 'MutationObserver' in window) {
          const observer = new MutationObserver(() => {
              if (section.classList.contains('visible')) {
                  syncCanvasSize(true);
              }
          });
          observer.observe(section, { attributes: true, attributeFilter: ['class'] });
      }
  
      function getLayerRect(layer) {
          const w = width * 0.08; // Fixed width for blocks
          const h = height * 0.7 * layer.h;
          const x = width * layer.x - w/2;
          const y = (height - h) / 2;
          return { x, y, w, h, cx: x + w/2, cy: y + h/2 };
      }
  
      function drawDashedArrow(fromX, fromY, toX, toY) {
          ctx.beginPath();
          ctx.setLineDash([5, 5]);
          ctx.strokeStyle = '#cbd5e1';
          ctx.lineWidth = 2;
          ctx.moveTo(fromX, fromY);
          ctx.lineTo(toX, toY);
          ctx.stroke();
          ctx.setLineDash([]);
      }
  
      function drawSolidLine(fromX, fromY, toX, toY, active) {
          ctx.beginPath();
          ctx.strokeStyle = active ? '#2d3748' : '#e2e8f0';
          ctx.lineWidth = active ? 3 : 2;
          ctx.moveTo(fromX, fromY);
          ctx.lineTo(toX, toY);
          ctx.stroke();
      }
  
      function drawBlock(rect, color, label, isHighlight) {
          ctx.fillStyle = color;
          
          // Glow effect if highlighted
          if (isHighlight) {
              ctx.shadowBlur = 15;
              ctx.shadowColor = color;
          } else {
              ctx.shadowBlur = 0;
          }
  
          // Draw rounded rect
          const r = 4;
          ctx.beginPath();
          ctx.roundRect(rect.x, rect.y, rect.w, rect.h, r);
          ctx.fill();
          ctx.shadowBlur = 0; // Reset
  
          // Border
          ctx.strokeStyle = isHighlight ? '#fff' : 'rgba(0,0,0,0.1)';
          ctx.lineWidth = 2;
          ctx.stroke();
      }
  
      function updateLabel(prog) {
          if (prog < 0.4) {
              labelDiv.textContent = "⬇️ Encoder: Reducing Resolution, Extracting Features";
              labelDiv.style.color = "#c53030";
          } else if (prog >= 0.4 && prog < 0.55) {
              labelDiv.textContent = "🛑 Bottleneck: Global Context Captured";
              labelDiv.style.color = "#742a2a";
          } else {
              labelDiv.textContent = "⚡ Decoder + Skips: Upsampling + Injecting Details";
              labelDiv.style.color = "#2b6cb0";
          }
      }
  
      function animate() {
          if (!width || !height) {
              if (!syncCanvasSize()) {
                  requestAnimationFrame(animate);
                  return;
              }
          }
  
          ctx.clearRect(0, 0, width, height);
  
          // 1. Draw Static Connections (Background)
          // Main path
          for (let i = 0; i < layers.length - 1; i++) {
              const start = getLayerRect(layers[i]);
              const end = getLayerRect(layers[i+1]);
              drawSolidLine(start.cx, start.cy, end.cx, end.cy, false);
          }
          // Skip connections
          layers.forEach(l => {
              if (l.skipFrom !== undefined) {
                  const start = getLayerRect(layers[l.skipFrom]);
                  const end = getLayerRect(l);
                  drawDashedArrow(start.cx, start.cy, end.cx, end.cy);
              }
          });
  
          // 2. Draw Blocks
          layers.forEach((l, index) => {
              // Highlight block if animation is passing through
              // There are 6 segments between 7 layers. 
              // Segment i is between layer i and i+1.
              // Progress maps 0..1 to 0..6
              const scaledP = animationProgress * (layers.length - 1);
              const isCurrent = Math.abs(scaledP - index) < 0.3;
              
              const rect = getLayerRect(l);
              drawBlock(rect, l.color, "", isCurrent);
          });
  
          // 3. Animation Logic (Main Particle)
          const totalSegments = layers.length - 1;
          const currentSegment = Math.floor(animationProgress * totalSegments);
          const segmentProgress = (animationProgress * totalSegments) - currentSegment;
  
          if (currentSegment < totalSegments) {
              const startNode = getLayerRect(layers[currentSegment]);
              const endNode = getLayerRect(layers[currentSegment + 1]);
  
              const curX = startNode.cx + (endNode.cx - startNode.cx) * segmentProgress;
              const curY = startNode.cy + (endNode.cy - startNode.cy) * segmentProgress;
  
              // Draw Main Particle
              ctx.beginPath();
              ctx.arc(curX, curY, 6, 0, Math.PI * 2);
              ctx.fillStyle = '#fff';
              ctx.fill();
              ctx.strokeStyle = '#2d3748';
              ctx.stroke();
          }
  
          // 4. Animation Logic (Skip Connection Particles)
          // We want skip particles to travel from Encoder -> Decoder
          // They should arrive at the Decoder roughly when the Main Particle arrives there.
          // Skip 0 (L0 -> L6): Should fire when Main is around L3/L4 so it arrives at L6.
          // Actually, let's make it simpler: Skip particle travels proportional to main progress.
          
          layers.forEach(destLayer => {
              if (destLayer.skipFrom !== undefined) {
                  const srcIndex = destLayer.skipFrom;
                  const destIndex = destLayer.id;
                  
                  // Determine when this skip should be active
                  // It should start moving when main particle passes srcIndex
                  // It should arrive when main particle reaches destIndex
                  
                  const startP = srcIndex / totalSegments;
                  const endP = destIndex / totalSegments;
                  
                  if (animationProgress > startP && animationProgress < endP) {
                      const localP = (animationProgress - startP) / (endP - startP);
                      
                      const srcRect = getLayerRect(layers[srcIndex]);
                      const destRect = getLayerRect(destLayer);
                      
                      const skipX = srcRect.cx + (destRect.cx - srcRect.cx) * localP;
                      
                      // Draw Skip Particle
                      ctx.beginPath();
                      ctx.arc(skipX, srcRect.cy, 5, 0, Math.PI * 2); // Moves horizontally
                      ctx.fillStyle = '#ecc94b'; // Gold
                      ctx.fill();
                      ctx.strokeStyle = '#b7791f';
                      ctx.stroke();
                      
                      // Little text "Details"
                      if (localP > 0.4 && localP < 0.6) {
                          ctx.fillStyle = '#744210';
                          ctx.font = '10px sans-serif';
                          ctx.fillText("Details", skipX - 15, srcRect.cy - 10);
                      }
                  }
              }
          });
  
          updateLabel(animationProgress);
  
          // Loop
          animationProgress += speed;
          if (animationProgress > 1) {
              animationProgress = 0;
          }
  
          requestAnimationFrame(animate);
      }
  
      animate();
  })();
  </script>
    <div class="continue-button" onclick="showNextSection(11)">Continue</div>
</section>

<section id="section11">
    <h3>1. The Encoder (Downsampling)</h3>
    <p>Max pooling reduces resolution, forcing the network to summarize the global shape of the person.</p>
    
    <div class="test-your-knowledge">
        <h3>Test Your Knowledge</h3>
        <h4>What is the primary purpose of the 'bottleneck' (the lowest resolution point) in the Hourglass network?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'While it does save memory, that is not the architectural reason for its existence in this context.')">To save computer memory.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct! At low resolution, the network can\'t see fine details, so it is forced to learn the overall relationship between body parts (e.g., legs are usually below torsos).')">To capture global context of the entire body.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'The bottleneck is part of the processing pipeline, not a frame rate control.')">To increase the frame rate of the video.</div>
        </div>
    </div>
    
    <div class="continue-button" id="continue-s11" onclick="showNextSection(12)" style="display: none;">Continue</div>
</section>

<section id="section12">
    <h3>2. The Decoder & Skip Connections</h3>
    <p><strong>The Decoder (Upsampling):</strong> We need to get back to the original image size to create a heatmap. We use nearest-neighbor upsampling.</p>
    <p><strong>Skip Connections:</strong> This is the magic sauce. When we crushed the image down to the bottleneck, we lost the fine details (like exactly which pixel is the elbow). Skip connections take the high-res details from the Encoder path and add them back to the Decoder path. This gives us the best of both worlds: Global Context + Local Detail.</p>
    <p>Just like CPM, we 'Stack' these hourglasses. The output of one hourglass becomes the input to the next, allowing for repeated refinement.</p>
    <div class="continue-button" onclick="showNextSection(13)">Continue</div>
</section>

<section id="section13">
    <h2>Review and Reflect</h2>
    <p>We've explored the engine room of Top-Down pose estimation. Both CPM and Stacked Hourglass Networks solve the same fundamental trade-off: <strong>Context vs. Precision</strong>.</p>
    <ul>
        <li><strong>CPM</strong> does it with sequential stages, large kernels, and intermediate supervision.</li>
        <li><strong>Hourglass</strong> does it with an Encoder-Decoder structure and skip connections.</li>
    </ul>
    <p>These models are incredibly accurate for single-person tasks. But remember our discussion in Lesson 2? Top-Down methods struggle when the scene is crowded because they have to run these heavy networks for <em>every single person</em> detected.</p>
    <div class="continue-button" onclick="showNextSection(14)">Time to test your architectural knowledge!</div>
</section>

<section id="section14">
    <div class="test-your-knowledge">
        <h3>Check Point 1: CPM</h3>
        <h4>How does the Convolutional Pose Machine (CPM) primarily increase its receptive field to see more context?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Actually, CPM is famous for doing the opposite compared to VGG-style networks.')">By using many small 3x3 filters.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct! Large kernels (like 9x9) and frequent pooling allow each neuron to see a much larger portion of the original image.')">By using aggressive pooling and large kernel sizes.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Skip connections are the hallmark of the Hourglass network (and ResNets), not the primary receptive field mechanism of CPM.')">By using skip connections.</div>
        </div>
    </div>
    <div class="continue-button" id="continue-s14" onclick="showNextSection(15)" style="display: none;">Continue</div>
</section>

<section id="section15">
    <div class="test-your-knowledge">
        <h3>Check Point 2: Hourglass</h3>
        <h4>What is the structural shape of a Stacked Hourglass module?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'That would be a standard CNN. Hourglass implies a change in width (resolution).')">A straight feed-forward line.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Spot on. It narrows down to a bottleneck and widens back up, resembling an hourglass.')">Symmetrical Encoder-Decoder (Downsample-Upsample).</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Recurrent Neural Networks (RNNs) loop, but Hourglass networks are spatial, not temporal loops.')">A circular loop.</div>
        </div>
    </div>
    <div class="continue-button" id="continue-s15" onclick="showNextSection(16)" style="display: none;">Continue</div>
</section>

<section id="section16">
    <div class="test-your-knowledge">
        <h3>Check Point 3: Training</h3>
        <h4>Why is Intermediate Supervision used in these deep networks?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Exactly. In very deep networks, the training signal can fade away before it reaches the early layers. Adding loss functions in the middle refreshes the signal.')">To prevent the Vanishing Gradient problem.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Calculating loss at multiple stages actually adds a tiny bit of computation during training, though it makes the training process more effective.')">To make the model run faster.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'You definitely still need Ground Truth to calculate the loss!')">To eliminate the need for Ground Truth.</div>
        </div>
    </div>
    <br>
    <p>Next up: What happens when 20 people are dancing in a crowd? Running an Hourglass network 20 times is too slow. In the next lesson, we'll learn how <strong>OpenPose</strong> flipped the script with Bottom-Up estimation.</p>
</section>

<button id="markCompletedBtn" class="mark-completed-button" onclick="toggleCompleted()">✓ Mark as Completed</button>
</div>

<script>
let currentSection = 1;
const totalSections = 16;

updateProgress();
if (currentSection === totalSections) {
    const completedButton = document.getElementById('markCompletedBtn');
    if (completedButton) completedButton.classList.add('show');
}

function showNextSection(nextSectionId) {
    const nextSectionElement = document.getElementById(`section${nextSectionId}`);
    const currentButton = event && event.target;
    if (!nextSectionElement) return;
    if (currentButton && currentButton.classList.contains('continue-button')) {
        currentButton.style.display = 'none';
    }
    nextSectionElement.classList.add('visible');
    currentSection = nextSectionId;
    updateProgress();
    if (currentSection === totalSections) {
        const completedButton = document.getElementById('markCompletedBtn');
        if (completedButton) completedButton.classList.add('show');
    }
    setTimeout(() => { nextSectionElement.scrollIntoView({ behavior: 'smooth', block: 'start' }); }, 200);
}

function updateProgress() {
    const progressBar = document.getElementById('progressBar');
    const progress = (currentSection / totalSections) * 100;
    progressBar.style.width = `${progress}%`;
}

function revealAnswer(id) {
    const revealText = document.getElementById(id);
    const revealButton = event && event.target;
    if (revealText) {
        revealText.style.display = "block";
        revealText.classList.add('animate-in');
    }
    if (revealButton) {
        revealButton.style.display = "none";
    }
}

function selectChoice(element, isCorrect, explanation) {
    const choices = element.parentNode.querySelectorAll('.choice-option');
    choices.forEach(choice => {
        choice.classList.remove('selected', 'correct', 'incorrect');
        const existing = choice.querySelector('.choice-explanation');
        if (existing) existing.remove();
    });
    element.classList.add('selected');
    element.classList.add(isCorrect ? 'correct' : 'incorrect');
    const explanationDiv = document.createElement('div');
    explanationDiv.className = 'choice-explanation';
    explanationDiv.style.display = 'block';
    explanationDiv.innerHTML = `<strong>${isCorrect ? 'Correct!' : 'Not quite.'}</strong> ${explanation}`;
    element.appendChild(explanationDiv);
    
    // Only show continue button if answer is correct
    if (!isCorrect) return;
    
    // Logic for revealing continue buttons in quiz sections
    const parentSection = element.closest('section');
    if (parentSection) {
        const sectionId = parentSection.id;
        const continueButton = parentSection.querySelector('.continue-button');
        // Logic for sections 11, 14, 15
        if ((sectionId === 'section11' || sectionId === 'section14' || sectionId === 'section15') && continueButton && continueButton.style.display === 'none') {
            setTimeout(() => {
                continueButton.style.display = 'block';
                continueButton.classList.add('show-with-animation');
            }, 800);
        }
    }
}

document.addEventListener('keydown', function(e) {
    if (e.key === 'ArrowRight' || e.key === ' ') {
        const btn = document.querySelector(`#section${currentSection} .continue-button`);
        if (btn && btn.style.display !== 'none') {
            e.preventDefault();
            btn.click();
        }
    }
});

document.documentElement.style.scrollBehavior = 'smooth';

function toggleCompleted() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    const isCompleted = button.classList.contains('completed');
    if (!isCompleted) {
        // Attempt LMS communication if available
        try {
            if (window.parent && window.parent.ProgressTracker) {
                // Placeholder IDs - in a real app these would be dynamic
                let courseId = 'computer-vision';
                let pathId = 'pose-estimation';
                let moduleId = 'cv-ch22-m1-top-down';
                let lessonId = 'cv-ch22-l2-refining-pose';
                
                window.parent.ProgressTracker.markLessonCompleted(courseId, pathId, moduleId, lessonId);
            }
        } catch (error) {
            console.error('Error with ProgressTracker:', error);
        }
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
        triggerCelebration();
        localStorage.setItem('lesson_cv-ch22-l2_completed', 'true');
    }
}

function triggerCelebration() {
    createConfetti();
    showSuccessMessage();
}

function createConfetti() {
    const confettiContainer = document.createElement('div');
    confettiContainer.className = 'confetti-container';
    document.body.appendChild(confettiContainer);
    const emojis = ['🎉', '🎊', '✨', '🌟', '🎈', '🏆', '👏', '🥳'];
    const colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7'];
    for (let i = 0; i < 40; i++) {
        setTimeout(() => {
            const confetti = document.createElement('div');
            confetti.className = 'confetti';
            if (Math.random() > 0.6) {
                confetti.textContent = emojis[Math.floor(Math.random() * emojis.length)];
            } else {
                confetti.innerHTML = '●';
                confetti.style.color = colors[Math.floor(Math.random() * colors.length)];
            }
            confetti.style.left = Math.random() * 100 + '%';
            confetti.style.animationDelay = Math.random() * 2 + 's';
            document.querySelector('.confetti-container').appendChild(confetti);
        }, i * 50);
    }
    setTimeout(() => { if (confettiContainer.parentNode) confettiContainer.parentNode.removeChild(confettiContainer); }, 5000);
}

function showSuccessMessage() {
    const successMessage = document.createElement('div');
    successMessage.className = 'success-message';
    successMessage.innerHTML = '🎉 Lesson Completed! Great Job! 🎉';
    document.body.appendChild(successMessage);
    setTimeout(() => { if (successMessage.parentNode) successMessage.parentNode.removeChild(successMessage); }, 2500);
}

window.addEventListener('load', function() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    const isCompleted = localStorage.getItem('lesson_cv-ch22-l2_completed') === 'true';
    if (isCompleted) {
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
    }
});
</script>
</body>
</html>