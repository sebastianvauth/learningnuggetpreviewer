<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<link rel="stylesheet" href="../../styles/lesson.css">
<title>Evolving Architectures & The Future</title>
<script>
window.MathJax = {
    tex: { inlineMath: [['\\(','\\)'], ['$', '$']] },
    options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
<div class="progress-container"><div class="progress-bar" id="progressBar"></div></div>
<div class="lesson-container">

<section id="section1" class="visible">
    <div class="visual-placeholder">
        <img src="images/1.jpg" alt="Comparison of a slot machine labeled 'Standard GAN' versus a vending machine labeled 'Conditional GAN'." loading="lazy">
    </div>
    <h1>Evolving Architectures & The Future</h1>
    <h2>The Need for Control</h2>
    <p>Standard GANs are wild. You feed them random noise, and they give you a random image. It’s like rolling a die: you might get a '1' (a picture of a cat) or a '6' (a picture of a boat), but you can't choose which one you get.</p>

    <p>But what if we want control? What if you are an artist and you don't just want 'an image,' you want specifically 'a zebra' or 'a winter scene'? To bridge the gap between random creativity and useful tools, we need to evolve the architecture.</p>
    <div class="continue-button" onclick="showNextSection(2)">Continue</div>
</section>

<section id="section2">
    <h2>Commanding the Generator: cGANs</h2>
    <p>The first evolution is the <strong>Conditional GAN (cGAN)</strong>. In the standard framework, the Generator (\(G\)) only takes noise (\(z\)) as input. In a cGAN, we add a condition (\(y\)).</p>
    <div class="continue-button" onclick="showNextSection(3)">Continue</div>
</section>

<section id="section3">
    <p>Ideally, we want to tell the Generator: "Here is some noise for variation, but make sure the output is a valid instance of class \(y\)." We do this by feeding the label \(y\) (like 'digit 7') into both the Generator and the Discriminator.</p>
    <p>Mathematically, the objective function changes slightly. We are no longer just looking at \(D(x)\), but \(D(x|y)\)—the probability that \(x\) is real <em>given</em> that it is supposed to be class \(y\).</p>
    <p>This simple addition allows us to control the output. If we train on handwriting digits, we can send a 'one-hot vector' requesting a specific number.</p>
    <div class="cgan-interactive-container">
      <!-- CSS for this specific component -->
      <div class="cgan-controls">
          <!-- Input Y: Class Label -->
          <div class="control-group">
              <span class="control-label">Condition ($y$): Select Digit</span>
              <div class="digit-grid" id="digitButtons">
                  <button class="digit-btn active" onclick="cgan.setDigit(0)">0</button>
                  <button class="digit-btn" onclick="cgan.setDigit(1)">1</button>
                  <button class="digit-btn" onclick="cgan.setDigit(2)">2</button>
                  <button class="digit-btn" onclick="cgan.setDigit(3)">3</button>
                  <button class="digit-btn" onclick="cgan.setDigit(4)">4</button>
                  <button class="digit-btn" onclick="cgan.setDigit(5)">5</button>
                  <button class="digit-btn" onclick="cgan.setDigit(6)">6</button>
                  <button class="digit-btn" onclick="cgan.setDigit(7)">7</button>
                  <button class="digit-btn" onclick="cgan.setDigit(8)">8</button>
                  <button class="digit-btn" onclick="cgan.setDigit(9)">9</button>
              </div>
          </div>
  
          <!-- Input Z: Noise -->
          <div class="control-group">
              <span class="control-label">Latent Noise ($z$): Variation</span>
              <div class="noise-slider-container">
                  <input type="range" id="noiseSlider" min="0" max="100" value="50" oninput="cgan.updateNoise(this.value)">
              </div>
              <p style="font-size: 0.8rem; color: #64748b; margin-top: 5px; margin-bottom: 0;">Adjusts slant, thickness, and style.</p>
          </div>
      </div>
  
      <div class="cgan-viz">
          <canvas id="cganCanvas" width="400" height="300"></canvas>
          <div class="viz-label">Generator Output (28x28 Upscaled)</div>
      </div>
  
      <!-- Logic -->
      <script>
          class CGANInteractive {
              constructor() {
                  this.canvas = document.getElementById('cganCanvas');
                  this.ctx = this.canvas.getContext('2d');
                  
                  // Offscreen canvas to simulate low-res GAN output
                  this.genCanvas = document.createElement('canvas');
                  this.genCanvas.width = 28;
                  this.genCanvas.height = 28;
                  this.genCtx = this.genCanvas.getContext('2d');
  
                  this.currentDigit = 0;
                  this.noiseVal = 0.5; // 0.0 to 1.0
                  
                  // Animation loop
                  this.time = 0;
                  this.animate = this.animate.bind(this);
                  requestAnimationFrame(this.animate);
              }
  
              setDigit(n) {
                  this.currentDigit = n;
                  // Update buttons UI
                  const buttons = document.querySelectorAll('.digit-btn');
                  buttons.forEach((btn, index) => {
                      if(index === n) btn.classList.add('active');
                      else btn.classList.remove('active');
                  });
              }
  
              updateNoise(val) {
                  this.noiseVal = val / 100;
              }
  
              // The "Generator" - Procedurally draws a digit based on inputs
              generateDigit(ctx, digit, noise) {
                  ctx.clearRect(0, 0, 28, 28);
                  ctx.fillStyle = "black";
                  ctx.fillRect(0, 0, 28, 28);
                  
                  ctx.fillStyle = "white";
                  ctx.strokeStyle = "white";
                  
                  // Map noise to style parameters
                  // Slant: -0.3 (left) to 0.3 (right)
                  const slant = (noise - 0.5) * 0.6; 
                  // Thickness: 1.5 to 3.5
                  const thickness = 1.5 + (noise * 2);
                  // Jitter/Scale: changes size slightly
                  const scale = 0.8 + (Math.sin(noise * 10) * 0.1);
                  
                  ctx.lineWidth = thickness;
                  ctx.lineCap = "round";
                  ctx.lineJoin = "round";
  
                  ctx.save();
                  
                  // Center and apply transformations
                  ctx.translate(14, 14);
                  ctx.scale(scale, scale);
                  ctx.transform(1, 0, slant, 1, 0, 0); // Apply slant (skew)
                  ctx.translate(-14, -14);
  
                  ctx.beginPath();
                  
                  // Drawing logic (The "Weights" of the generator)
                  switch(digit) {
                      case 0:
                          ctx.ellipse(14, 14, 6, 9, 0, 0, Math.PI * 2);
                          break;
                      case 1:
                          ctx.moveTo(14 + (noise*4 - 2), 4);
                          ctx.lineTo(14, 24);
                          if (noise > 0.7) { // Add serif for high noise
                              ctx.moveTo(10, 24); ctx.lineTo(18, 24);
                              ctx.moveTo(10, 5); ctx.lineTo(14, 4);
                          }
                          break;
                      case 2:
                          ctx.moveTo(8, 8);
                          ctx.bezierCurveTo(8, 2, 20, 2, 20, 8);
                          ctx.bezierCurveTo(20, 14, 8, 20, 8, 24);
                          ctx.lineTo(22, 24);
                          break;
                      case 3:
                          ctx.moveTo(9, 6);
                          ctx.lineTo(19, 6);
                          ctx.lineTo(14, 13);
                          ctx.bezierCurveTo(22, 13, 22, 24, 12, 24);
                          break;
                      case 4:
                          ctx.moveTo(20, 20);
                          ctx.lineTo(20, 4);
                          ctx.lineTo(6, 16);
                          ctx.lineTo(24, 16);
                          break;
                      case 5:
                          ctx.moveTo(20, 6);
                          ctx.lineTo(10, 6);
                          ctx.lineTo(8, 12);
                          ctx.bezierCurveTo(24, 10, 22, 26, 8, 22);
                          break;
                      case 6:
                          ctx.arc(14, 18, 6, 0, Math.PI * 2);
                          ctx.moveTo(20, 14);
                          ctx.bezierCurveTo(16, 2, 8, 10, 8, 18);
                          break;
                      case 7:
                          ctx.moveTo(6, 6);
                          ctx.lineTo(22, 6);
                          ctx.lineTo(10 + (noise*6), 24); // Noise changes angle
                          if (noise < 0.3) { // Crossbar for low noise
                              ctx.moveTo(10, 14); ctx.lineTo(18, 14);
                          }
                          break;
                      case 8:
                          ctx.arc(14, 9, 5, 0, Math.PI * 2);
                          ctx.arc(14, 19, 6, 0, Math.PI * 2);
                          break;
                      case 9:
                          ctx.arc(14, 10, 6, 0, Math.PI * 2);
                          ctx.moveTo(20, 10);
                          ctx.bezierCurveTo(20, 18, 16, 24, 10, 24);
                          break;
                  }
                  ctx.stroke();
                  ctx.restore();
              }
  
              drawVisualization() {
                  // Clear main canvas
                  this.ctx.fillStyle = "#1a202c";
                  this.ctx.fillRect(0, 0, 400, 300);
  
                  // 1. Draw Inputs Visualization
                  this.ctx.font = "12px monospace";
                  this.ctx.fillStyle = "#a0aec0";
                  
                  // Draw Label Vector (Y)
                  this.ctx.fillText("Condition (y)", 20, 40);
                  for(let i=0; i<10; i++) {
                      const isActive = (i === this.currentDigit);
                      this.ctx.fillStyle = isActive ? "#48bb78" : "#2d3748";
                      this.ctx.fillRect(20, 50 + (i*14), 10, 10);
                      if(isActive) {
                          this.ctx.strokeStyle = "#48bb78";
                          this.ctx.strokeRect(18, 48 + (i*14), 14, 14);
                      }
                  }
  
                  // Draw Noise Vector (Z)
                  this.ctx.fillStyle = "#a0aec0";
                  this.ctx.fillText("Noise (z)", 20, 220);
                  this.ctx.fillStyle = "#ed8936";
                  // Visualize noise as a changing bar graph
                  for(let i=0; i<5; i++) {
                      // Create a pseudo-random visual based on the slider value
                      const barHeight = 5 + Math.abs(Math.sin(this.noiseVal * (i+1) + this.time)) * 20;
                      this.ctx.fillRect(20 + (i*8), 230, 6, barHeight);
                  }
  
                  // 2. Draw Generator "Black Box"
                  this.ctx.fillStyle = "#2d3748";
                  this.ctx.strokeStyle = "#4a5568";
                  this.ctx.lineWidth = 2;
                  this.ctx.beginPath();
                  this.ctx.roundRect(100, 50, 80, 200, 10);
                  this.ctx.fill();
                  this.ctx.stroke();
                  
                  this.ctx.fillStyle = "white";
                  this.ctx.font = "bold 14px sans-serif";
                  this.ctx.textAlign = "center";
                  this.ctx.fillText("G", 140, 145);
                  this.ctx.font = "10px sans-serif";
                  this.ctx.fillStyle = "#a0aec0";
                  this.ctx.fillText("Generator", 140, 165);
  
                  // Connector Lines
                  this.ctx.strokeStyle = "#4a5568";
                  this.ctx.lineWidth = 1;
                  this.ctx.beginPath();
                  // From Y
                  this.ctx.moveTo(40, 50 + (this.currentDigit * 14) + 5);
                  this.ctx.lineTo(100, 100);
                  // From Z
                  this.ctx.moveTo(60, 240);
                  this.ctx.lineTo(100, 200);
                  // Output Line
                  this.ctx.moveTo(180, 150);
                  this.ctx.lineTo(220, 150);
                  this.ctx.stroke();
  
                  // 3. Generate the digit on offscreen canvas
                  this.generateDigit(this.genCtx, this.currentDigit, this.noiseVal);
  
                  // 4. Draw the Pixelated Output
                  // We want to draw the 28x28 image scaled up to 140x140 at (220, 80)
                  const startX = 220;
                  const startY = 80;
                  const pixelSize = 5;
  
                  const imgData = this.genCtx.getImageData(0, 0, 28, 28);
                  const data = imgData.data;
  
                  for(let y = 0; y < 28; y++) {
                      for(let x = 0; x < 28; x++) {
                          const index = (y * 28 + x) * 4;
                          const r = data[index]; // Black background, white text
                          // Use the red channel (since it's grayscale) for intensity
                          
                          // Only draw if not black
                          if(r > 10) {
                              const intensity = r / 255;
                              // Greenish-blue AI color style
                              this.ctx.fillStyle = `rgba(100, 255, 218, ${intensity})`; 
                              this.ctx.fillRect(startX + x*pixelSize, startY + y*pixelSize, pixelSize - 0.5, pixelSize - 0.5);
                          } else {
                              // Draw faint grid for background
                              this.ctx.fillStyle = "rgba(255,255,255,0.02)";
                              this.ctx.fillRect(startX + x*pixelSize, startY + y*pixelSize, pixelSize - 0.5, pixelSize - 0.5);
                          }
                      }
                  }
              }
  
              animate() {
                  this.time += 0.05;
                  this.drawVisualization();
                  requestAnimationFrame(this.animate);
              }
          }
  
          // Initialize
          const cgan = new CGANInteractive();
      </script>
  </div>
    <div class="vocab-section">
        <h3>Build Your Vocab</h3>
        <h4>Conditional GAN (cGAN)</h4>
        <p>A variation of GAN where both the Generator and Discriminator receive additional information (a class label or data) to condition the generation process, allowing for controlled outputs.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(4)">Continue</div>
</section>

<section id="section4">
    <p>This concept extends beyond simple class labels. What if the 'condition' isn't just a number, but an entire image? This leads us to <strong>Pix2Pix</strong>.</p>
    <p>Pix2Pix is a framework for <strong>Image-to-Image Translation</strong>. It takes an input image (like a black and white sketch) and translates it into an output image (like a color photo).</p>
    <p>However, Pix2Pix has a major limitation: it requires <strong>Paired Data</strong>. To train it to turn sketches into photos, you need a dataset where every single photo has an exact matching sketch.</p>
    <div class="visual-placeholder">
        <img src="images/2.jpg" alt="Diagram illustrating the difference between perfectly paired datasets and unpaired datasets." loading="lazy">
        <p class="image-caption">Pix2Pix thrives only when every sketch has a matching photo—luxury data most teams simply don’t have.</p>
    </div>
    <p>Finding these perfect pairs is often impossible in the real world. This brings us to one of the most clever architectures in computer vision.</p>
    <div class="continue-button" onclick="showNextSection(5)">Continue</div>
</section>

<section id="section5">
    <h2>CycleGAN: The Unpaired Magic</h2>
    <p>Imagine you want to build a filter that turns horses into zebras. You have thousands of photos of horses, and thousands of photos of zebras. But you do not have a photo of 'Mr. Ed the Horse' standing in the <em>exact</em> same pose as 'Marty the Zebra.'</p>
    <div class="continue-button" onclick="showNextSection(6)">Continue</div>
</section>

<section id="section6">
    <p>This is <strong>Unpaired Data</strong>. Standard Pix2Pix fails here because it doesn't know which zebra photo corresponds to which horse photo. <strong>CycleGAN</strong> solves this using the principle of <strong>Cycle Consistency</strong>.</p>
    <p>CycleGAN uses two Generators:</p>
    <ul>
        <li>\(G: X \to Y\) (Horse to Zebra)</li>
        <li>\(F: Y \to X\) (Zebra to Horse)</li>
    </ul>
    <p>The logic is simple: If I translate a sentence from English to French, and then translate it back to English, I should arrive at the original sentence. Similarly, if we turn a Horse image (\(x\)) into a Zebra (\(G(x)\)), and then feed that Zebra into the second generator (\(F(G(x))\)), we should get the original Horse back.</p>
    <p>Mathematically, we minimize the <strong>Cycle Consistency Loss</strong>:</p>
    <p>$$ L_{cycle} = || F(G(x)) - x ||_1 $$</p>
    <div class="cyclegan-interactive-container">
      <div class="cycle-controls">
          <button class="cycle-btn active" onclick="cycleApp.setMode('animals')">Horse ↔ Zebra</button>
          <button class="cycle-btn" onclick="cycleApp.setMode('seasons')">Summer ↔ Winter</button>
      </div>
  
      <div class="canvas-wrapper">
          <canvas id="cycleCanvas" width="600" height="380"></canvas>
      </div>
      
      <div class="cycle-legend">
          <div class="legend-item"><div class="dot" style="background:#667eea"></div>Data Flow</div>
          <div class="legend-item"><div class="dot" style="background:#f56565"></div>Cycle Consistency Loss</div>
      </div>
  
      <script>
          class CycleGANViz {
              constructor() {
                  this.canvas = document.getElementById('cycleCanvas');
                  this.ctx = this.canvas.getContext('2d');
                  this.mode = 'animals'; // 'animals' or 'seasons'
                  this.width = 600;
                  this.height = 380;
                  this.time = 0;
                  
                  // Animation loop
                  this.animate = this.animate.bind(this);
                  requestAnimationFrame(this.animate);
              }
  
              setMode(newMode) {
                  this.mode = newMode;
                  // Update UI buttons
                  const buttons = document.querySelectorAll('.cycle-btn');
                  buttons.forEach(btn => {
                      if(btn.textContent.includes(newMode === 'animals' ? 'Horse' : 'Summer')) {
                          btn.classList.add('active');
                      } else {
                          btn.classList.remove('active');
                      }
                  });
              }
  
              // --- Drawing Helpers ---
  
              drawBox(x, y, label, sublabel) {
                  const w = 100;
                  const h = 60;
                  
                  // Shadow
                  this.ctx.fillStyle = "rgba(0,0,0,0.1)";
                  this.ctx.beginPath();
                  this.ctx.roundRect(x+4, y+4, w, h, 8);
                  this.ctx.fill();
  
                  // Box
                  const grad = this.ctx.createLinearGradient(x, y, x+w, y+h);
                  grad.addColorStop(0, "#2d3748");
                  grad.addColorStop(1, "#1a202c");
                  this.ctx.fillStyle = grad;
                  this.ctx.beginPath();
                  this.ctx.roundRect(x, y, w, h, 8);
                  this.ctx.fill();
  
                  // Text
                  this.ctx.fillStyle = "#fff";
                  this.ctx.font = "bold 16px sans-serif";
                  this.ctx.textAlign = "center";
                  this.ctx.fillText(label, x + w/2, y + h/2 + 5);
                  
                  if(sublabel) {
                      this.ctx.fillStyle = "#a0aec0";
                      this.ctx.font = "10px sans-serif";
                      this.ctx.fillText(sublabel, x + w/2, y + h/2 + 20);
                  }
              }
  
              drawHorse(x, y, scale=1, isZebra=false) {
                  this.ctx.save();
                  this.ctx.translate(x, y);
                  this.ctx.scale(scale, scale);
                  
                  // Body color
                  this.ctx.fillStyle = isZebra ? "#fff" : "#8B4513";
                  this.ctx.strokeStyle = "#000";
                  
                  // Draw Body shape
                  this.ctx.beginPath();
                  this.ctx.moveTo(0, 0);
                  this.ctx.lineTo(30, 0); // back
                  this.ctx.lineTo(35, 10); // neck base
                  this.ctx.lineTo(40, -10); // head top
                  this.ctx.lineTo(45, -5); // nose
                  this.ctx.lineTo(40, 15); // neck front
                  this.ctx.lineTo(35, 30); // front leg
                  this.ctx.lineTo(30, 30); 
                  this.ctx.lineTo(30, 20); // belly
                  this.ctx.lineTo(5, 20); 
                  this.ctx.lineTo(5, 30); // back leg
                  this.ctx.lineTo(0, 30);
                  this.ctx.closePath();
                  this.ctx.fill();
                  this.ctx.stroke();
  
                  // Stripes if Zebra
                  if(isZebra) {
                      this.ctx.strokeStyle = "#000";
                      this.ctx.lineWidth = 2;
                      this.ctx.beginPath();
                      this.ctx.moveTo(5, 0); this.ctx.lineTo(5, 20);
                      this.ctx.moveTo(15, 0); this.ctx.lineTo(12, 20);
                      this.ctx.moveTo(25, 0); this.ctx.lineTo(20, 20);
                      this.ctx.stroke();
                  }
  
                  this.ctx.restore();
              }
  
              drawLandscape(x, y, scale=1, isWinter=false) {
                  this.ctx.save();
                  this.ctx.translate(x, y);
                  this.ctx.scale(scale, scale);
  
                  // Sky box clip
                  this.ctx.beginPath();
                  this.ctx.rect(-20, -30, 80, 70);
                  this.ctx.clip();
  
                  // Sky
                  this.ctx.fillStyle = isWinter ? "#cbd5e1" : "#90cdf4";
                  this.ctx.fillRect(-20, -30, 80, 70);
  
                  // Ground
                  this.ctx.fillStyle = isWinter ? "#fff" : "#68d391";
                  this.ctx.fillRect(-20, 20, 80, 20);
  
                  // Sun
                  if(!isWinter) {
                      this.ctx.fillStyle = "#f6e05e";
                      this.ctx.beginPath();
                      this.ctx.arc(40, -10, 10, 0, Math.PI*2);
                      this.ctx.fill();
                  }
  
                  // Tree Trunk
                  this.ctx.fillStyle = "#4a3c31"; // dark wood
                  this.ctx.fillRect(15, 5, 10, 25);
  
                  // Leaves/Snow
                  this.ctx.fillStyle = isWinter ? "#e2e8f0" : "#2f855a";
                  this.ctx.beginPath();
                  this.ctx.moveTo(20, -15);
                  this.ctx.lineTo(40, 15);
                  this.ctx.lineTo(0, 15);
                  this.ctx.closePath();
                  this.ctx.fill();
  
                  if(isWinter) {
                      // Snowflake logic or just white branches
                      this.ctx.strokeStyle = "#fff";
                      this.ctx.lineWidth = 1;
                      this.ctx.beginPath();
                      this.ctx.moveTo(20, -15); this.ctx.lineTo(20, 15);
                      this.ctx.stroke();
                  }
  
                  // Border
                  this.ctx.strokeStyle = "#4a5568";
                  this.ctx.lineWidth = 2;
                  this.ctx.strokeRect(-20, -30, 80, 70);
  
                  this.ctx.restore();
              }
  
              drawContent(x, y, type) {
                  // Background card for image
                  this.ctx.fillStyle = "#fff";
                  this.ctx.shadowBlur = 10;
                  this.ctx.shadowColor = "rgba(0,0,0,0.1)";
                  this.ctx.fillRect(x-40, y-40, 80, 80);
                  this.ctx.shadowBlur = 0;
                  this.ctx.strokeStyle = "#e2e8f0";
                  this.ctx.lineWidth = 1;
                  this.ctx.strokeRect(x-40, y-40, 80, 80);
  
                  // Draw content based on mode/step
                  if (this.mode === 'animals') {
                      // Type: 0=Horse, 1=Zebra
                      this.drawHorse(x-20, y-10, 1, type === 1);
                  } else {
                      // Type: 0=Summer, 1=Winter
                      this.drawLandscape(x-20, y-5, 1, type === 1);
                  }
              }
  
              // --- Animation Logic ---
  
              drawArrow(x1, y1, x2, y2, progressOffset) {
                  this.ctx.beginPath();
                  this.ctx.moveTo(x1, y1);
                  this.ctx.lineTo(x2, y2);
                  this.ctx.strokeStyle = "#cbd5e1";
                  this.ctx.lineWidth = 4;
                  this.ctx.stroke();
  
                  // Arrowhead
                  const angle = Math.atan2(y2-y1, x2-x1);
                  this.ctx.beginPath();
                  this.ctx.moveTo(x2, y2);
                  this.ctx.lineTo(x2 - 10 * Math.cos(angle - Math.PI/6), y2 - 10 * Math.sin(angle - Math.PI/6));
                  this.ctx.lineTo(x2 - 10 * Math.cos(angle + Math.PI/6), y2 - 10 * Math.sin(angle + Math.PI/6));
                  this.ctx.fillStyle = "#cbd5e1";
                  this.ctx.fill();
  
                  // Moving Packet
                  // Global time creates loop. offset shifts it for this segment
                  let localP = (this.time + progressOffset) % 4; 
                  if(localP >= 0 && localP <= 1) {
                      const px = x1 + (x2 - x1) * localP;
                      const py = y1 + (y2 - y1) * localP;
                      
                      this.ctx.shadowBlur = 10;
                      this.ctx.shadowColor = "#667eea";
                      this.ctx.fillStyle = "#667eea";
                      this.ctx.beginPath();
                      this.ctx.arc(px, py, 6, 0, Math.PI*2);
                      this.ctx.fill();
                      this.ctx.shadowBlur = 0;
                  }
              }
  
              drawCurvedArrow(x1, y1, x2, y2) {
                  // Bezier curve for the loop back
                  this.ctx.beginPath();
                  this.ctx.moveTo(x1, y1);
                  this.ctx.bezierCurveTo(x1+50, y1, x2+50, y2, x2, y2);
                  this.ctx.strokeStyle = "#cbd5e1";
                  this.ctx.lineWidth = 4;
                  this.ctx.stroke();
  
                  // Packet logic (rough approximation for bezier)
                  let localP = (this.time + 2) % 4; // phase 2 of 4
                  if(localP >= 0 && localP <= 1) {
                      // Simple linear interp for the dot for simplicity, though curve is better
                      // Let's do a quick quadratic bezier calc
                      const cp1x = x1+50, cp1y = y1;
                      const cp2x = x2+50, cp2y = y2;
                      const t = localP;
                      
                      const px = Math.pow(1-t,3)*x1 + 3*Math.pow(1-t,2)*t*cp1x + 3*(1-t)*Math.pow(t,2)*cp2x + Math.pow(t,3)*x2;
                      const py = Math.pow(1-t,3)*y1 + 3*Math.pow(1-t,2)*t*cp1y + 3*(1-t)*Math.pow(t,2)*cp2y + Math.pow(t,3)*y2;
  
                      this.ctx.shadowBlur = 10;
                      this.ctx.shadowColor = "#667eea";
                      this.ctx.fillStyle = "#667eea";
                      this.ctx.beginPath();
                      this.ctx.arc(px, py, 6, 0, Math.PI*2);
                      this.ctx.fill();
                      this.ctx.shadowBlur = 0;
                  }
              }
  
              drawLoss(x1, y1, x2, y2) {
                  this.ctx.beginPath();
                  this.ctx.setLineDash([5, 5]);
                  this.ctx.moveTo(x1, y1);
                  this.ctx.lineTo(x2, y2);
                  this.ctx.strokeStyle = "#f56565";
                  this.ctx.lineWidth = 2;
                  this.ctx.stroke();
                  this.ctx.setLineDash([]);
  
                  // Label
                  this.ctx.save();
                  this.ctx.translate(x1 - 10, (y1+y2)/2);
                  this.ctx.rotate(-Math.PI/2);
                  this.ctx.fillStyle = "#f56565";
                  this.ctx.font = "bold 12px sans-serif";
                  this.ctx.textAlign = "center";
                  this.ctx.fillText("Cycle Consistency Loss", 0, 0);
                  this.ctx.restore();
              }
  
              animate() {
                  this.render();
              }

              render() {
                  this.ctx.clearRect(0, 0, this.width, this.height);
                  this.time += 0.01;
  
                  const topY = 80;
                  const botY = 300;
                  const leftX = 80;
                  const midX = 300;
                  const rightX = 520;
  
                  // 1. Draw Network Boxes
                  this.drawBox(midX - 50, topY - 30, "G", "X → Y");
                  this.drawBox(midX - 50, botY - 30, "F", "Y → X");
  
                  // 2. Draw Images (Nodes)
                  
                  // Input X (Top Left)
                  this.drawContent(leftX, topY, 0); // Type 0 (Horse/Summer)
                  this.ctx.fillStyle = "#4a5568";
                  this.ctx.font = "bold 14px sans-serif";
                  this.ctx.textAlign = "center";
                  this.ctx.fillText("Real Input (x)", leftX, topY - 50);
  
                  // Generated Y (Top Right)
                  this.drawContent(rightX, topY, 1); // Type 1 (Zebra/Winter)
                  this.ctx.fillText("Fake (G(x))", rightX, topY - 50);
  
                  // Reconstructed X (Bottom Left)
                  this.drawContent(leftX, botY, 0); // Type 0 (Horse/Summer)
                  this.ctx.fillText("Reconstructed (x')", leftX, botY + 60);
  
                  // 3. Draw Flows
                  // X -> G
                  this.drawArrow(leftX + 45, topY, midX - 55, topY, 0);
                  // G -> Y
                  this.drawArrow(midX + 55, topY, rightX - 45, topY, 1);
                  // Y -> F (Curved down)
                  this.drawCurvedArrow(rightX + 45, topY, midX + 55, botY);
                  // F -> X_recon
                  this.drawArrow(midX - 55, botY, leftX + 45, botY, 3);
  
                  // 4. Draw Loss
                  this.drawLoss(leftX, topY + 45, leftX, botY - 45);
  
                  requestAnimationFrame(this.animate);
              }
          }
  
          const cycleApp = new CycleGANViz();
      </script>
  </div>
    <div class="continue-button" onclick="showNextSection(7)">Continue</div>
</section>

<section id="section7">
    <p>This constraint forces the generator to preserve the structure of the original image (the pose of the horse, the background trees) while only changing the texture (the stripes).</p>
    <div class="check-your-knowledge">
        <h3>Stop & Think</h3>
        <h4>Why is the ability to train on 'Unpaired Data' (CycleGAN) generally considered more valuable for real-world applications than 'Paired Data' (Pix2Pix)?</h4>
        <div id="cuy-paired-answer" style="display:none;" class="animate-in">
            <strong>Answer:</strong> Data scarcity! It is incredibly easy to scrape separate collections of photos (e.g., a folder of Monets and a folder of landscape photos). It is nearly impossible to find datasets where a specific photo exists in two different states simultaneously.
        </div>
        <button class="reveal-button" onclick="revealAnswer('cuy-paired-answer')">Reveal Answer</button>
    </div>
    <p>CycleGAN allows us to perform artistic style transfer, season transfer (Summer \(\leftrightarrow\) Winter), and object transfiguration without expensive manual labeling.</p>
    <div class="continue-button" onclick="showNextSection(8)">Continue</div>
</section>

<section id="section8">
    <h2>The New King: Diffusion Models</h2>
    <p>While GANs opened the door to the creative universe, they are notoriously difficult to train. They are unstable, prone to mode collapse, and hard to tune.</p>
    <p>In recent years, the state-of-the-art has shifted toward <strong>Diffusion Models</strong> (the tech behind DALL-E, Midjourney, and Stable Diffusion).</p>
    <div class="visual-placeholder">
        <img src="images/3.jpg" alt="Training-curve comparison highlighting GAN oscillations versus smoother diffusion trajectories." loading="lazy">
        <p class="image-caption">Diffusion models swap the adversarial chaos for a calmer denoising climb—even if it takes more steps.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(9)">Continue</div>
</section>

<section id="section9">
    <p>Instead of the adversarial game, Diffusion models use a slower, more methodical process. They act like a movie played in reverse.</p>
    <ol>
        <li><strong>Forward Process:</strong> Take an image and slowly add noise until it is pure static.</li>
        <li><strong>Reverse Process:</strong> Train a neural network to look at the static and predict the noise to remove, step-by-step, revealing the image underneath.</li>
    </ol>
    <p>This 'denoising' approach is significantly more stable than the minimax game of GANs, allowing us to train massive models on billions of images.</p>
    <div class="why-it-matters">
        <h3>Why It Matters</h3>
        <p>While Diffusion Models are currently dominant for high-quality image generation, GANs remain faster (real-time generation) and are still crucial for specific tasks like image-to-image translation (CycleGAN).</p>
    </div>
    <div class="frequently-asked">
        <h3>Frequently Asked Question</h3>
        <h4>If Diffusion models are better, are GANs dead?</h4>
        <p>Not at all! Diffusion models are slow because they require many steps to generate one image. GANs generate an image in a single pass, making them much faster. Research is now combining ideas from both to get the best of both worlds.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(10)">Continue</div>
</section>

<section id="section10">
    <div class="test-your-knowledge">
        <h3>Test Your Knowledge: Architectures</h3>
        <h4>You have a dataset of black and white photos of historic cities and a completely separate dataset of color photos of modern cities. You want to colorize the historic photos. Which architecture is most appropriate?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Pix2Pix requires paired data—exact matches of B&W and Color versions of the same image. You have separate datasets, so this won\'t work.')">Pix2Pix</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'A standard cGAN generates images from noise + a label. It is not designed to translate an existing complex image into another style while preserving structure.')">Conditional GAN (Standard)</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct! You have two domains (B&W Historic, Color Modern) but no paired examples. CycleGAN can learn the mapping between these domains using cycle consistency.')">CycleGAN</div>
        </div>
    </div>
    <div class="continue-button" id="continue-q1" onclick="showNextSection(11)" style="display: none;">Continue</div>
</section>

<section id="section11">
    <div class="test-your-knowledge">
        <h3>Test Your Knowledge: Mechanics</h3>
        <h4>Which of the following best describes the 'Cycle Consistency' loss used in CycleGAN?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'That is part of the adversarial loss, not the cycle consistency loss.')">The Discriminator must not be able to tell if the image was cycled or not.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Exactly. \\(F(G(x)) \\approx x\\). This ensures the content is preserved during translation.')">If you translate image X to domain Y, and then back to domain X, you should get the original image X.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Generative models aim to match the distribution, not memorize specific pixels of the training set.')">The Generator must produce images that are pixel-perfect matches to the training set.</div>
        </div>
    </div>
    <div class="continue-button" id="continue-q2" onclick="showNextSection(12)" style="display: none;">Continue</div>
</section>

<section id="section12">
    <div class="test-your-knowledge">
        <h3>Test Your Knowledge: Trends</h3>
        <h4>Why has the field largely shifted towards Diffusion Models for high-fidelity image generation?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Actually, Diffusion models are generally slower at inference time because of the iterative denoising process.')">They are much faster to execute than GANs.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct. They avoid the unstable adversarial game (minimax) and mode collapse issues.')">They offer much greater training stability and diversity than GANs.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'All machine learning models require data.')">They do not require any training data.</div>
        </div>
    </div>
    <div class="continue-button" id="continue-q3" onclick="showNextSection(13)" style="display: none;">Continue</div>
</section>

<section id="section13">
    <h2>Review and Reflect</h2>
    <p>In this final lesson, we explored how the GAN framework evolved to solve specific problems:</p>
    <ul>
        <li><strong>Conditional GANs (cGANs)</strong> added the ability to control outputs using labels.</li>
        <li><strong>Pix2Pix</strong> enabled Image-to-Image translation but required strict paired data.</li>
        <li><strong>CycleGAN</strong> introduced <strong>Cycle Consistency</strong>, allowing us to translate between domains (like Horse \(\leftrightarrow\) Zebra) without needing paired examples.</li>
        <li>Finally, we looked at <strong>Diffusion Models</strong>, which trade some speed for stability, becoming the modern standard for creative AI generation.</li>
    </ul>
    <p>You have now completed the journey through the Generative Universe. From the basic adversarial game to the advanced architectures that power today's AI art, you understand the mechanisms that allow computers to imagine.</p>
    
    <p>Keep exploring, and perhaps you will build the next architecture that redefines what is possible.</p>
</section>

<button id="markCompletedBtn" class="mark-completed-button" onclick="toggleCompleted()">✓ Mark as Completed</button>
</div>

<script>
let currentSection = 1;
const totalSections = 13;

updateProgress();
if (currentSection === totalSections) {
    const completedButton = document.getElementById('markCompletedBtn');
    if (completedButton) completedButton.classList.add('show');
}

function showNextSection(nextSectionId) {
    const nextSectionElement = document.getElementById(`section${nextSectionId}`);
    const currentButton = event && event.target;
    if (!nextSectionElement) return;
    if (currentButton && currentButton.classList.contains('continue-button')) {
        currentButton.style.display = 'none';
    }
    nextSectionElement.classList.add('visible');
    currentSection = nextSectionId;
    updateProgress();
    if (currentSection === totalSections) {
        const completedButton = document.getElementById('markCompletedBtn');
        if (completedButton) completedButton.classList.add('show');
    }
    setTimeout(() => { nextSectionElement.scrollIntoView({ behavior: 'smooth', block: 'start' }); }, 200);
}

function updateProgress() {
    const progressBar = document.getElementById('progressBar');
    const progress = (currentSection / totalSections) * 100;
    progressBar.style.width = `${progress}%`;
}

function revealAnswer(id) {
    const revealText = document.getElementById(id);
    const revealButton = event && event.target;
    if (revealText) {
        revealText.style.display = "block";
        revealText.classList.add('animate-in');
    }
    if (revealButton) {
        revealButton.style.display = "none";
    }
}

function selectChoice(element, isCorrect, explanation) {
    const choices = element.parentNode.querySelectorAll('.choice-option');
    choices.forEach(choice => {
        choice.classList.remove('selected', 'correct', 'incorrect');
        const existing = choice.querySelector('.choice-explanation');
        if (existing) existing.remove();
    });
    element.classList.add('selected');
    element.classList.add(isCorrect ? 'correct' : 'incorrect');
    const explanationDiv = document.createElement('div');
    explanationDiv.className = 'choice-explanation';
    explanationDiv.style.display = 'block';
    explanationDiv.innerHTML = `<strong>${isCorrect ? 'Correct!' : 'Not quite.'}</strong> ${explanation}`;
    element.appendChild(explanationDiv);
    
    // Only show continue button if answer is correct
    if (!isCorrect) return;
    
    // Handle Continue Button appearance for Quiz Sections
    const parentSection = element.closest('section');
    if (parentSection) {
        const sectionId = parentSection.id;
        let continueBtnId = '';
        if (sectionId === 'section10') continueBtnId = 'continue-q1';
        if (sectionId === 'section11') continueBtnId = 'continue-q2';
        if (sectionId === 'section12') continueBtnId = 'continue-q3';
        
        if (continueBtnId) {
            const continueButton = document.getElementById(continueBtnId);
            if (continueButton && continueButton.style.display === 'none') {
                setTimeout(() => {
                    continueButton.style.display = 'block';
                    continueButton.classList.add('show-with-animation');
                }, 800);
            }
        }
    }
}

document.addEventListener('keydown', function(e) {
    if (e.key === 'ArrowRight' || e.key === ' ') {
        const btn = document.querySelector(`#section${currentSection} .continue-button`);
        if (btn && btn.style.display !== 'none') {
            e.preventDefault();
            btn.click();
        }
    }
});

document.documentElement.style.scrollBehavior = 'smooth';

function toggleCompleted() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    const isCompleted = button.classList.contains('completed');
    if (!isCompleted) {
        try {
            if (window.parent && window.parent.ProgressTracker) {
                // Default fallback IDs - these would typically be dynamic in a real LMS
                let courseId = 'computer-vision';
                let pathId = 'generative-adversarial-networks';
                let moduleId = 'cv-ch21-m1-foundations';
                let lessonId = 'cv-ch21-l2-evolving-architectures'; // Updated ID for this lesson
                
                if (window.parent.currentRoute) {
                    const route = window.parent.currentRoute;
                    if (route.courseId) courseId = route.courseId;
                    if (route.pathId) pathId = route.pathId;
                    if (route.moduleId) moduleId = route.moduleId;
                    if (route.lessonId) lessonId = route.lessonId;
                }
                const urlParams = new URLSearchParams(window.location.search);
                if (urlParams.get('course')) courseId = urlParams.get('course');
                if (urlParams.get('path')) pathId = urlParams.get('path');
                if (urlParams.get('module')) moduleId = urlParams.get('module');
                if (urlParams.get('lesson')) lessonId = urlParams.get('lesson');
                window.parent.ProgressTracker.markLessonCompleted(courseId, pathId, moduleId, lessonId);
            }
        } catch (error) {
            console.error('Error with ProgressTracker:', error);
        }
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
        triggerCelebration();
        localStorage.setItem('lesson_cv-ch21-m1-l2_completed', 'true');
    }
}

function triggerCelebration() {
    createConfetti();
    showSuccessMessage();
}

function createConfetti() {
    const confettiContainer = document.createElement('div');
    confettiContainer.className = 'confetti-container';
    document.body.appendChild(confettiContainer);
    const emojis = ['🎉', '🎊', '✨', '🌟', '🎈', '🏆', '👏', '🥳'];
    const colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7'];
    for (let i = 0; i < 40; i++) {
        setTimeout(() => {
            const confetti = document.createElement('div');
            confetti.className = 'confetti';
            if (Math.random() > 0.6) {
                confetti.textContent = emojis[Math.floor(Math.random() * emojis.length)];
            } else {
                confetti.innerHTML = '●';
                confetti.style.color = colors[Math.floor(Math.random() * colors.length)];
            }
            confetti.style.left = Math.random() * 100 + '%';
            confetti.style.animationDelay = Math.random() * 2 + 's';
            document.querySelector('.confetti-container').appendChild(confetti);
        }, i * 50);
    }
    setTimeout(() => { if (confettiContainer.parentNode) confettiContainer.parentNode.removeChild(confettiContainer); }, 5000);
}

function showSuccessMessage() {
    const successMessage = document.createElement('div');
    successMessage.className = 'success-message';
    successMessage.innerHTML = '🎉 Lesson Completed! Great Job! 🎉';
    document.body.appendChild(successMessage);
    setTimeout(() => { if (successMessage.parentNode) successMessage.parentNode.removeChild(successMessage); }, 2500);
}

window.addEventListener('load', function() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    if (window.parent && window.parent.ProgressTracker) {
        // LMS Integration Check
        let courseId = 'computer-vision';
        let pathId = 'generative-adversarial-networks';
        let moduleId = 'cv-ch21-m1-foundations';
        let lessonId = 'cv-ch21-l2-evolving-architectures';
        
        if (window.parent.currentRoute) {
            const route = window.parent.currentRoute;
            if (route.courseId) courseId = route.courseId;
            if (route.pathId) pathId = route.pathId;
            if (route.moduleId) moduleId = route.moduleId;
            if (route.lessonId) lessonId = route.lessonId;
        }
        const urlParams = new URLSearchParams(window.location.search);
        if (urlParams.get('course')) courseId = urlParams.get('course');
        if (urlParams.get('path')) pathId = urlParams.get('path');
        if (urlParams.get('module')) moduleId = urlParams.get('module');
        if (urlParams.get('lesson')) lessonId = urlParams.get('lesson');
        const progress = window.parent.ProgressTracker.getLessonProgress(courseId, pathId, moduleId, lessonId);
        if (progress.state === window.parent.ProgressTracker.STATES.COMPLETED) {
            button.classList.add('completed');
            button.innerHTML = '✅ Completed!';
            return;
        }
    }
    const isCompleted = localStorage.getItem('lesson_cv-ch21-m1-l2_completed') === 'true';
    if (isCompleted) {
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
    }
});
</script>
</body>
</html>