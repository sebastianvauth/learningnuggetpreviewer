<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<link rel="stylesheet" href="../../styles/lesson.css">
<title>Context & Boundaries – Advanced Refinements</title>
<script>
window.MathJax = {
    tex: { inlineMath: [['\\(','\\)'], ['$', '$']] },
    options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script src="../interactive-fixes.js"></script>
</head>
<body>
<div class="progress-container"><div class="progress-bar" id="progressBar"></div></div>
<div class="lesson-container">

<!-- SECTION 1: The Last Mile -->
<section id="section1" class="visible">
    <div class="image-placeholder">
        <img src="images/1.jpg" alt="Side-by-side comparison of a fuzzy U-Net prediction versus a crisp ground-truth mask." loading="lazy">
    </div>
    <h1>Context & Boundaries – Advanced Refinements</h1>
    <h2>The Last Mile</h2>
    <p>You've made it to the final lesson of this chapter! We've built the Encoder-Decoder and refined it with U-Net's skip connections. In many cases, U-Net is enough. But if you are building a self-driving car or detecting cancerous cells, "good enough" isn't acceptable.</p>

    <p>Even the best U-Net can suffer from two specific headaches:</p>
    <ul>
        <li><strong>Context Errors:</strong> Missing the "big picture" (e.g., seeing a grey shape but not realizing it's a boat because the water isn't in view).</li>
        <li><strong>Boundary Errors:</strong> Fuzzy, blob-like edges that fail to capture fine details like bicycle spokes.</li>
    </ul>
    <div class="continue-button" onclick="showNextSection(2)">Continue</div>
</section>

<!-- SECTION 2: Last Mile Intro -->
<section id="section2">
    <p>In this lesson, we are going to add the final polish to our models using advanced techniques to expand our field of view and snap our edges into place.</p>
    <div class="continue-button" onclick="showNextSection(3)">Continue</div>
</section>

<!-- SECTION 3: The Context Problem -->
<section id="section3">
    <h2>The Context Problem: Seeing the Forest</h2>
    <p>Imagine you are looking at a photo through a straw. You see a patch of grey metal. Is it a car door? A refrigerator? A boat hull?</p>
    <p>To decide, you need to move the straw to see what surrounds the grey patch. If you see asphalt, it's a car. If you see water, it's a boat. This surrounding information is called <strong>Context</strong>.</p>
    <div class="continue-button" onclick="showNextSection(4)">Continue</div>
</section>

<!-- SECTION 4: Receptive Field -->
<section id="section4">
    <p>In CNN terms, the "straw" is the <strong>Receptive Field</strong>—the area of the input image that a specific neuron is looking at. If the receptive field is too small, the network makes local guesses without global understanding.</p>
    <div class="continue-button" onclick="showNextSection(5)">Continue</div>
</section>

<!-- SECTION 5: PSPNet -->
<section id="section5">
    <p>One powerful solution is <strong>Pyramid Pooling</strong>, introduced by PSPNet (Pyramid Scene Parsing Network).</p>
    <div class="visual-placeholder">
        <img src="images/2.jpg" alt="Diagram showing PSPNet pyramid pooling branches at multiple scales." loading="lazy">
        <p class="image-caption">Pyramid pooling lets the model look through several “straws” at once—from global context down to fine detail.</p>
    </div>
    <p>PSPNet effectively looks at the image through four different straws at once: one huge straw that sees the whole image, and several smaller straws for details. It combines these views to ensure the network understands that "grey metal surrounded by water" equals "boat."</p>
    <div class="vocab-section">
        <h3>Build Your Vocab</h3>
        <h4>Pyramid Pooling (PSPNet)</h4>
        <p>A technique that pools the feature map at different scales (e.g., whole-image, half-image, small regions) and concatenates them. This gives the network both local detail and global context simultaneously.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(6)">Continue</div>
</section>

<!-- SECTION 6: Atrous Convolution Intro -->
<section id="section6">
    <h2>Atrous Convolution: The Magic Kernel</h2>
    <p>Another way to see more context is to increase the receptive field by using larger convolutional kernels. But a \(7 \times 7\) kernel has way more parameters to learn than a \(3 \times 3\) kernel, making the model heavy and slow.</p>
    <p>Enter <strong>Atrous Convolution</strong> (also known as Dilated Convolution). The DeepLab family of models made this famous.</p>
    <div class="image-placeholder">
        <img src="images/3.jpg" alt="Illustration comparing a standard 3x3 kernel with an atrous kernel that has dilation gaps." loading="lazy">
        <p class="image-caption">Dilated kernels spread their weights without extra parameters, expanding the receptive field in one move.</p>
    </div>
    <p>The idea is simple but brilliant: take a standard kernel and insert "holes" (zeros) between the weights to spread them out.</p>
    <div class="continue-button" onclick="showNextSection(7)">Continue</div>
</section>

<!-- SECTION 7: Atrous Math -->
<section id="section7">
    <p>Let's look at the math of the coverage.</p>
    <p>For a standard \(3 \times 3\) kernel, the width of the view is 3 pixels. For an atrous kernel with a dilation rate \(r\), the effective kernel size \(K_{eff}\) is calculated as:</p>
    <p>$$ K_{eff} = K + (K - 1)(r - 1) $$</p>
    <p>Where \(K\) is the original kernel size (usually 3).</p>
    <p>If we use a rate of \(r=2\) (one hole between weights):</p>
    <p>$$ K_{eff} = 3 + (3 - 1)(2 - 1) = 3 + 2(1) = 5 $$</p>
    <p>We now cover a \(5 \times 5\) area using the same number of parameters (9 weights) as a standard \(3 \times 3\) convolution!</p>
    <div class="continue-button" onclick="showNextSection(8)">Continue</div>
</section>

<!-- SECTION 8: Atrous Check & Interactive -->
<section id="section8">
    <div class="check-your-knowledge">
        <h3>Check Your Understanding</h3>
        <h4>Calculate the effective kernel size if we use a 3x3 kernel with a dilation rate of r=4.</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Not quite. Remember the formula adds $(K-1)(r-1)$.')">7</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct! $3 + (2 \\times 3) = 9$. The kernel now sees a $9 \\times 9$ patch of the image.')">9</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'That\'s a bit too high. Check the formula again: $K_{eff} = 3 + (2)(3)$.')">12</div>
        </div>
    </div>

    <p>By increasing the dilation rate, we can exponentially expand the receptive field without reducing resolution or adding computational cost. Let's visualize this.</p>
    <!-- START: Atrous Convolution Interactive Module -->
<div class="interactive-container" style="background: #f8fafc; border: 2px solid #e2e8f0; border-radius: 12px; padding: 20px; text-align: center; font-family: sans-serif;">
    
  <!-- Canvas for Visuals -->
  <div style="position: relative; margin-bottom: 20px;">
      <canvas id="atrousCanvas" width="600" height="350" style="width: 100%; max-width: 600px; border-radius: 8px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); background: #fff;"></canvas>
      <div style="position: absolute; bottom: 10px; right: 10px; background: rgba(255,255,255,0.9); padding: 5px 10px; border-radius: 4px; font-size: 0.8rem; color: #64748b;">
          Visualizing 3×3 Kernel
      </div>
  </div>

  <!-- Controls -->
  <div style="max-width: 500px; margin: 0 auto;">
      <div style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px;">
          <label for="dilationSlider" style="font-weight: 600; color: #2d3748;">Dilation Rate (r): <span id="rateDisplay" style="color: #667eea; font-size: 1.2rem;">1</span></label>
          <div style="font-size: 0.9rem; color: #4a5568; background: #e6f3ff; padding: 4px 12px; border-radius: 12px; border: 1px solid #4facfe;">
              Effective Size: <strong id="kEffDisplay">3 × 3</strong>
          </div>
      </div>
      
      <input type="range" id="dilationSlider" min="1" max="12" value="1" step="1" 
             style="width: 100%; -webkit-appearance: none; appearance: none; height: 8px; border-radius: 5px; background: #cbd5e1; outline: none; margin: 15px 0;">
      
      <p style="font-size: 0.9rem; color: #64748b; margin-top: 10px; line-height: 1.4;">
          <span id="contextDescription">At <strong>r=1</strong>, the network only sees the tire details.</span>
      </p>
  </div>

  <script>
  (function() {
      const canvas = document.getElementById('atrousCanvas');
      const ctx = canvas.getContext('2d');
      const slider = document.getElementById('dilationSlider');
      const rateDisplay = document.getElementById('rateDisplay');
      const kEffDisplay = document.getElementById('kEffDisplay');
      const contextDesc = document.getElementById('contextDescription');

      // Configuration
      const pixelSize = 12; // Visual size of one "feature map pixel"
      const kernelSize = 3; // Standard 3x3 convolution
      
      // Scene Elements (Fixed positions)
      const carX = 250;
      const carY = 220;
      // Target: The rear tire
      const targetX = carX + 35; 
      const targetY = carY + 35;

      function drawScene() {
          // 1. Sky
          const grd = ctx.createLinearGradient(0, 0, 0, 350);
          grd.addColorStop(0, "#e0f2fe"); // Light blue
          grd.addColorStop(1, "#ffffff");
          ctx.fillStyle = grd;
          ctx.fillRect(0, 0, canvas.width, canvas.height);

          // 2. Background Buildings (Silhouette)
          ctx.fillStyle = "#cbd5e1";
          ctx.beginPath();
          ctx.moveTo(0, 250);
          ctx.lineTo(50, 150);
          ctx.lineTo(100, 150);
          ctx.lineTo(100, 200);
          ctx.lineTo(150, 120);
          ctx.lineTo(220, 120);
          ctx.lineTo(220, 250);
          ctx.lineTo(300, 250);
          ctx.lineTo(300, 100);
          ctx.lineTo(400, 100);
          ctx.lineTo(400, 250);
          ctx.lineTo(450, 180);
          ctx.lineTo(550, 180);
          ctx.lineTo(550, 250);
          ctx.lineTo(600, 250);
          ctx.lineTo(600, 350);
          ctx.lineTo(0, 350);
          ctx.fill();

          // 3. Road
          ctx.fillStyle = "#475569";
          ctx.fillRect(0, 250, canvas.width, 100);
          // Road markings
          ctx.fillStyle = "#94a3b8";
          ctx.fillRect(0, 290, 100, 10);
          ctx.fillRect(150, 290, 100, 10);
          ctx.fillRect(300, 290, 100, 10);
          ctx.fillRect(450, 290, 100, 10);
          ctx.fillRect(600, 290, 100, 10);

          // 4. The Car
          // Car Body
          ctx.fillStyle = "#ef4444"; // Red
          ctx.beginPath();
          ctx.roundRect(carX, carY, 160, 50, 10); // Main body
          ctx.roundRect(carX + 30, carY - 35, 100, 35, [15, 15, 0, 0]); // Roof
          ctx.fill();
          
          // Windows
          ctx.fillStyle = "#bfdbfe";
          ctx.beginPath();
          ctx.roundRect(carX + 40, carY - 25, 35, 20, 5);
          ctx.roundRect(carX + 85, carY - 25, 35, 20, 5);
          ctx.fill();

          // Wheels
          ctx.fillStyle = "#1e293b";
          ctx.beginPath();
          ctx.arc(carX + 35, carY + 35, 18, 0, Math.PI * 2); // Rear (Target)
          ctx.arc(carX + 125, carY + 35, 18, 0, Math.PI * 2); // Front
          ctx.fill();
          
          // Hubcaps
          ctx.fillStyle = "#94a3b8";
          ctx.beginPath();
          ctx.arc(carX + 35, carY + 35, 8, 0, Math.PI * 2);
          ctx.arc(carX + 125, carY + 35, 8, 0, Math.PI * 2);
          ctx.fill();
      }

      function drawReceptiveField(rate) {
          const r = parseInt(rate);
          
          // Calculate math
          // K_eff = K + (K-1)(r-1)
          // K_eff = 3 + 2(r-1)
          const kEff = 3 + 2 * (r - 1);
          
          // Visual logic
          // We are simulating a 3x3 kernel centered at (0,0) relative to target
          // Offsets are: -1, 0, 1 multiplied by rate
          
          const offsets = [-1, 0, 1];
          
          // 1. Draw the "Coverage Box" (Receptive Field)
          // Total width in visual pixels
          const totalWidth = kEff * pixelSize;
          const halfWidth = totalWidth / 2;
          
          ctx.fillStyle = "rgba(79, 172, 254, 0.25)"; // Light blue transparent
          ctx.strokeStyle = "#4facfe";
          ctx.lineWidth = 2;
          
          ctx.beginPath();
          // Rectangle centered on targetX, targetY
          ctx.rect(targetX - halfWidth, targetY - halfWidth, totalWidth, totalWidth);
          ctx.fill();
          ctx.stroke();

          // 2. Draw the "Active Weights" (The atrous holes)
          // These are the 9 actual pixels the kernel "sees"
          ctx.fillStyle = "#d946ef"; // Pink/Purple dot
          
          offsets.forEach(yOffset => {
              offsets.forEach(xOffset => {
                  // Calculate position
                  // The gap between weights is 'rate * pixelSize'
                  const dx = xOffset * r * pixelSize;
                  const dy = yOffset * r * pixelSize;
                  
                  const pX = targetX + dx;
                  const pY = targetY + dy;

                  // Draw connection lines to center (optional visuals to show structure)
                  if (r > 1) {
                     ctx.strokeStyle = "rgba(217, 70, 239, 0.3)";
                     ctx.lineWidth = 1;
                     ctx.beginPath();
                     ctx.moveTo(targetX, targetY);
                     ctx.lineTo(pX, pY);
                     ctx.stroke();
                  }

                  // Draw the weight point
                  ctx.beginPath();
                  ctx.arc(pX, pY, 4, 0, Math.PI * 2);
                  ctx.fill();
              });
          });

          // Update Text
          rateDisplay.textContent = r;
          kEffDisplay.textContent = `${kEff} × ${kEff}`;
          
          // Update Context Description based on rate
          updateDescription(r);
      }

      function updateDescription(r) {
          let msg = "";
          if (r === 1) {
              msg = "At <strong>r=1</strong> (Standard), the network only sees the rubber of the tire.";
          } else if (r <= 3) {
              msg = `At <strong>r=${r}</strong>, the field expands to see the whole wheel and the fender.`;
          } else if (r <= 6) {
              msg = `At <strong>r=${r}</strong>, the network recognizes it's looking at a car chassis.`;
          } else if (r <= 9) {
              msg = `At <strong>r=${r}</strong>, the context includes the road surface and the car windows.`;
          } else {
              msg = `At <strong>r=${r}</strong>, the network sees the car, the road, and the buildings—full global context!`;
          }
          contextDesc.innerHTML = msg;
      }

      // Animation Loop
      function animate() {
          ctx.clearRect(0, 0, canvas.width, canvas.height);
          drawScene();
          drawReceptiveField(slider.value);
          requestAnimationFrame(animate);
      }

      // Initialize
      slider.addEventListener('input', () => {
          // Logic handled in animate loop via slider.value
      });

      // Start
      animate();
  })();
  </script>
</div>
<!-- END: Atrous Convolution Interactive Module -->
    <div class="continue-button" onclick="showNextSection(9)">Continue</div>
</section>

<!-- SECTION 9: The Boundary Problem -->
<section id="section9">
    <h2>The Boundary Problem: The Fine Print</h2>
    <p>We've fixed the context. Now let's fix the edges.</p>
    <p>Because CNNs downsample images (pooling), they lose spatial precision. Even with U-Net's skip connections, the final output can look slightly "blobby." Precise details, like the thin stick of a traffic sign or the spokes of a bicycle, are often lost.</p>
    <div class="continue-button" onclick="showNextSection(10)">Continue</div>
</section>

<!-- SECTION 10: CRF -->
<section id="section10">
    <p>To fix this, we often use a post-processing step called a <strong>Conditional Random Field (CRF)</strong>.</p>
    <p>Think of a CRF as a "smart snap-to-grid" tool in Photoshop, but for pixels. It looks at the raw image and the blurry prediction mask and enforces a simple logic rule:</p>
    <div class="visual-placeholder">
        <img src="images/4.jpg" alt="Three-panel CRF refinement sequence showing a blurry mask, CRF analysis, and a crisp refined mask." loading="lazy">
        <p class="image-caption">CRFs take a fuzzy prediction, analyze neighboring pixels, and snap the mask to the true edges.</p>
    </div>
    <p><strong>The Rule:</strong> "If two pixels are physically close to each other AND have very similar colors in the original image, they should probably belong to the same class."</p>
    <p>The CRF effectively penalizes the model for saying one pixel is "Road" and the neighbor is "Sidewalk" if both pixels are the exact same shade of grey asphalt. It smoothes out the noise and snaps the boundary to the sharp edges found in the original photograph.</p>
    <div class="continue-button" onclick="showNextSection(11)">Continue</div>
</section>

<!-- SECTION 11: CRF Review & Vocab -->
<section id="section11">
    <div class="check-your-knowledge">
        <h3>Stop and Think</h3>
        <h4>CRFs are great for accuracy, so why don't we use them in every single real-time application (like self-driving cars)?</h4>
        <div id="cuy-crf-answer" style="display:none;" class="animate-in">
            <strong>Answer:</strong> It's about speed. CRFs are a post-processing step that involves complex iterative probability calculations. While they make the mask look beautiful, they can be computationally expensive and slow down the system, which is risky for high-speed driving.
        </div>
        <button class="reveal-button" onclick="revealAnswer('cuy-crf-answer')">Reveal Answer</button>
    </div>

    <p>While traditional CRFs are slow, modern research is finding ways to approximate them inside the neural network itself to get the best of both worlds.</p>
    <div class="vocab-section">
        <h3>Build Your Vocab</h3>
        <h4>Conditional Random Field (CRF)</h4>
        <p>A statistical modeling method used as a post-processing step in segmentation. It refines object boundaries by ensuring that pixels with similar colors and proximity are assigned the same label.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(12)">Continue</div>
</section>

<!-- SECTION 12: Review and Reflect -->
<section id="section12">
    <h2>Review and Reflect</h2>
    <p>You have now completed the module on Semantic Segmentation! We started with the basic definition (classifying every pixel), moved through the metrics (IoU), built the architecture (Encoder-Decoder & U-Net), and finally polished the results with Context and Boundary refinements.</p>
    <p>Here is a quick recap of the advanced refinements we just learned:</p>
    <ul>
        <li><strong>Context is King:</strong> Small receptive fields confuse the model. <strong>PSPNet</strong> fixes this by pooling at multiple scales.</li>
        <li><strong>Atrous Convolution:</strong> We can expand the receptive field without losing resolution by adding "holes" to our kernels.</li>
        <li><strong>Sharp Edges:</strong> <strong>CRFs</strong> act as a post-processing step to snap prediction masks to the high-contrast edges of the original image.</li>
    </ul>

    <div class="why-it-matters">
        <h3>Why It Matters</h3>
        <p>These refinements are what separate a 'student project' from a 'state-of-the-art' system. In medical imaging, a CRF refining a tumor boundary by a few millimeters can make the difference between a successful surgery and leaving cancer cells behind.</p>
    </div>

    <div class="test-your-knowledge">
        <h3>Test Your Knowledge</h3>
        <h4>Which of the following best describes the benefit of Atrous (Dilated) Convolution?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Atrous convolution usually keeps the number of parameters the same (e.g., 9 weights for a 3x3 kernel), it just spreads them out.')">It reduces the number of parameters in the network to make it faster.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct! It allows the kernel to \'see\' a wider area without needing to downsample the image size.')">It increases the Receptive Field without reducing spatial resolution.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'No, snapping boundaries is the job of the Conditional Random Field (CRF).')">It automatically snaps the segmentation mask to the object boundaries.</div>
        </div>
    </div>

    <div style="margin-top: 30px;">
        <h3>Frequently Asked</h3>
        <p><strong>Can I train the CRF together with the Neural Network?</strong><br>
        Traditionally, CRFs are separate post-processing steps that are not differentiable, meaning you can't train them with backpropagation. However, recent methods (like CRF-RNN) have managed to formulate the CRF iterations as recurrent neural network layers, allowing for true end-to-end training. It's an active area of research!</p>
    </div>
</section>

<button id="markCompletedBtn" class="mark-completed-button" onclick="toggleCompleted()">✓ Mark as Completed</button>
</div>

<script>
let currentSection = 1;
const totalSections = 12;

updateProgress();
if (currentSection === totalSections) {
    const completedButton = document.getElementById('markCompletedBtn');
    if (completedButton) completedButton.classList.add('show');
}

function showNextSection(nextSectionId) {
    const nextSectionElement = document.getElementById(`section${nextSectionId}`);
    const currentButton = event && event.target;
    if (!nextSectionElement) return;
    if (currentButton && currentButton.classList.contains('continue-button')) {
        currentButton.style.display = 'none';
    }
    nextSectionElement.classList.add('visible');
    currentSection = nextSectionId;
    updateProgress();
    if (currentSection === totalSections) {
        const completedButton = document.getElementById('markCompletedBtn');
        if (completedButton) completedButton.classList.add('show');
    }
    setTimeout(() => { nextSectionElement.scrollIntoView({ behavior: 'smooth', block: 'start' }); }, 200);
}

function updateProgress() {
    const progressBar = document.getElementById('progressBar');
    const progress = (currentSection / totalSections) * 100;
    progressBar.style.width = `${progress}%`;
}

function revealAnswer(id) {
    const revealText = document.getElementById(id);
    const revealButton = event && event.target;
    if (revealText) {
        revealText.style.display = "block";
        revealText.classList.add('animate-in');
    }
    if (revealButton) {
        revealButton.style.display = "none";
    }
}

function selectChoice(element, isCorrect, explanation) {
    const choices = element.parentNode.querySelectorAll('.choice-option');
    choices.forEach(choice => {
        choice.classList.remove('selected', 'correct', 'incorrect');
        const existing = choice.querySelector('.choice-explanation');
        if (existing) existing.remove();
    });
    element.classList.add('selected');
    element.classList.add(isCorrect ? 'correct' : 'incorrect');
    const explanationDiv = document.createElement('div');
    explanationDiv.className = 'choice-explanation';
    explanationDiv.style.display = 'block';
    explanationDiv.innerHTML = `<strong>${isCorrect ? 'Correct!' : 'Not quite.'}</strong> ${explanation}`;
    element.appendChild(explanationDiv);
}

document.addEventListener('keydown', function(e) {
    if (e.key === 'ArrowRight' || e.key === ' ') {
        const btn = document.querySelector(`#section${currentSection} .continue-button`);
        if (btn && btn.style.display !== 'none') {
            e.preventDefault();
            btn.click();
        }
    }
});

document.documentElement.style.scrollBehavior = 'smooth';

function toggleCompleted() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    const isCompleted = button.classList.contains('completed');
    if (!isCompleted) {
        try {
            if (window.parent && window.parent.ProgressTracker) {
                // Update these IDs based on the specific course structure
                let courseId = 'computer-vision';
                let pathId = 'semantic-segmentation';
                let moduleId = 'cv-ch21-m1-foundations';
                let lessonId = 'cv-ch21-l2-context-boundaries'; 
                
                if (window.parent.currentRoute) {
                    const route = window.parent.currentRoute;
                    if (route.courseId) courseId = route.courseId;
                    if (route.pathId) pathId = route.pathId;
                    if (route.moduleId) moduleId = route.moduleId;
                    if (route.lessonId) lessonId = route.lessonId;
                }
                const urlParams = new URLSearchParams(window.location.search);
                if (urlParams.get('course')) courseId = urlParams.get('course');
                if (urlParams.get('path')) pathId = urlParams.get('path');
                if (urlParams.get('module')) moduleId = urlParams.get('module');
                if (urlParams.get('lesson')) lessonId = urlParams.get('lesson');
                window.parent.ProgressTracker.markLessonCompleted(courseId, pathId, moduleId, lessonId);
            }
        } catch (error) {
            console.error('Error with ProgressTracker:', error);
        }
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
        triggerCelebration();
        localStorage.setItem('lesson_cv-ch21-m1-l2_completed', 'true');
    }
}

function triggerCelebration() {
    createConfetti();
    showSuccessMessage();
}

function createConfetti() {
    const confettiContainer = document.createElement('div');
    confettiContainer.className = 'confetti-container';
    document.body.appendChild(confettiContainer);
    const emojis = ['🎉', '🎊', '✨', '🌟', '🎈', '🏆', '👏', '🥳'];
    const colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7'];
    for (let i = 0; i < 40; i++) {
        setTimeout(() => {
            const confetti = document.createElement('div');
            confetti.className = 'confetti';
            if (Math.random() > 0.6) {
                confetti.textContent = emojis[Math.floor(Math.random() * emojis.length)];
            } else {
                confetti.innerHTML = '●';
                confetti.style.color = colors[Math.floor(Math.random() * colors.length)];
            }
            confetti.style.left = Math.random() * 100 + '%';
            confetti.style.animationDelay = Math.random() * 2 + 's';
            document.querySelector('.confetti-container').appendChild(confetti);
        }, i * 50);
    }
    setTimeout(() => { if (confettiContainer.parentNode) confettiContainer.parentNode.removeChild(confettiContainer); }, 5000);
}

function showSuccessMessage() {
    const successMessage = document.createElement('div');
    successMessage.className = 'success-message';
    successMessage.innerHTML = '🎉 Lesson Completed! Great Job! 🎉';
    document.body.appendChild(successMessage);
    setTimeout(() => { if (successMessage.parentNode) successMessage.parentNode.removeChild(successMessage); }, 2500);
}

window.addEventListener('load', function() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    // Check local storage or parent tracker
    const isCompleted = localStorage.getItem('lesson_cv-ch21-m1-l2_completed') === 'true';
    if (isCompleted) {
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
    }
});
</script>
</body>
</html>