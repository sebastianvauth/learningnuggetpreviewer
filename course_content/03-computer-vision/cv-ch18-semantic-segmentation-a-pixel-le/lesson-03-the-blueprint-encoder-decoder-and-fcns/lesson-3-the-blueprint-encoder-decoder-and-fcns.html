<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<link rel="stylesheet" href="../../styles/lesson.css">
<title>The Blueprint – Encoder-Decoder & FCNs</title>
<script>
window.MathJax = {
    tex: { inlineMath: [['\\(','\\)'], ['$', '$']] },
    options: { skipHtmlTags: ['script','noscript','style','textarea','pre','code'] }
};
</script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
<script src="../interactive-fixes.js"></script>
</head>
<body>
<div class="progress-container"><div class="progress-bar" id="progressBar"></div></div>
<div class="lesson-container">

<section id="section1" class="visible">
        <div class="image-placeholder">
        <img src="images/1.jpg" alt="Side-by-side comparison of classification label and segmentation mask of a cat">
    </div>
    <h1>The Blueprint – Encoder-Decoder & FCNs</h1>
    <h2>The Semantic-Spatial Paradox</h2>
    <p>In previous lessons, we learned that Convolutional Neural Networks (CNNs) are amazing at telling us <em>what</em> is in an image. But Semantic Segmentation asks a harder question: <em>where</em> exactly is it, down to the last pixel?</p>

    <p>We have a bit of a paradox here. To recognize an object, a neural network usually needs to 'zoom out' and look at the big picture, ignoring small details. But to draw a perfect outline of that object, it needs to 'zoom in' and focus entirely on those details.</p>
    <div class="continue-button" onclick="showNextSection(2)">Continue</div>
</section>

<section id="section2">
    <p>Think about how a standard classification network works. It uses <strong>pooling layers</strong> to progressively shrink the image. An input image of size $256 \times 256$ might be crunched down to a tiny $7 \times 7$ feature map.</p>
    <div class="continue-button" onclick="showNextSection(3)">Continue</div>
</section>

<section id="section3">
    <p>This shrinking is great for understanding <em>context</em> (knowing it's a cat). But it is terrible for <em>spatial resolution</em> (knowing where the cat's ear ends and the wall begins). We've essentially destroyed the location data to get the semantic meaning.</p>
    <!-- Interactive Pooling Module -->
<div class="interactive-box" id="poolingInteractive">
  <div class="canvas-wrapper">
      <canvas id="poolingCanvas"></canvas>
  </div>
  
  <div class="controls-wrapper">
      <div class="slider-labels">
          <span>Input Image<br>(Raw Pixels)</span>
          <span>Deep Feature Map<br>(Abstract Concepts)</span>
      </div>
      <input type="range" min="0" max="100" value="0" class="interactive-slider" id="poolingSlider">
      <div class="slider-caption">Drag to apply Pooling Layers</div>
  </div>

  <script>
    (function() {
        const canvas = document.getElementById('poolingCanvas');
        const ctx = canvas.getContext('2d');
        const slider = document.getElementById('poolingSlider');
        
        // Configuration
        const width = 600;
        const height = 300;
        canvas.width = width;
        canvas.height = height;
        
        // Offscreen canvas for pixelation trick
        const offCanvas = document.createElement('canvas');
        const offCtx = offCanvas.getContext('2d');

        // Draw the "Subject" (A simple vector cat) on a context
        function drawSubject(context, w, h) {
            // Background
            context.fillStyle = '#f0f4f8';
            context.fillRect(0, 0, w, h);
            
            // 1. Calculate Scale FIRST
            // We base scale on the size of the canvas
            const scale = Math.min(w, h) / 200;

            // 2. Calculate Center
            const cx = w / 2;
            
            // FIX: The vertical offset must be multiplied by the scale.
            // Previously it was just "+ 20", which pushed the cat off-screen 
            // when the canvas got small. Now it scales down with the image.
            const cy = (h / 2) + (20 * scale);

            context.save();
            context.translate(cx, cy);
            context.scale(scale, scale);

            // Ears
            context.fillStyle = '#2d3748';
            context.beginPath();
            context.moveTo(-50, -60);
            context.lineTo(-70, -110);
            context.lineTo(-20, -80);
            context.fill();
            context.beginPath();
            context.moveTo(50, -60);
            context.lineTo(70, -110);
            context.lineTo(20, -80);
            context.fill();

            // Head
            context.beginPath();
            context.arc(0, -30, 60, 0, Math.PI * 2);
            context.fill();

            // Eyes
            context.fillStyle = '#4fd1c5';
            context.beginPath();
            context.ellipse(-20, -40, 12, 18, 0, 0, Math.PI*2);
            context.ellipse(20, -40, 12, 18, 0, 0, Math.PI*2);
            context.fill();
            
            // Pupils
            context.fillStyle = '#fff';
            context.beginPath();
            context.arc(-18, -45, 4, 0, Math.PI*2);
            context.arc(22, -45, 4, 0, Math.PI*2);
            context.fill();

            // Nose
            context.fillStyle = '#fc8181';
            context.beginPath();
            context.moveTo(-8, -10);
            context.lineTo(8, -10);
            context.lineTo(0, 5);
            context.fill();

            // Whiskers
            context.strokeStyle = '#cbd5e1';
            context.lineWidth = 2;
            context.beginPath();
            context.moveTo(-30, -5); context.lineTo(-70, -10);
            context.moveTo(-30, 5);  context.lineTo(-70, 15);
            context.moveTo(30, -5);  context.lineTo(70, -10);
            context.moveTo(30, 5);   context.lineTo(70, 15);
            context.stroke();

            context.restore();
        }

        function drawBars(val) {
            const barWidth = 40;
            const maxBarHeight = 180;
            const startX = 400;
            const groundY = 240;

            // Normalize slider 0-1
            const t = val / 100;

            // Spatial Precision Bar (Decreases)
            const spatialH = maxBarHeight * (1 - t * 0.95); 
            
            // Semantic Meaning Bar (Increases)
            const semanticH = maxBarHeight * (0.1 + t * 0.9); 

            // Draw Axes
            ctx.strokeStyle = '#94a3b8';
            ctx.lineWidth = 2;
            ctx.beginPath();
            ctx.moveTo(startX - 20, groundY);
            ctx.lineTo(startX + 160, groundY);
            ctx.stroke();

            // 1. Spatial Bar
            const gradSpatial = ctx.createLinearGradient(0, groundY - spatialH, 0, groundY);
            gradSpatial.addColorStop(0, '#4facfe');
            gradSpatial.addColorStop(1, '#00f2fe');
            
            ctx.fillStyle = gradSpatial;
            ctx.fillRect(startX, groundY - spatialH, barWidth, spatialH);
            
            // Label
            ctx.fillStyle = '#4a5568';
            ctx.font = 'bold 12px sans-serif';
            ctx.textAlign = 'center';
            ctx.fillText('Spatial', startX + barWidth/2, groundY + 20);
            ctx.fillText('Precision', startX + barWidth/2, groundY + 35);

            // 2. Semantic Bar
            const gradSemantic = ctx.createLinearGradient(0, groundY - semanticH, 0, groundY);
            gradSemantic.addColorStop(0, '#764ba2');
            gradSemantic.addColorStop(1, '#667eea');

            ctx.fillStyle = gradSemantic;
            ctx.fillRect(startX + 80, groundY - semanticH, barWidth, semanticH);

            // Label
            ctx.fillStyle = '#4a5568';
            ctx.textAlign = 'center';
            ctx.fillText('Semantic', startX + 80 + barWidth/2, groundY + 20);
            ctx.fillText('Meaning', startX + 80 + barWidth/2, groundY + 35);
        }

        function render() {
            // Clear main canvas
            ctx.clearRect(0, 0, width, height);

            const val = parseInt(slider.value);
            
            // --- 1. Draw Image (Left Side) ---
            const imgSize = 220;
            const imgX = 50;
            const imgY = 40;
            
            const maxRes = 200;
            const minRes = 4; // Allow it to get very blocky
            // Logarithmic feel for pooling
            const currentRes = Math.max(minRes, Math.floor(maxRes - (val/100 * (maxRes - minRes))));
            
            // Set offscreen canvas size to current resolution
            offCanvas.width = currentRes;
            offCanvas.height = currentRes;
            
            // Draw clear cat to tiny offscreen canvas
            drawSubject(offCtx, currentRes, currentRes);

            // Draw tiny canvas back to main canvas, stretched
            ctx.imageSmoothingEnabled = false; 
            ctx.drawImage(offCanvas, 0, 0, currentRes, currentRes, imgX, imgY, imgSize, imgSize);
            
            // Draw border around image
            ctx.strokeStyle = '#cbd5e1';
            ctx.lineWidth = 4;
            ctx.strokeRect(imgX, imgY, imgSize, imgSize);

            // Label Image
            ctx.fillStyle = '#2d3748';
            ctx.font = 'bold 14px sans-serif';
            ctx.textAlign = 'center';
            ctx.fillText(`Feature Map: ${currentRes}x${currentRes}`, imgX + imgSize/2, imgY - 10);

            // --- 2. Draw Arrow ---
            ctx.fillStyle = '#cbd5e1';
            ctx.beginPath();
            ctx.moveTo(300, 140);
            ctx.lineTo(340, 140);
            ctx.lineTo(340, 130);
            ctx.lineTo(360, 150);
            ctx.lineTo(340, 170);
            ctx.lineTo(340, 160);
            ctx.lineTo(300, 160);
            ctx.fill();

            // --- 3. Draw Bars (Right Side) ---
            drawBars(val);
        }

        slider.addEventListener('input', render);
        render();
    })();
    </script>
</div>
    <p>So, how do we get the best of both worlds? We need an architecture that can shrink the image to understand it, but then expand it back to its original size to map it.</p>
    <div class="continue-button" onclick="showNextSection(4)">Continue</div>
</section>

<section id="section4">
    <p>Enter the <strong>Encoder-Decoder</strong> architecture. It looks like an hourglass.</p>
    <div class="image-placeholder">
        <img src="images/2.jpg" alt="Hourglass-style diagram showing encoder downsampling and decoder upsampling paths">
        <p class="image-caption">Encoder compresses for semantics, decoder expands for spatial detail.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(5)">Continue</div>
</section>

<section id="section5">
    <p>The <strong>Encoder</strong> (the contracting path) captures the 'what'. It reduces spatial dimensions but increases feature depth. The <strong>Decoder</strong> (the expanding path) recovers the 'where'. It attempts to reconstruct the spatial resolution.</p>
    <div class="vocab-section">
        <h3>Build Your Vocab</h3>
        <h4>Encoder-Decoder</h4>
        <p>A neural network design pattern where the 'Encoder' compresses the input into a compact representation (downsampling) to capture semantic meaning, and the 'Decoder' reconstructs the output from that representation (upsampling) to recover spatial details.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(6)">Continue</div>
</section>

<section id="section6">
    <div class="check-your-knowledge">
        <h3>Check Your Understanding</h3>
        <h4>What happens to the spatial resolution of an image as it travels deeper into the Encoder?</h4>
        <div id="cuy-encoder-answer" style="display:none;" class="animate-in">
            <strong>Answer:</strong> It decreases, reducing spatial detail. The image dimensions ($H \times W$) get smaller, trading spatial precision for semantic richness.
        </div>
        <button class="reveal-button" onclick="revealAnswer('cuy-encoder-answer')">Reveal Answer</button>
    </div>
    <div class="continue-button" onclick="showNextSection(7)">Continue</div>
</section>

<section id="section7">
    <h2>Fully Convolutional Networks (FCN)</h2>
    <p>The breakthrough moment came with the <strong>Fully Convolutional Network (FCN)</strong>. Before FCNs, networks ended with 'Fully Connected' layers to output a single class label.</p>
    <div class="continue-button" onclick="showNextSection(8)">Continue</div>
</section>

<section id="section8">
    <p>Fully Connected layers are destructive. They take a 3D feature map (height, width, channels) and flatten it into a 1D vector. This kills all spatial information. You can't make a map if you've flattened the world into a list!</p>
    <div class="image-placeholder">
        <img src="images/3.jpg" alt="Comic illustrating how flattening loses spatial information needed for segmentation">
        <p class="image-caption">Flattening destroys “where” information—no wonder the robot can’t find the tree.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(9)">Continue</div>
</section>

<section id="section9">
    <p>FCNs introduced a clever trick called <strong>Convolutionalization</strong>. They replaced those flattening layers with <strong>$1 \times 1$ Convolutions</strong>.</p>
    <div class="continue-button" onclick="showNextSection(10)">Continue</div>
</section>

<section id="section10">
    <p>By using a $1 \times 1$ convolution, the network keeps the spatial grid intact. Instead of outputting a single probability vector, it outputs a coarse <strong>heatmap</strong> of probabilities for the entire image.</p>
    <!-- Interactive FCN Variants Module (Corrected) -->
<div class="interactive-box" id="skipConnectionsInteractive">
    <div class="canvas-wrapper">
        <canvas id="skipsCanvas"></canvas>
    </div>
    
    <div class="controls-wrapper">
        <div class="variant-labels">
            <span id="lbl-32s" class="variant-label">FCN-32s<br><small>(No Skips)</small></span>
            <span id="lbl-16s" class="variant-label">FCN-16s<br><small>(Pool4 Skip)</small></span>
            <span id="lbl-8s" class="variant-label">FCN-8s<br><small>(Pool3+4 Skips)</small></span>
        </div>
        
        <input type="range" min="1" max="3" step="0.05" value="1" class="interactive-slider" id="skipsSlider">
        
        <div id="skips-caption" class="slider-caption">
            Result: Very Coarse. The mask is just a blob.
        </div>
    </div>

    <script>
    (function() {
        const canvas = document.getElementById('skipsCanvas');
        const ctx = canvas.getContext('2d');
        const slider = document.getElementById('skipsSlider');
        const caption = document.getElementById('skips-caption');
        
        // Labels
        const lbl32 = document.getElementById('lbl-32s');
        const lbl16 = document.getElementById('lbl-16s');
        const lbl8 = document.getElementById('lbl-8s');
        
        // Config
        const width = 500;
        const height = 300;
        canvas.width = width;
        canvas.height = height;
        
        // Offscreen canvas for generating the low-res mask
        const maskCanvas = document.createElement('canvas');
        const maskCtx = maskCanvas.getContext('2d');

        // Draw the Cat Outline (Input Image)
        function drawCatOutline(context, w, h, isMask = false) {
            // FIX: Use proportional positioning instead of fixed pixels
            // The original design was tuned for h=300. 
            // We calculate a ratio based on the current height vs the reference height (300).
            const ratio = h / 300;
            
            const cx = w / 2;
            // Originally `h/2 + 20`. Now we scale the 20 by the ratio.
            const cy = h / 2 + (20 * ratio); 
            
            // Scale based on dimensions
            const scale = Math.min(w, h) / 220;

            context.save();
            context.translate(cx, cy);
            context.scale(scale, scale);

            if (isMask) {
                context.fillStyle = '#9f7aea'; // Purple mask
            } else {
                context.fillStyle = '#2d3748'; // Dark Grey for cat
            }

            // Ears
            context.beginPath();
            context.moveTo(-50, -60); context.lineTo(-70, -110); context.lineTo(-20, -80);
            context.fill();
            context.beginPath();
            context.moveTo(50, -60); context.lineTo(70, -110); context.lineTo(20, -80);
            context.fill();

            // Head
            context.beginPath();
            context.arc(0, -30, 60, 0, Math.PI * 2);
            context.fill();

            if (!isMask) {
                // Eyes
                context.fillStyle = '#4fd1c5';
                context.beginPath();
                context.ellipse(-20, -40, 12, 18, 0, 0, Math.PI*2);
                context.ellipse(20, -40, 12, 18, 0, 0, Math.PI*2);
                context.fill();
                
                // Whiskers
                context.strokeStyle = '#cbd5e1';
                context.lineWidth = 2;
                context.beginPath();
                context.moveTo(-30, -5); context.lineTo(-70, -10);
                context.moveTo(-30, 5);  context.lineTo(-70, 15);
                context.moveTo(30, -5);  context.lineTo(70, -10);
                context.moveTo(30, 5);   context.lineTo(70, 15);
                context.stroke();
            }

            context.restore();
        }

        function render() {
            ctx.clearRect(0, 0, width, height);

            const val = parseFloat(slider.value);

            // Update UI Labels
            lbl32.className = val < 1.5 ? 'variant-label active' : 'variant-label';
            lbl16.className = val >= 1.5 && val < 2.5 ? 'variant-label active' : 'variant-label';
            lbl8.className = val >= 2.5 ? 'variant-label active' : 'variant-label';

            // Update Caption
            if (val < 1.5) caption.innerHTML = "<strong>FCN-32s:</strong> Very Coarse. The mask is just a blob.";
            else if (val < 2.5) caption.innerHTML = "<strong>FCN-16s:</strong> Better. Some shape emerges, but edges are soft.";
            else caption.innerHTML = "<strong>FCN-8s:</strong> Sharp! Skip connections recover the whiskers and ears.";

            // 1. Draw Real Image (Background)
            ctx.globalAlpha = 1.0;
            drawCatOutline(ctx, width, height, false);

            // 2. Generate Mask
            // Map slider to resolution factor
            let resFactor; 
            if (val <= 2) {
                 const t = val - 1; // 0 to 1
                 resFactor = 0.03 + (t * 0.03); 
            } else {
                 const t = val - 2;
                 resFactor = 0.06 + (t * 0.19);
            }

            const maskW = Math.floor(width * resFactor);
            const maskH = Math.floor(height * resFactor);
            
            maskCanvas.width = maskW;
            maskCanvas.height = maskH;

            // Draw solid mask on tiny canvas
            drawCatOutline(maskCtx, maskW, maskH, true);

            // 3. Draw Mask Overlay
            ctx.imageSmoothingEnabled = true; // Use bilinear smoothing for the blobby effect
            ctx.globalAlpha = 0.6;
            ctx.globalCompositeOperation = 'source-over';
            ctx.drawImage(maskCanvas, 0, 0, maskW, maskH, 0, 0, width, height);
            
            ctx.globalAlpha = 1.0;
        }

        slider.addEventListener('input', render);
        render();
    })();
    </script>
</div>
    <p>This allows the network to accept input images of <em>any</em> size and produce a 2D output map corresponding to that size. No flattening required.</p>
    <div class="continue-button" onclick="showNextSection(11)">Continue</div>
</section>

<section id="section11">
    <div class="test-your-knowledge">
        <h3>Test Your Knowledge</h3>
        <h4>Why are Fully Connected layers problematic for semantic segmentation?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Speed isn\'t the primary issue here.')">They are too slow to compute.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct! Segmentation requires spatial coordinates, which flattening removes.')">They destroy spatial information by flattening the input.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Fully connected layers can handle color data just fine.')">They cannot handle color images.</div>
        </div>
    </div>
    <div class="continue-button" id="continue-after-test-knowledge-1" onclick="showNextSection(12)" style="display: none;">Continue</div>
</section>

<section id="section12">
    <h2>The Innovation: Skip Connections</h2>
    <p>So, FCNs gave us a heatmap. But there was a problem. Because of all the pooling in the Encoder, the heatmap was very coarse and blurry. It looked like a blocky video game from the 1980s.</p>
    <div class="continue-button" onclick="showNextSection(13)">Continue</div>
</section>

<section id="section13">
    <p>Simply upsampling this blurry map doesn't bring back the details. You can't magically recreate a detailed face from a 5-pixel thumbnail... or can you?</p>
    <div class="stop-and-think">
        <h3>Stop and Think</h3>
        <h4>If the Decoder can learn to upsample, why do we even need extra help from the Encoder?</h4>
        <div id="sat-skips-answer" style="display:none;" class="animate-in">
            <strong>Hint:</strong> Think about information loss. Once you downsample (pool) an image, the fine details are mathematically gone from that layer. The decoder can't "guess" details that no longer exist.
        </div>
        <button class="reveal-button" onclick="revealAnswer('sat-skips-answer')">Reveal Hint</button>
    </div>
    <div class="continue-button" onclick="showNextSection(14)">Continue</div>
</section>

<section id="section14">
    <p>To fix this, FCNs introduced <strong>Skip Connections</strong>. This is like building a bridge from the early layers of the Encoder directly to the Decoder.</p>
    <div class="image-placeholder">
        <img src="images/4.jpg" alt="Skip connections bridging early encoder layers to later decoder layers in a U-shaped network diagram">
        <p class="image-caption">Skip connections act like wires that bypass the bottleneck, piping high-resolution spatial detail directly into the upsampling decoder.</p>
    </div>
    <p>The early layers of a network don't know <em>what</em> objects are, but they know exactly <em>where</em> the edges and lines are (high spatial resolution). The skip connections pipe this precise location data into the decoder.</p>
    <div class="continue-button" onclick="showNextSection(15)">Continue</div>
</section>

<section id="section15">
    <p>The decoder then fuses the semantic info (from the deep layers) with the spatial info (from the skip connections).</p>
 <!-- Interactive FCN Variants Module (Section 15 - FIXED DUPLICATE) -->
<div class="interactive-box" id="skipConnectionsInteractive2">
    <div class="canvas-wrapper">
        <canvas id="skipsCanvas2"></canvas>
    </div>

    <div class="controls-wrapper">
        <div class="variant-labels">
            <span id="lbl-32s-2" class="variant-label">FCN-32s<br><small>(No Skips)</small></span>
            <span id="lbl-16s-2" class="variant-label">FCN-16s<br><small>(Pool4 Skip)</small></span>
            <span id="lbl-8s-2" class="variant-label">FCN-8s<br><small>(Pool3+4 Skips)</small></span>
        </div>

        <input type="range" min="1" max="3" step="0.05" value="1" class="interactive-slider" id="skipsSlider2">

        <div id="skips-caption-2" class="slider-caption">
            Result: Very Coarse. The mask is just a blob.
        </div>
    </div>

    <script>
    (function() {
        const canvas = document.getElementById('skipsCanvas2');
        if (!canvas) return;
        const ctx = canvas.getContext('2d');
        const slider = document.getElementById('skipsSlider2');
        const caption = document.getElementById('skips-caption-2');

        // Labels
        const lbl32 = document.getElementById('lbl-32s-2');
        const lbl16 = document.getElementById('lbl-16s-2');
        const lbl8 = document.getElementById('lbl-8s-2');
        
        // Config
        const width = 500;
        const height = 300;
        canvas.width = width;
        canvas.height = height;
        
        // Offscreen canvas for generating the low-res mask
        const maskCanvas = document.createElement('canvas');
        const maskCtx = maskCanvas.getContext('2d');

        // Draw the Cat Outline (Input Image)
        function drawCatOutline(context, w, h, isMask = false) {
            // FIX: Use proportional positioning instead of fixed pixels
            // The original design was tuned for h=300. 
            // We calculate a ratio based on the current height vs the reference height (300).
            const ratio = h / 300;
            
            const cx = w / 2;
            // Originally `h/2 + 20`. Now we scale the 20 by the ratio.
            const cy = h / 2 + (20 * ratio); 
            
            // Scale based on dimensions
            const scale = Math.min(w, h) / 220;

            context.save();
            context.translate(cx, cy);
            context.scale(scale, scale);

            if (isMask) {
                context.fillStyle = '#9f7aea'; // Purple mask
            } else {
                context.fillStyle = '#2d3748'; // Dark Grey for cat
            }

            // Ears
            context.beginPath();
            context.moveTo(-50, -60); context.lineTo(-70, -110); context.lineTo(-20, -80);
            context.fill();
            context.beginPath();
            context.moveTo(50, -60); context.lineTo(70, -110); context.lineTo(20, -80);
            context.fill();

            // Head
            context.beginPath();
            context.arc(0, -30, 60, 0, Math.PI * 2);
            context.fill();

            if (!isMask) {
                // Eyes
                context.fillStyle = '#4fd1c5';
                context.beginPath();
                context.ellipse(-20, -40, 12, 18, 0, 0, Math.PI*2);
                context.ellipse(20, -40, 12, 18, 0, 0, Math.PI*2);
                context.fill();
                
                // Whiskers
                context.strokeStyle = '#cbd5e1';
                context.lineWidth = 2;
                context.beginPath();
                context.moveTo(-30, -5); context.lineTo(-70, -10);
                context.moveTo(-30, 5);  context.lineTo(-70, 15);
                context.moveTo(30, -5);  context.lineTo(70, -10);
                context.moveTo(30, 5);   context.lineTo(70, 15);
                context.stroke();
            }

            context.restore();
        }

        function render() {
            if (!ctx || !canvas) return;
            ctx.clearRect(0, 0, width, height);

            const val = parseFloat(slider ? slider.value : 1);

            // Update UI Labels with null checks
            if (lbl32) lbl32.className = val < 1.5 ? 'variant-label active' : 'variant-label';
            if (lbl16) lbl16.className = val >= 1.5 && val < 2.5 ? 'variant-label active' : 'variant-label';
            if (lbl8) lbl8.className = val >= 2.5 ? 'variant-label active' : 'variant-label';

            // Update Caption
            if (caption) {
                if (val < 1.5) caption.innerHTML = "<strong>FCN-32s:</strong> Very Coarse. The mask is just a blob.";
                else if (val < 2.5) caption.innerHTML = "<strong>FCN-16s:</strong> Better. Some shape emerges, but edges are soft.";
                else caption.innerHTML = "<strong>FCN-8s:</strong> Sharp! Skip connections recover the whiskers and ears.";
            }

            // 1. Draw Real Image (Background)
            ctx.globalAlpha = 1.0;
            drawCatOutline(ctx, width, height, false);

            // 2. Generate Mask
            // Map slider to resolution factor
            let resFactor;
            if (val <= 2) {
                 const t = val - 1; // 0 to 1
                 resFactor = 0.03 + (t * 0.03);
            } else {
                 const t = val - 2;
                 resFactor = 0.06 + (t * 0.19);
            }

            const maskW = Math.floor(width * resFactor);
            const maskH = Math.floor(height * resFactor);

            maskCanvas.width = maskW;
            maskCanvas.height = maskH;

            // Draw solid mask on tiny canvas
            drawCatOutline(maskCtx, maskW, maskH, true);

            // 3. Draw Mask Overlay
            ctx.imageSmoothingEnabled = true; // Use bilinear smoothing for the blobby effect
            ctx.globalAlpha = 0.6;
            ctx.globalCompositeOperation = 'source-over';
            ctx.drawImage(maskCanvas, 0, 0, maskW, maskH, 0, 0, width, height);

            ctx.globalAlpha = 1.0;
        }

        if (slider) slider.addEventListener('input', render);

        // Wait for section visibility before initial render
        const parentSection = canvas.closest('section');
        if (parentSection && !parentSection.classList.contains('visible')) {
            const observer = new MutationObserver(() => {
                if (parentSection.classList.contains('visible')) {
                    setTimeout(render, 100);
                    observer.disconnect();
                }
            });
            observer.observe(parentSection, { attributes: true, attributeFilter: ['class'] });
        } else {
            setTimeout(render, 100);
        }
    })();
    </script>
</div>
    <p>As you can see, the FCN-8s model, which uses more skip connections, produces much sharper boundaries than FCN-32s.</p>
    <div class="vocab-section">
        <h3>Build Your Vocab</h3>
        <h4>Skip Connections</h4>
        <p>Connections in a neural network that bypass one or more layers. In segmentation, they transfer high-resolution feature maps from the encoder to the decoder to help recover spatial details lost during pooling.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(16)">Continue</div>
</section>

<section id="section16">
    <div class="why-it-matters">
        <h3>Why It Matters</h3>
        <p>This Encoder-Decoder structure with Skip Connections is the blueprint for virtually all modern dense prediction tasks. Whether you are segmenting tumors in MRI scans or estimating depth for self-driving cars, this is the architecture you will use.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(17)">Continue</div>
</section>

<section id="section17">
    <p>We have the structure now, but we still haven't explained the math of <em>how</em> the decoder makes the image bigger. We'll cover that 'Transposed Convolution' in the next lesson.</p>
    <div class="faq-section">
        <h3>Frequently Asked Question</h3>
        <h4>Can I use any classification network as an encoder?</h4>
        <p>Yes! It is very common to use a pre-trained network like ResNet or VGG as the encoder. You simply remove the final fully connected layers (the "head") and attach your own decoder. This is called Transfer Learning and speeds up training significantly.</p>
    </div>
    <div class="continue-button" onclick="showNextSection(18)">Continue</div>
</section>

<section id="section18">
    <h2>Review and Reflect</h2>
    <p>We've established the structural foundation for Semantic Segmentation.</p>
    <p>Here is what we covered:</p>
    <ul>
        <li><strong>The Paradox:</strong> We need to downsample to understand semantics, but upsample to map location.</li>
        <li><strong>The Solution:</strong> An <strong>Encoder-Decoder</strong> architecture.</li>
        <li><strong>FCNs:</strong> Replaced fully connected layers with convolutions to preserve spatial data.</li>
        <li><strong>Skip Connections:</strong> The vital "bridges" that carry fine details from the encoder to the decoder to sharpen the final mask.</li>
    </ul>
    
    <div class="test-your-knowledge">
        <h3>Final Check</h3>
        <h4>What is the primary purpose of a Skip Connection in a segmentation network?</h4>
        <div class="multiple-choice">
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'While they can help with gradient flow, their primary purpose in segmentation is about detail recovery.')">To make the model train faster by skipping layers.</div>
            <div class="choice-option" data-correct="true" onclick="selectChoice(this, true, 'Correct! They combine the \'where\' (from early layers) with the \'what\' (from deep layers).')">To re-introduce high-resolution spatial details to the decoder.</div>
            <div class="choice-option" data-correct="false" onclick="selectChoice(this, false, 'Skip connections usually increase the computational load slightly because there is more data to process in the decoder.')">To reduce the number of parameters in the network.</div>
        </div>
    </div>
    <div class="continue-button" id="continue-after-test-knowledge-2" style="display: none;" onclick="showMarkCompleted()">Continue</div>
</section>

<button id="markCompletedBtn" class="mark-completed-button" onclick="toggleCompleted()">✓ Mark as Completed</button>
</div>

<script>
let currentSection = 1;
const totalSections = 18;

updateProgress();
if (currentSection === totalSections) {
    const completedButton = document.getElementById('markCompletedBtn');
    if (completedButton) completedButton.classList.add('show');
}

function showNextSection(nextSectionId) {
    const nextSectionElement = document.getElementById(`section${nextSectionId}`);
    const currentButton = event && event.target;
    if (!nextSectionElement) return;
    if (currentButton && currentButton.classList.contains('continue-button')) {
        currentButton.style.display = 'none';
    }
    nextSectionElement.classList.add('visible');
    currentSection = nextSectionId;
    updateProgress();
    
    // Auto-scroll logic
    setTimeout(() => { nextSectionElement.scrollIntoView({ behavior: 'smooth', block: 'start' }); }, 200);
}

function showMarkCompleted() {
    // This function handles the final flow when the last continue button is clicked
    const btn = document.getElementById('continue-after-test-knowledge-2');
    if(btn) btn.style.display = 'none';
    
    const completedButton = document.getElementById('markCompletedBtn');
    if (completedButton) {
        completedButton.classList.add('show');
        completedButton.scrollIntoView({ behavior: 'smooth', block: 'center' });
    }
}

function updateProgress() {
    const progressBar = document.getElementById('progressBar');
    const progress = (currentSection / totalSections) * 100;
    progressBar.style.width = `${progress}%`;
}

function revealAnswer(id) {
    const revealText = document.getElementById(id);
    const revealButton = event && event.target;
    if (revealText) {
        revealText.style.display = "block";
        revealText.classList.add('animate-in');
    }
    if (revealButton) {
        revealButton.style.display = "none";
    }
}

function selectChoice(element, isCorrect, explanation) {
    const choices = element.parentNode.querySelectorAll('.choice-option');
    choices.forEach(choice => {
        choice.classList.remove('selected', 'correct', 'incorrect');
        const existing = choice.querySelector('.choice-explanation');
        if (existing) existing.remove();
    });
    element.classList.add('selected');
    element.classList.add(isCorrect ? 'correct' : 'incorrect');
    const explanationDiv = document.createElement('div');
    explanationDiv.className = 'choice-explanation';
    explanationDiv.style.display = 'block';
    explanationDiv.innerHTML = `<strong>${isCorrect ? 'Correct!' : 'Not quite.'}</strong> ${explanation}`;
    element.appendChild(explanationDiv);
    
    // Only show continue button if answer is correct
    if (!isCorrect) return;
    
    // Logic to show continue button after selection
    const parentSection = element.closest('section');
    if (parentSection) {
        let continueBtnId = '';
        if(parentSection.id === 'section11') continueBtnId = 'continue-after-test-knowledge-1';
        if(parentSection.id === 'section18') continueBtnId = 'continue-after-test-knowledge-2';
        
        if (continueBtnId) {
            const continueButton = document.getElementById(continueBtnId);
            if (continueButton && continueButton.style.display === 'none') {
                setTimeout(() => {
                    continueButton.style.display = 'block';
                    continueButton.classList.add('show-with-animation');
                }, 800);
            }
        }
    }
}

document.addEventListener('keydown', function(e) {
    if (e.key === 'ArrowRight' || e.key === ' ') {
        const btn = document.querySelector(`#section${currentSection} .continue-button`);
        if (btn && btn.style.display !== 'none') {
            e.preventDefault();
            btn.click();
        }
    }
});

document.documentElement.style.scrollBehavior = 'smooth';

function toggleCompleted() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    const isCompleted = button.classList.contains('completed');
    if (!isCompleted) {
        // Attempt parent LMS communication
        try {
            if (window.parent && window.parent.ProgressTracker) {
                // These IDs should be updated based on the actual course structure
                let courseId = 'computer-vision';
                let pathId = 'semantic-segmentation';
                let moduleId = 'cv-ch22-m1-foundations';
                let lessonId = 'cv-ch22-l2-encoder-decoder';
                
                if (window.parent.currentRoute) {
                    const route = window.parent.currentRoute;
                    if (route.courseId) courseId = route.courseId;
                    if (route.pathId) pathId = route.pathId;
                    if (route.moduleId) moduleId = route.moduleId;
                    if (route.lessonId) lessonId = route.lessonId;
                }
                // Fallback to URL params
                const urlParams = new URLSearchParams(window.location.search);
                if (urlParams.get('course')) courseId = urlParams.get('course');
                if (urlParams.get('path')) pathId = urlParams.get('path');
                if (urlParams.get('module')) moduleId = urlParams.get('module');
                if (urlParams.get('lesson')) lessonId = urlParams.get('lesson');
                
                window.parent.ProgressTracker.markLessonCompleted(courseId, pathId, moduleId, lessonId);
            }
        } catch (error) {
            console.error('Error with ProgressTracker:', error);
        }
        
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
        triggerCelebration();
        localStorage.setItem('lesson_cv-ch22-m1-l2_completed', 'true');
    }
}

function triggerCelebration() {
    createConfetti();
    showSuccessMessage();
}

function createConfetti() {
    const confettiContainer = document.createElement('div');
    confettiContainer.className = 'confetti-container';
    document.body.appendChild(confettiContainer);
    const emojis = ['🎉', '🎊', '✨', '🌟', '🎈', '🏆', '👏', '🥳', '🖼️', '🏗️'];
    const colors = ['#ff6b6b', '#4ecdc4', '#45b7d1', '#96ceb4', '#ffeaa7'];
    for (let i = 0; i < 40; i++) {
        setTimeout(() => {
            const confetti = document.createElement('div');
            confetti.className = 'confetti';
            if (Math.random() > 0.6) {
                confetti.textContent = emojis[Math.floor(Math.random() * emojis.length)];
            } else {
                confetti.innerHTML = '●';
                confetti.style.color = colors[Math.floor(Math.random() * colors.length)];
            }
            confetti.style.left = Math.random() * 100 + '%';
            confetti.style.animationDelay = Math.random() * 2 + 's';
            document.querySelector('.confetti-container').appendChild(confetti);
        }, i * 50);
    }
    setTimeout(() => { if (confettiContainer.parentNode) confettiContainer.parentNode.removeChild(confettiContainer); }, 5000);
}

function showSuccessMessage() {
    const successMessage = document.createElement('div');
    successMessage.className = 'success-message';
    successMessage.innerHTML = '🎉 Lesson Completed! Great Job! 🎉';
    document.body.appendChild(successMessage);
    setTimeout(() => { if (successMessage.parentNode) successMessage.parentNode.removeChild(successMessage); }, 2500);
}

window.addEventListener('load', function() {
    const button = document.getElementById('markCompletedBtn');
    if (!button) return;
    
    // Check local storage
    const isCompleted = localStorage.getItem('lesson_cv-ch22-m1-l2_completed') === 'true';
    if (isCompleted) {
        button.classList.add('completed');
        button.innerHTML = '✅ Completed!';
    }
});
</script>
</body>
</html>