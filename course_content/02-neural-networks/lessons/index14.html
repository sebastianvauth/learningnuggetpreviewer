<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lesson 14: The Extended Family: A Quick Tour of More Activations</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          max-width: 800px;
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          font-size: 150%;
      }
      section {
          margin-bottom: 20px;
          padding: 20px;
          background-color: #ffffff;
          display: none;
          opacity: 0;
          transition: opacity 0.5s ease-in;
      }
      h1, h2, h3, h4 {
          color: #333;
          margin-top: 20px;
      }
      p, li {
          line-height: 1.6;
          color: #444;
          margin-bottom: 20px;
      }
      ul {
          padding-left: 20px;
      }
      .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          text-align: left;
      }
      .image-placeholder img, .interactive-placeholder img {
          max-width: 100%;
          height: auto;
          border-radius: 5px;
      }
      .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          padding: 20px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .vocab-section {
          background-color: #f0f8ff;
      }
      .vocab-section h3 {
          color: #1e90ff;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .vocab-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .vocab-term {
          font-weight: bold;
          color: #1e90ff;
      }
      .why-it-matters {
          background-color: #ffe6f0;
      }
      .why-it-matters h3 {
          color: #d81b60;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .stop-and-think {
          background-color: #e6e6ff;
      }
      .stop-and-think h3 {
          color: #4b0082;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .continue-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #007bff;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .reveal-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #4b0082;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .test-your-knowledge {
          background-color: #e6ffe6; /* Light green background */
      }
      .test-your-knowledge h3 {
          color: #28a745; /* Dark green heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .test-your-knowledge h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .test-your-knowledge p {
          margin-bottom: 15px;
      }
      .check-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #28a745; /* Green background */
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
          border: none;
          font-size: 1em;
      }
      .option {
          margin-bottom: 10px;
          padding: 10px;
          border: 1px solid #ddd;
          border-radius: 5px;
          cursor: pointer;
      }
      .option:hover {
          background-color: #f5f5f5;
      }
      .option.selected {
          background-color: #d4edda;
          border-color: #c3e6cb;
      }
      .option-feedback {
          margin-top: 5px;
          padding: 10px;
          border-radius: 5px;
          display: none;
      }
      .correct-feedback {
          background-color: #d4edda;
          border-color: #c3e6cb;
          color: #155724;
      }
      .incorrect-feedback {
          background-color: #f8d7da;
          border-color: #f5c6cb;
          color: #721c24;
      }
      .faq-section {
          background-color: #fffbea; /* Light yellow background */
      }
      .faq-section h3 {
          color: #ffcc00; /* Bright yellow heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .faq-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .gallery-card {
          border: 1px solid #ddd;
          border-radius: 8px;
          padding: 15px;
          margin-bottom: 20px;
          background-color: #f9f9f9;
      }
      .gallery-card h3 {
          margin-top: 0;
          color: #333;
      }
      .gallery-card p {
          margin-bottom: 10px;
      }
      .gallery-card img {
          max-width: 100%;
          height: auto;
          margin-bottom: 10px;
      }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <section id="section1">
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="An image of a large, sprawling 'Activation Function Family Tree'. The main branches are labeled 'Sigmoid-like', 'Tanh-like', 'ReLU-like'. Many smaller branches sprout off, showing names like ELU, SELU, PReLU, Swish, GELU, etc., indicating a diverse and growing family.">
      </div>
      <h1>Lesson 14: The Extended Family: A Quick Tour of More Activations</h1>
      <h2>Beyond the Famous Four</h2>
      <p>Hey activation explorers! We've spent some quality time with the 'famous four' of our Activation Function Zoo – Identity, Sigmoid, Tanh, and ReLU, plus its close cousin Leaky ReLU. These cover a lot of ground and are fundamental to understanding neural networks.</p>
      <p>But the world of activation functions is constantly evolving! Researchers are always on the hunt for new functions that might help networks train faster, generalize better, or overcome some of the limitations of existing ones. So, today, we're going to take a very quick, high-level tour of some other members of this extended family. You don't need to memorize all their details, but it's good to be aware that they exist, as you might encounter them in research papers or advanced tutorials. (This lesson glances at the variety shown on Slides 23 & 24).</p>
      <div class="continue-button" onclick="showNextSection(2)">Continue</div>
  </section>

  <section id="section2">
      <h2>Why More Functions? The Quest for Improvement</h2>
      <p>You might be wondering, if ReLU is so good, why do we need even more functions?</p>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A cartoon scientist in a lab, mixing different mathematical symbols in beakers, trying to concoct the 'perfect' activation function. A checklist next to them shows desired properties like 'No Dying Neurons!', 'Fast!', 'Smooth Gradients!'.">
      </div>
      <p>Well, even ReLU isn't perfect (remember the 'dying ReLU' issue?). The ongoing research into new activation functions is driven by goals like:</p>
      <ul>
          <li><strong>Alleviating Vanishing/Exploding Gradients:</strong> Finding functions that maintain healthy gradient flow through very deep networks.</li>
          <li><strong>Avoiding 'Dying' Neurons:</strong> Ensuring all neurons can continue to learn.</li>
          <li><strong>Better Centering of Outputs:</strong> Like Tanh, having outputs centered around zero can be beneficial.</li>
          <li><strong>Smoother Approximations:</strong> Sometimes a smoother function (with continuous derivatives) can lead to more stable training.</li>
          <li><strong>Computational Efficiency:</strong> Keeping them fast to compute is always a plus.</li>
          <li><strong>Improved Regularization:</strong> Some activation functions might inherently help prevent overfitting.</li>
      </ul>
      <div class="continue-button" onclick="showNextSection(3)">Continue</div>
  </section>

  <section id="section3">
      <h2>A Quick Gallery of Relatives and Newcomers</h2>
      <p>Let's just briefly introduce a few other names you might see. We won't go into their math in detail here, but you can always look them up using the Wikipedia reference from the slides if you're curious!</p>
      
      <div class="gallery-card">
          <h3>PReLU (Parametric ReLU)</h3>
          <div class="image-placeholder">
              <img src="/placeholder.svg?height=150&width=300" alt="Graph similar to Leaky ReLU">
          </div>
          <p><strong>Characteristic:</strong> Like Leaky ReLU, but the 'leak' slope α is a <em>learnable parameter</em> instead of being fixed!</p>
      </div>
      
      <div class="gallery-card">
          <h3>ELU (Exponential Linear Unit)</h3>
          <div class="image-placeholder">
              <img src="/placeholder.svg?height=150&width=300" alt="Graph shows it's like ReLU for positive inputs, but smoothly goes negative for negative inputs, saturating at -α">
          </div>
          <p><strong>Characteristic:</strong> Aims to make neuron activations more zero-mean and has a non-zero gradient for negative inputs, helping with dying neurons.</p>
      </div>
      
      <div class="gallery-card">
          <h3>Swish (or SiLU - Sigmoid Linear Unit)</h3>
          <div class="image-placeholder">
              <img src="/placeholder.svg?height=150&width=300" alt="Graph shows a smooth, non-monotonic bump">
          </div>
          <p><strong>Characteristic:</strong> A smooth function (x * sigmoid(βx)) that often performs well, outperforming ReLU on some tasks. β can be learnable.</p>
      </div>
      
      <div class="gallery-card">
          <h3>GELU (Gaussian Error Linear Unit)</h3>
          <div class="image-placeholder">
              <img src="/placeholder.svg?height=150&width=300" alt="Graph shows a smooth, ReLU-like shape">
          </div>
          <p><strong>Characteristic:</strong> Approximates the expected transformation of a ReLU neuron whose input is stochastic. Used in models like BERT and GPT.</p>
      </div>
      
      <div class="gallery-card">
          <h3>Softplus</h3>
          <div class="image-placeholder">
              <img src="/placeholder.svg?height=150&width=300" alt="Graph is a smooth version of ReLU">
          </div>
          <p><strong>Characteristic:</strong> log(1+e^x). It's always positive and smooth.</p>
      </div>
      
      <p>As you can see from the gallery, there's a lot of creativity!</p>
      
      <ul>
          <li><strong>PReLU (Parametric ReLU):</strong> This is like Leaky ReLU, but instead of fixing the small slope δ for negative inputs, PReLU makes this slope α a <em>parameter that the network learns</em> during training. So, each neuron can learn its own optimal 'leakiness'!</li>
          <li><strong>ELU (Exponential Linear Unit):</strong> For positive inputs, ELU acts like ReLU (φ(z)=z). For negative inputs, it becomes α(e^z - 1), which allows negative outputs and helps push the mean activation closer to zero. It also has non-zero gradients for negative inputs.</li>
          <li><strong>SELU (Scaled Exponential Linear Unit):</strong> A special version of ELU where α and another scaling factor λ are set to specific values that, under certain conditions, can make neuron activations self-normalizing (meaning they tend to preserve a mean of 0 and standard deviation of 1 through layers).</li>
          <li><strong>Swish (also known as SiLU - Sigmoid Linear Unit):</strong> Defined as φ(z) = z * sigmoid(βz). It's a smooth, non-monotonic function (it can dip down before going up) that has shown strong performance in some deep networks. The β parameter can also be learnable.</li>
          <li><strong>GELU (Gaussian Error Linear Unit):</strong> This one's a bit more complex, often involving the Gaussian cumulative distribution function. It's become popular in state-of-the-art Transformer models (like BERT and GPT) used in natural language processing.</li>
      </ul>
      
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>The existence of this diverse 'family' shows that the field is dynamic. While ReLU is a strong default, for specific problems or network architectures, one of these other functions might provide an edge in performance or training stability. Knowing they exist helps you understand research papers and advanced model designs.</p>
      </div>
      
      <div class="continue-button" onclick="showNextSection(4)">Continue</div>
  </section>

  <section id="section4">
      <h2>The Main Takeaway: Non-Linearity is Key</h2>
      <p>You definitely don't need to memorize the formulas for all these extended family members right now!</p>
      
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A large, central, glowing φ symbol (representing the concept of a non-linear activation function). Around it, smaller icons of the various function graphs (Sigmoid, Tanh, ReLU, Leaky ReLU, ELU, Swish etc.) are orbiting, like planets around a sun.">
      </div>
      
      <p>The most important things to remember about activation functions are:</p>
      
      <ol>
          <li><strong>They introduce crucial non-linearity</strong> into the network, allowing it to learn complex patterns.</li>
          <li><strong>Their derivatives are essential for training</strong> via gradient-based methods like backpropagation.</li>
          <li>There's a core set (Identity, Sigmoid, Tanh, ReLU, Leaky ReLU) that forms the foundation.</li>
          <li>Research continues to produce new variants, each aiming to improve on some aspect of existing functions.</li>
      </ol>
      
      <p>If you encounter an unfamiliar activation function in the wild (like in a research paper or a new model architecture), the best thing to do is to look up its definition and properties. The Wikipedia page we shared (from Slides 23 & 24) is an excellent starting point for this: <a href="https://en.wikipedia.org/wiki/Activation_function" target="_blank">https://en.wikipedia.org/wiki/Activation_function</a></p>
      
      <div class="test-your-knowledge">
          <h3>Test Your Knowledge</h3>
          <h4>What is a common motivation for researchers developing new activation functions beyond ReLU?</h4>
          <div class="option" onclick="selectOption(this, false)">
              <p>To make neural networks purely linear.</p>
              <div class="option-feedback incorrect-feedback">
                  Actually, the goal is usually to maintain or enhance non-linearity while addressing issues like dying neurons or improving gradient flow.
              </div>
          </div>
          <div class="option" onclick="selectOption(this, false)">
              <p>To reduce the computational cost of the max(0,z) operation.</p>
              <div class="option-feedback incorrect-feedback">
                  ReLU is already very computationally cheap. New functions might be slightly more complex but aim for other benefits.
              </div>
          </div>
          <div class="option" onclick="selectOption(this, true)">
              <p>To address limitations like the 'dying ReLU' problem or to improve training dynamics (e.g., better gradient flow, zero-centered activations).</p>
              <div class="option-feedback correct-feedback">
                  Correct! Many newer activation functions aim to overcome issues like dying neurons, ensure smoother or more stable gradients, or achieve properties like self-normalization.
              </div>
          </div>
          <div class="option" onclick="selectOption(this, false)">
              <p>To eliminate the need for a bias term in neurons.</p>
              <div class="option-feedback incorrect-feedback">
                  Activation functions and bias terms serve different (though related) roles in a neuron's computation.
              </div>
          </div>
      </div>
      
      <div class="continue-button" onclick="showNextSection(5)">Continue</div>
  </section>

  <section id="section5">
      <h2>Shifting Focus: What About Multiple Outputs?</h2>
      <p>We've spent a lot of time understanding how individual neurons 'activate' and process their net input. This is crucial for hidden layers.</p>
      
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A fork in the road. One path is labeled 'Binary Output (0/1) - Sigmoid'. The other, wider path is labeled 'Multiple Outputs (Class A, B, C...) - Next Up: Softmax!'.">
      </div>
      
      <p>But what happens at the very end of the network – the <strong>output layer</strong>? Especially when our network needs to choose between <em>more than two</em> possible categories, like deciding if an image is a cat, a dog, a bird, or a fish? We need a special way to handle this multi-way decision and turn the raw outputs of our final layer neurons into meaningful probabilities.</p>
      
      <p>That's where our next major topic comes in: <strong>Multiclass Classification</strong> and the indispensable <strong>Softmax function</strong>. Get ready to see how networks make those big decisions!</p>
  </section>

  <script>
      // Show the first section initially
      document.getElementById("section1").style.display = "block";
      document.getElementById("section1").style.opacity = "1";

      function showNextSection(nextSectionId) {
          const currentButton = event.target;
          const nextSection = document.getElementById("section" + nextSectionId);
          
          currentButton.style.display = "none";
          
          nextSection.style.display = "block";
          setTimeout(() => {
              nextSection.style.opacity = "1";
          }, 10);

          setTimeout(() => {
              nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }, 500);
      }

      function selectOption(option, isCorrect) {
          // Remove selected class from all options
          const options = document.querySelectorAll('.option');
          options.forEach(opt => {
              opt.classList.remove('selected');
              opt.querySelector('.option-feedback').style.display = 'none';
          });
          
          // Add selected class to clicked option
          option.classList.add('selected');
          
          // Show feedback
          const feedback = option.querySelector('.option-feedback');
          feedback.style.display = 'block';
      }

      function revealAnswer(id) {
          const revealText = document.getElementById(id);
          const revealButton = event.target;
          
          revealText.style.display = "block";
          revealButton.style.display = "none";
      }
  </script>
</body>
</html>