<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>The Overfitting/Underfitting Playground</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          max-width: 800px;
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          font-size: 150%;
      }
      section {
          margin-bottom: 20px;
          padding: 20px;
          background-color: #ffffff;
          display: none;
          opacity: 0;
          transition: opacity 0.5s ease-in;
      }
      h1, h2, h3, h4 {
          color: #333;
          margin-top: 20px;
      }
      p, li {
          line-height: 1.6;
          color: #444;
          margin-bottom: 20px;
      }
      ul {
          padding-left: 20px;
      }
      .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          text-align: left;
      }
      .image-placeholder img, .interactive-placeholder img {
          max-width: 100%;
          height: auto;
          border-radius: 5px;
      }
      .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          padding: 20px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .vocab-section {
          background-color: #f0f8ff;
      }
      .vocab-section h3 {
          color: #1e90ff;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .vocab-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .vocab-term {
          font-weight: bold;
          color: #1e90ff;
      }
      .why-it-matters {
          background-color: #ffe6f0;
      }
      .why-it-matters h3 {
          color: #d81b60;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .stop-and-think {
          background-color: #e6e6ff;
      }
      .stop-and-think h3 {
          color: #4b0082;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .continue-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #007bff;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .reveal-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #4b0082;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .test-your-knowledge {
          background-color: #e6ffe6; /* Light green background */
      }
      .test-your-knowledge h3 {
          color: #28a745; /* Dark green heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .test-your-knowledge h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .test-your-knowledge p {
          margin-bottom: 15px;
      }
      .check-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #28a745; /* Green background */
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
          border: none;
          font-size: 1em;
      }
      .faq-section {
          background-color: #fffbea; /* Light yellow background */
      }
      .faq-section h3 {
          color: #ffcc00; /* Bright yellow heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .faq-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .task {
          background-color: #f0f0f0;
          padding: 15px;
          border-radius: 8px;
          margin-bottom: 15px;
      }
      .task h4 {
          margin-top: 0;
          color: #333;
      }
      .option {
          margin-bottom: 10px;
          padding: 10px;
          border-radius: 5px;
          background-color: #f9f9f9;
          cursor: pointer;
      }
      .option.selected {
          background-color: #d4edda;
          border: 1px solid #28a745;
      }
      .option-feedback {
          display: none;
          margin-top: 10px;
          padding: 10px;
          background-color: #f8f9fa;
          border-radius: 5px;
      }
      .correct {
          color: #28a745;
          font-weight: bold;
      }
      .incorrect {
          color: #dc3545;
          font-weight: bold;
      }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <section id="section1">
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A cartoon character (a scientist or an AI) carefully walking a tightrope. One end of the tightrope is labeled 'Underfitting (Too Simple!)' and the other 'Overfitting (Too Complex!)'. The character is trying to stay balanced in the middle, labeled 'Good Generalization!'">
      </div>
      <h1>Experiment Lesson 24.1: The Overfitting/Underfitting Playground - Finding the Sweet Spot!</h1>
      <h2>Balancing Act: Model Complexity</h2>
      <p>Hey model explorers! In our conceptual Lesson 24, we tied together the Universal Approximation Theorem with the very real-world challenges of <strong>underfitting</strong> and <strong>overfitting</strong>. We saw that while neural networks <em>can</em> theoretically approximate very complex functions (especially if we give them enough neurons), this flexibility can be a double-edged sword.</p>
      <ul>
          <li><strong>Too simple a model (too few neurons/low capacity):</strong> It might <strong>underfit</strong>, failing to capture the true patterns in the data.</li>
          <li><strong>Too complex a model (too many neurons/high capacity) for the amount of data:</strong> It might <strong>overfit</strong>, learning the noise in the training data perfectly but failing to generalize to new, unseen data.</li>
      </ul>
      <p>Today, you get to experience this balancing act firsthand! In this <strong>Overfitting/Underfitting Playground</strong>, you'll adjust the complexity of a neural network (by changing the number of neurons in its hidden layer) and observe how it affects its ability to fit a sample dataset. Your mission: find that 'sweet spot'!</p>
      <div class="continue-button" onclick="showNextSection(2)">Continue</div>
  </section>

  <section id="section2">
      <h2>The Playground: Fitting a Noisy Sine Wave</h2>
      <p>We'll be working with a <strong>regression task</strong>. Imagine we have some data points that roughly follow a sine wave pattern, but they have some random noise added to them (just like real-world data!). Our neural network will have <strong>one hidden layer</strong> and will try to learn the underlying sine wave.</p>
      
      <div class="interactive-placeholder">
          <img src="/placeholder.svg?height=400&width=600" alt="An 'Overfitting/Underfitting Playground' widget for regression, showing a scatter plot with noisy data points following a sine wave pattern, a slider to adjust the number of hidden neurons, and displays for training and test RMSE values.">
      </div>
      
      <div class="task">
          <h4>Task 1: Witness Underfitting!</h4>
          <p>1. Set the 'Number of Hidden Neurons (N)' to a very small value (e.g., N=1 or N=2).</p>
          <p>2. Click 'Retrain Model'.</p>
          <p>3. <strong>Observe:</strong> How well does the red curve (network's fit) match the underlying grey sine wave? How well does it pass through the blue training points?</p>
          <p>4. <strong>Note:</strong> What are the Training RMSE and Test RMSE values? Are they high or low?</p>
          <p>5. <strong>Your Diagnosis:</strong> Is this underfitting or overfitting? Why?</p>
          <button class="reveal-button" onclick="revealAnswer('task1-feedback')">Reveal Feedback</button>
          <p id="task1-feedback" style="display: none;">When N is very small, the red curve will be very simple (like a step or a single bend) and won't capture the sine wave well. Both Training and Test RMSE will be high. This is classic <strong>underfitting</strong> – the model is too simple.</p>
      </div>
      
      <div class="task">
          <h4>Task 2: Finding a 'Good' Fit!</h4>
          <p>1. Gradually increase 'N' (e.g., try N=5, N=10, N=15).</p>
          <p>2. Click 'Retrain Model' each time.</p>
          <p>3. <strong>Observe:</strong> How does the red curve change? Does it start to follow the grey sine wave better?</p>
          <p>4. <strong>Note:</strong> Pay close attention to <em>both</em> Training RMSE and Test RMSE. Try to find a value of N where both are reasonably low, and the Test RMSE is not much higher than (or is even close to) the Training RMSE.</p>
          <p>5. <strong>Your Diagnosis:</strong> What value of N seems to give a good, generalizable fit?</p>
          <button class="reveal-button" onclick="revealAnswer('task2-feedback')">Reveal Feedback</button>
          <p id="task2-feedback" style="display: none;">Around N=5 to N=15 (depending on the exact data/noise), the red curve should nicely approximate the grey sine wave. Both Training and Test RMSE should be significantly lower than in Task 1. The Test RMSE might be slightly higher than Training RMSE, which is normal, but they should be close. This is a <strong>good fit</strong> – the model has learned the underlying pattern without fitting too much noise.</p>
      </div>
      
      <div class="task">
          <h4>Task 3: The Danger of Overfitting!</h4>
          <p>1. Now, crank up 'N' to a very large value (e.g., N=50 or N=100).</p>
          <p>2. Click 'Retrain Model'.</p>
          <p>3. <strong>Observe:</strong> What happens to the red curve? Does it pass very closely through the blue <em>training</em> points?</p>
          <p>4. <strong>Note:</strong> What happens to the Training RMSE? What happens to the <em>Test RMSE</em> (make sure test points are visible)?</p>
          <p>5. <strong>Your Diagnosis:</strong> Is this underfitting or overfitting? Why?</p>
          <button class="reveal-button" onclick="revealAnswer('task3-feedback')">Reveal Feedback</button>
          <p id="task3-feedback" style="display: none;">When N is very large, the red curve will likely become very 'wiggly' and try to pass through almost every single blue training point, including the noise. The Training RMSE will become extremely low (the model is 'perfect' on data it has seen). However, the Test RMSE will likely shoot up significantly! The red curve will probably deviate wildly from the underlying grey sine wave and miss the orange test points badly. This is classic <strong>overfitting</strong> – the model memorized the training noise and lost the ability to generalize.</p>
      </div>
      
      <p>Playing with this should give you a real feel for the <strong>bias-variance tradeoff</strong> in action!</p>
      <ul>
          <li><strong>Underfitting (High Bias, Low Complexity):</strong> The model makes strong (wrong) assumptions and can't even fit the training data well.</li>
          <li><strong>Overfitting (High Variance, High Complexity for the Data):</strong> The model is too sensitive to the training data's specifics (noise) and doesn't generalize to new data.</li>
          <li><strong>Good Fit (Balanced):</strong> The model captures the true underlying signal without being overly swayed by the noise.</li>
      </ul>
      
      <div class="vocab-section">
          <h3>Build Your Vocab</h3>
          <h4 class="vocab-term">Bias-Variance Tradeoff</h4>
          <p>A central problem in supervised learning. <br><strong>Bias</strong> is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss relevant relations (underfitting). <br><strong>Variance</strong> is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data (overfitting).<br>There's often a tradeoff: models with lower bias tend to have higher variance, and vice-versa. The goal is to find a good balance.</p>
      </div>
      
      <div class="continue-button" onclick="showNextSection(3)">Continue</div>
  </section>

  <section id="section3">
      <h2>Reflections on Model Complexity</h2>
      <p>This experiment visually demonstrates some key practical aspects related to the Universal Approximation Theorem:</p>
      
      <ol>
          <li><strong>Capacity is Needed:</strong> With too few neurons (low capacity), the network simply <em>cannot</em> represent the complexity of the true underlying function, even if the UAT says a solution <em>exists</em> with <em>enough</em> neurons. This is underfitting.</li>
          <li><strong>Too Much Capacity Can Be Harmful (for finite data):</strong> With a very large number of neurons (high capacity), the network <em>can</em> represent the training data almost perfectly. However, if your training dataset is finite and noisy, the network might use its high capacity to fit the noise rather than just the true signal. This leads to overfitting.</li>
          <li><strong>Finding the 'Sweet Spot':</strong> The art of machine learning often involves finding a model complexity that is 'just right' for the amount and nature of your data – complex enough to capture the true patterns, but not so complex that it overfits the noise. Techniques like regularization, early stopping, or using validation sets (which we haven't covered yet) help manage this.</li>
      </ol>
      
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="Three graphs side-by-side: 1. 'Underfitting': A few data points with a very simple line far from them. 2. 'Good Fit': Data points with a curve that nicely captures the trend. 3. 'Overfitting': Data points with an extremely wiggly line passing through every single point.">
      </div>
      
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>The UAT tells us what's <em>theoretically possible</em> in terms of representation. This experiment shows us the <em>practical challenge</em> of achieving good performance. It's not enough for a model to be <em>able</em> to represent a function; it also needs to be <em>learnable</em> from the available data and <em>generalize</em> well to new data.</p>
      </div>
      
      <div class="test-your-knowledge">
          <h3>Test Your Knowledge</h3>
          <h4>If your neural network has a very low training error but a very high test error, what is most likely happening?</h4>
          <div class="option" onclick="selectOption(this, 'option1', false)">
              <p>Underfitting</p>
              <div id="option1" class="option-feedback">
                  <p class="incorrect">Incorrect.</p>
                  <p>Underfitting usually means high error on <em>both</em> training and test sets because the model is too simple.</p>
              </div>
          </div>
          <div class="option" onclick="selectOption(this, 'option2', true)">
              <p>Overfitting</p>
              <div id="option2" class="option-feedback">
                  <p class="correct">Correct!</p>
                  <p>Low training error (model fits training data well) combined with high test error (model doesn't generalize to new data) is the classic sign of overfitting.</p>
              </div>
          </div>
          <div class="option" onclick="selectOption(this, 'option3', false)">
              <p>The Universal Approximation Theorem is incorrect.</p>
              <div id="option3" class="option-feedback">
                  <p class="incorrect">Incorrect.</p>
                  <p>The UAT is about representational capacity, not necessarily about generalization performance on finite data.</p>
              </div>
          </div>
          <div class="option" onclick="selectOption(this, 'option4', false)">
              <p>The activation functions are all linear.</p>
              <div id="option4" class="option-feedback">
                  <p class="incorrect">Incorrect.</p>
                  <p>If all activations were linear, the model would likely underfit complex data, leading to high error on both sets.</p>
              </div>
          </div>
      </div>
      
      <div class="continue-button" onclick="showNextSection(4)">Continue</div>
  </section>

  <section id="section4">
      <h2>The Broader Picture: Quotes to Ponder</h2>
      <p>Let's revisit those insightful quotes from Lesson 24 (Slide 56) now that you've had this hands-on experience:</p>
      
      <ol>
          <li><strong>Ian Goodfellow:</strong>
              <blockquote><em>"A feedforward network with a single layer is sufficient to represent any function, but the layer may be infeasibly large and may fail to learn and generalize correctly."</em></blockquote>
              <p>You saw that even for our simple sine wave, a single neuron was too simple. We needed more. And if we made it <em>too</em> large (N=100), it failed to generalize (high test error).</p>
          </li>
          <li><strong>Elon Musk:</strong>
              <blockquote><em>"Introducing non-linearity via an activation function allows us to approximate any function. It's quite simple, really."</em></blockquote>
              <p>The Sigmoid activation in our hidden layer was crucial. Without that non-linearity, even with 100 hidden neurons, our network would only be able to learn a straight line (if the output was also linear) – it would massively underfit the sine wave.</p>
          </li>
      </ol>
      
      <div class="faq-section">
          <h3>Frequently Asked Questions</h3>
          <h4>How do I know the 'right' number of hidden neurons for my problem in the real world?</h4>
          <p>That's the million-dollar question! There's no magic formula. It often involves:</p>
          <ol>
              <li><strong>Starting with rules of thumb or established architectures</strong> for similar problems.</li>
              <li><strong>Experimentation (Hyperparameter Tuning):</strong> Trying different numbers of neurons (and layers) and evaluating their performance on a separate <em>validation set</em> (not the test set, which is kept pristine until the very end).</li>
              <li><strong>Regularization Techniques:</strong> Using methods that help prevent overfitting even if the network is slightly larger than necessary (e.g., L2 regularization, dropout).</li>
              <li><strong>Considering the amount of data:</strong> More complex models generally need more data.</li>
          </ol>
          <p>It's an iterative process of designing, training, and evaluating.</p>
      </div>
      
      <div class="continue-button" onclick="showNextSection(5)">Continue</div>
  </section>

  <section id="section5">
      <h2>Experiment Concluded & Course Foundations Laid!</h2>
      <p>Fantastic work navigating the Overfitting/Underfitting Playground!</p>
      
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A character (student) confidently standing on the 'Good Fit' peak between the valleys of 'Underfitting' and 'Overfitting', holding a flag of achievement. The UAT scroll is in their backpack.">
      </div>
      
      <p>This experiment, combined with our theoretical discussions, should give you a strong intuition for the power and practicalities of neural networks. You've seen how model complexity, governed by elements like the number of hidden neurons, directly impacts learning and generalization.</p>
      
      <p><strong>This lesson also marks the end of our foundational module on the core concepts of Neural Networks and the Universal Approximation Theorem!</strong></p>
      
      <p>We've covered an incredible amount of ground, from the history of AI, the building blocks of neurons and layers, the crucial role of activation functions, the mathematics of matrix notation and Softmax, how simple networks can solve logic puzzles, how to measure network performance with loss/cost functions, and the theoretical underpinnings of their expressive power.</p>
      
      <p><strong>You now have the essential vocabulary and conceptual framework to understand:</strong></p>
      <ul>
          <li>How neural networks are structured.</li>
          <li>How they process information (the forward pass).</li>
          <li>Why they are so powerful (non-linearity, UAT).</li>
          <li>Some of the key challenges in training them (underfitting, overfitting).</li>
      </ul>
      
      <p>This is a massive achievement! The next steps in a typical deep learning journey would be to dive into how these networks actually <em>learn</em> their weights and biases – through the fascinating process of <strong>Gradient Descent</strong> and <strong>Backpropagation</strong>. You'd also explore more specialized architectures like CNNs and RNNs. But for now, give yourself a pat on the back. You've built a fantastic foundation!</p>
      
      <p>Thank you for joining this part of the course. Keep exploring, keep learning, and keep building!</p>
  </section>

  <script>
      // Show the first section initially
      document.getElementById("section1").style.display = "block";
      document.getElementById("section1").style.opacity = "1";

      function showNextSection(nextSectionId) {
          const currentButton = event.target;
          const nextSection = document.getElementById("section" + nextSectionId);
          
          currentButton.style.display = "none";
          
          nextSection.style.display = "block";
          setTimeout(() => {
              nextSection.style.opacity = "1";
          }, 10);

          setTimeout(() => {
              nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }, 500);
      }

      function revealAnswer(id) {
          const revealText = document.getElementById(id);
          const revealButton = event.target;
          
          revealText.style.display = "block";
          revealButton.style.display = "none";
      }
      
      function selectOption(element, feedbackId, isCorrect) {
          // Remove selected class from all options
          const options = document.querySelectorAll('.option');
          options.forEach(option => {
              option.classList.remove('selected');
          });
          
          // Add selected class to clicked option
          element.classList.add('selected');
          
          // Hide all feedback
          const feedbacks = document.querySelectorAll('.option-feedback');
          feedbacks.forEach(feedback => {
              feedback.style.display = 'none';
          });
          
          // Show feedback for selected option
          document.getElementById(feedbackId).style.display = 'block';
      }
  </script>
</body>
</html>