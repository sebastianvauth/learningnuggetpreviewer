<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Coding Lesson 17.1: Implementing Softmax (NumPy)</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          max-width: 800px;
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          font-size: 150%;
      }
      section {
          margin-bottom: 20px;
          padding: 20px;
          background-color: #ffffff;
          display: none;
          opacity: 0;
          transition: opacity 0.5s ease-in;
      }
      h1, h2, h3, h4 {
          color: #333;
          margin-top: 20px;
      }
      p, li {
          line-height: 1.6;
          color: #444;
          margin-bottom: 20px;
      }
      ul {
          padding-left: 20px;
      }
      .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          text-align: left;
      }
      .image-placeholder img, .interactive-placeholder img {
          max-width: 100%;
          height: auto;
          border-radius: 5px;
      }
      .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          padding: 20px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .vocab-section {
          background-color: #f0f8ff;
      }
      .vocab-section h3 {
          color: #1e90ff;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .vocab-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .vocab-term {
          font-weight: bold;
          color: #1e90ff;
      }
      .why-it-matters {
          background-color: #ffe6f0;
      }
      .why-it-matters h3 {
          color: #d81b60;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .stop-and-think {
          background-color: #e6e6ff;
      }
      .stop-and-think h3 {
          color: #4b0082;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .continue-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #007bff;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .reveal-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #4b0082;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .test-your-knowledge {
          background-color: #e6ffe6; /* Light green background */
      }
      .test-your-knowledge h3 {
          color: #28a745; /* Dark green heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .test-your-knowledge h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .test-your-knowledge p {
          margin-bottom: 15px;
      }
      .check-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #28a745; /* Green background */
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
          border: none;
          font-size: 1em;
      }
      .faq-section {
          background-color: #fffbea; /* Light yellow background */
      }
      .faq-section h3 {
          color: #ffcc00; /* Bright yellow heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .faq-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      pre {
          background-color: #f5f5f5;
          padding: 15px;
          border-radius: 5px;
          overflow-x: auto;
          font-size: 0.9em;
          line-height: 1.4;
      }
      .solution {
          border-left: 4px solid #28a745;
          padding-left: 15px;
          margin-top: 10px;
          margin-bottom: 20px;
          display: none;
      }
      .solution-title {
          color: #28a745;
          font-weight: bold;
          margin-bottom: 5px;
      }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <section id="section1">
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A Python snake character cheerfully typing on a laptop. The laptop screen shows the Softmax formula s_j = exp(z_j) / Î£exp(z_k) being translated into NumPy code. NumPy logo is visible.">
      </div>
      <h1>Coding Lesson 17.1: Implementing Softmax (NumPy) - Code the Probabilities!</h1>
      <p>Hey Pythonistas! In our last exercise lesson, you got your hands dirty calculating Softmax probabilities manually. You saw the exponentiation, the summation, and the normalization. It's a powerful function for turning raw scores (logits) into a nice probability distribution for multiclass classification.</p>
      <p>Now, it's time to make our computers do the heavy lifting! In this coding lesson, we're going to implement the <strong>Softmax function</strong> using Python and the indispensable <strong>NumPy</strong> library. This will not only solidify your understanding but also give you a reusable piece of code that's a core component in many neural network implementations. Fire up your Jupyter Notebook or Google Colab, and let's get coding!</p>
      <div class="continue-button" onclick="showNextSection(2)">Continue</div>
  </section>

  <section id="section2">
      <h2>The Softmax Recap: Our Coding Blueprint</h2>
      <p>Let's quickly recall the formula we're aiming to implement. For a vector of K logits $$z = [z_1, z_2, ..., z_K]$$, the Softmax probability $$s_j$$ for class $$j$$ is:</p>
      <p>\[ s_j = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}} \]</p>
      <p>Our goal is to write a Python function that takes a NumPy array of logits $$z$$ as input and returns a NumPy array of probabilities $$s$$.</p>
      <div class="vocab-section">
          <h3>Build Your Vocab</h3>
          <h4 class="vocab-term">Vectorization (in NumPy)</h4>
          <p>The process of performing operations on entire arrays (vectors or matrices) at once, rather than element by element using explicit loops. NumPy is highly optimized for vectorized operations, making code cleaner, more readable, and much, much faster than using Python loops for numerical computations.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(3)">Continue</div>
  </section>

  <section id="section3">
      <h2>Setting Up: Your NumPy Playground</h2>
      <p>First, let's import NumPy and define an example vector of logits that we can test our function with.</p>
      <pre># Import NumPy
import numpy as np

# Example vector of logits (raw scores)
# Let's say these are for 4 classes
z_vector = np.array([2.0, 1.0, 0.1, 0.5])

print("Our example logits (z_vector):\n", z_vector)</pre>
      <p>Run this in your notebook. This $$z\_vector$$ will be the input to our Softmax function.</p>
      <div class="stop-and-think">
          <h3>Stop and Think</h3>
          <h4>If $$z\_vector$$ has 4 elements, how many elements will the output probability vector $$s\_vector$$ have? What should be the sum of all elements in $$s\_vector$$?</h4>
          <button class="reveal-button" onclick="revealAnswer('stop-and-think-1')">Reveal</button>
          <p id="stop-and-think-1" style="display: none;">The output $$s\_vector$$ will also have 4 elements, one probability for each class. The sum of all elements in $$s\_vector$$ should be very close to 1.0 (allowing for tiny floating-point inaccuracies).</p>
      </div>
      <div class="continue-button" onclick="showNextSection(4)">Continue</div>
  </section>

  <section id="section4">
      <h2>Task 1: Exponentiate the Logits</h2>
      <p>The first step in the Softmax calculation is to exponentiate each logit in $$z\_vector$$. NumPy has a handy function for this: $$np.exp()$$.</p>
      <pre># Task 1: Calculate e^z for each element in z_vector

# YOUR CODE HERE for exp_z:
exp_z = np.exp(z_vector)
# ------------

print("Exponentiated logits (exp_z):\n", exp_z)</pre>
      <p>Add your line of code. When you print $$exp\_z$$, you should see that each element of $$z\_vector$$ has been exponentiated.</p>
      <p><strong>Example values (approximate):</strong><br>
      $$e^{2.0} \approx 7.389$$<br>
      $$e^{1.0} \approx 2.718$$<br>
      $$e^{0.1} \approx 1.105$$<br>
      $$e^{0.5} \approx 1.649$$</p>
      <p>So $$exp\_z$$ should be $$[7.389, 2.718, 1.105, 1.649]$$ (approximately).</p>
      <button class="reveal-button" onclick="revealSolution('solution-1')">Show Solution</button>
      <div id="solution-1" class="solution">
          <div class="solution-title">Solution for Task 1</div>
          <pre>exp_z = np.exp(z_vector)</pre>
      </div>
      <div class="continue-button" onclick="showNextSection(5)">Continue</div>
  </section>

  <section id="section5">
      <h2>Task 2: Sum the Exponentiated Values</h2>
      <p>Next, we need to sum up all the elements in our $$exp\_z$$ vector. This will be our normalization constant. NumPy's $$np.sum()$$ function is perfect for this.</p>
      <pre># Task 2: Calculate the sum of all elements in exp_z

# YOUR CODE HERE for sum_exp_z:
sum_exp_z = np.sum(exp_z)
# ------------

print("Sum of exponentiated logits (sum_exp_z):\n", sum_exp_z)</pre>
      <p>Implement this. For our example $$exp\_z$$, the sum $$sum\_exp\_z$$ should be approximately $$7.389 + 2.718 + 1.105 + 1.649 = 12.861$$.</p>
      <button class="reveal-button" onclick="revealSolution('solution-2')">Show Solution</button>
      <div id="solution-2" class="solution">
          <div class="solution-title">Solution for Task 2</div>
          <pre>sum_exp_z = np.sum(exp_z)</pre>
      </div>
      <div class="continue-button" onclick="showNextSection(6)">Continue</div>
  </section>

  <section id="section6">
      <h2>Task 3: Calculate the Probabilities</h2>
      <p>Finally, to get our Softmax probabilities, we divide each element in $$exp\_z$$ by $$sum\_exp\_z$$. Thanks to NumPy's broadcasting, this is a very simple operation!</p>
      <pre># Task 3: Divide each element of exp_z by sum_exp_z to get the probabilities

# YOUR CODE HERE for probabilities_s:
probabilities_s = exp_z / sum_exp_z
# ------------

print("Softmax probabilities (s):\n", probabilities_s)
print("Sum of probabilities:", np.sum(probabilities_s)) # Should be very close to 1!</pre>
      <p>After running this, $$probabilities\_s$$ will hold your Softmax output! Check that the sum of these probabilities is indeed very close to 1.0.</p>
      <p><strong>Expected Probabilities (approximate, for our example):</strong><br>
      $$s_1 = 7.389 / 12.861 \approx 0.574$$<br>
      $$s_2 = 2.718 / 12.861 \approx 0.211$$<br>
      $$s_3 = 1.105 / 12.861 \approx 0.086$$<br>
      $$s_4 = 1.649 / 12.861 \approx 0.128$$</p>
      <p>(Sum $$\approx 0.574 + 0.211 + 0.086 + 0.128 = 0.999$$. Close to 1!)</p>
      <button class="reveal-button" onclick="revealSolution('solution-3')">Show Solution</button>
      <div id="solution-3" class="solution">
          <div class="solution-title">Solution for Task 3</div>
          <pre>probabilities_s = exp_z / sum_exp_z</pre>
      </div>
      <div class="continue-button" onclick="showNextSection(7)">Continue</div>
  </section>

  <section id="section7">
      <h2>Bonus: Numerical Stability - The Max Trick!</h2>
      <p>There's a potential issue with our current Softmax implementation. If any of our logits $$z_j$$ are very large, $$exp(z_j)$$ can become astronomically huge, potentially causing an <strong>overflow error</strong> (a number too large for the computer to store).</p>
      <p>Luckily, there's a neat mathematical trick! Remember we said Softmax is 'shift invariant'? If we subtract the <em>same constant</em> $$C$$ from <em>all</em> logits $$z_j$$ before exponentiating, the final Softmax probabilities remain unchanged:</p>
      <p>\[ s_j = \frac{e^{z_j - C}}{\sum_{k=1}^{K} e^{z_k - C}} \]</p>
      <p>A common choice for $$C$$ is $$max(z\_vector)$$, the largest value in our logit vector. Subtracting the max ensures that the largest exponentiated term will be $$exp(0) = 1$$, and all other terms will be $$exp(negative\_number)$$, which are small. This prevents overflow!</p>
      <pre># Bonus: Implement numerically stable Softmax

def numerically_stable_softmax(z_vec):
    # YOUR CODE HERE: Subtract the maximum logit for numerical stability
    shifted_z = z_vec - np.max(z_vec)
    # ----
    
    exp_shifted_z = np.exp(shifted_z)
    sum_exp_shifted_z = np.sum(exp_shifted_z)
    stable_probabilities = exp_shifted_z / sum_exp_shifted_z
    return stable_probabilities

# Test with our original z_vector
stable_s = numerically_stable_softmax(z_vector)
print("Numerically stable Softmax probabilities (s):\n", stable_s)
print("Sum of stable probabilities:", np.sum(stable_s))

# Test with large logits that might cause overflow otherwise
large_z_vector = np.array([1000.0, 1001.0, 999.0])
# np.exp(large_z_vector) # This would likely overflow or give inf/nan
stable_large_s = numerically_stable_softmax(large_z_vector)
print("\nStable Softmax with large logits:\n", stable_large_s)
print("Sum:", np.sum(stable_large_s))</pre>
      <p>You should find that $$stable\_s$$ for our original $$z\_vector$$ is identical (or extremely close, due to floating point precision) to $$probabilities\_s$$ you calculated earlier. And the $$numerically\_stable\_softmax$$ function should handle $$large\_z\_vector$$ without errors, whereas a direct $$np.exp(large\_z\_vector)$$ might have issues.</p>
      <button class="reveal-button" onclick="revealSolution('solution-4')">Show Solution</button>
      <div id="solution-4" class="solution">
          <div class="solution-title">Solution for Bonus Task (shifted_z part)</div>
          <pre>shifted_z = z_vec - np.max(z_vec)</pre>
      </div>
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>Numerical stability is a big deal in machine learning. Operations can involve very large or very small numbers, and if we're not careful, our programs can crash due to overflow (too large) or underflow (too small, becoming zero and losing precision). Tricks like subtracting the max in Softmax are standard practice to make implementations robust.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(8)">Continue</div>
  </section>

  <section id="section8">
      <h2>Final Task: Wrap it in a Function</h2>
      <p>Now, let's put our best (numerically stable) implementation into a reusable function.</p>
      <pre># Final Task: Create a complete softmax function

def softmax(z):
    """
    Computes softmax probabilities for each set of scores in z.
    z can be a 1D array of scores, or a 2D array where each column is a set of scores.
    """
    # For 2D array (multiple examples, each column is one example's logits)
    # we want to apply the max trick column-wise (axis=0 if features are rows)
    # If z is 1D, np.max(z, axis=0) is just np.max(z)
    # However, if z is (K features, M examples), max should be taken over features for each example.
    # So, if z is KxM, max(z, axis=0) gives 1xM maxes. This is what we want to subtract.    
    # For a single vector (Kx1 or 1xK), this works fine.
    
    # Ensure z is at least 1D for np.max when axis is not specified for simple 1D array
    # z_eff = np.atleast_1d(z)
    # shifted_z = z_eff - np.max(z_eff) # Simple max for 1D case
    
    # More general approach for 1D or 2D (where columns are examples):
    # If z is (num_classes, num_examples), then max should be taken along axis 0.
    # If z is (num_examples, num_classes), then max should be taken along axis 1.
    # Let's assume z is (num_classes, num_examples) or 1D (num_classes)
    if z.ndim == 1:
        shifted_z = z - np.max(z)
        exp_shifted_z = np.exp(shifted_z)
        s = exp_shifted_z / np.sum(exp_shifted_z)
    elif z.ndim == 2:
        # Assuming columns are examples, features/classes are rows
        # Example: z = np.array([[1,2,3],[4,5,6]]) # 2 classes, 3 examples
        # max_per_column = np.max(z, axis=0, keepdims=True) # shape (1, num_examples)
        # shifted_z = z - max_per_column
        # exp_shifted_z = np.exp(shifted_z)
        # s = exp_shifted_z / np.sum(exp_shifted_z, axis=0, keepdims=True)
        
        # Let's stick to the 1D case for simplicity based on current lesson scope
        # and ensure students test with a 1D array first.
        # The prompt earlier used z_vector which was 1D.
        # For a more general function, handling axes carefully is key.
        # For this lesson, let's assume z is a 1D array of logits for ONE example.
        # If you want to generalize, you'd need to specify axis for max and sum.
        
        # Simplest for 1D array (matching earlier examples):
        # YOUR CODE HERE, using the numerically stable approach
        shifted_z = z - np.max(z) # Subtract max for stability
        exp_values = np.exp(shifted_z)
        probabilities = exp_values / np.sum(exp_values)
        s = probabilities
        # -----
    else:
        raise ValueError("Input z must be 1D or 2D array")
        
    return s

# Test your function
test_logits_1d = np.array([2.0, 1.0, 0.1])
probabilities_1d = softmax(test_logits_1d)
print("Softmax for 1D input:\n", probabilities_1d)
print("Sum:", np.sum(probabilities_1d))

# Example for instructor: How to handle 2D array (M examples, K classes per example)
# if z.ndim == 2: # z is (K,M) - K classes, M examples
#    shifted_z = z - np.max(z, axis=0, keepdims=True)
#    exp_shifted_z = np.exp(shifted_z)
#    s = exp_shifted_z / np.sum(exp_shifted_z, axis=0, keepdims=True)
# test_logits_2d = np.array([[2.0, 3.0],[1.0, 0.5],[0.1, -1.0]]) # 3 classes, 2 examples
# print("\nSoftmax for 2D input (columns are examples):\n", softmax(test_logits_2d))</pre>
      <p>Now you have a robust $$softmax()$$ function you can use! The way it's written above is primarily for a 1D array of logits (for one example). Generalizing it to handle a 2D array (where each column might be an example, and rows are classes) requires careful use of $$axis$$ arguments in $$np.max$$ and $$np.sum$$ to operate column-wise. For now, master the 1D case!</p>
      <button class="reveal-button" onclick="revealSolution('solution-5')">Show Solution</button>
      <div id="solution-5" class="solution">
          <div class="solution-title">Solution for Final Task (1D case)</div>
          <pre>def softmax(z):
    # Ensure z is a numpy array for operations
    # z_arr = np.array(z)
    # Numerically stable softmax for 1D array
    shifted_z = z - np.max(z) # Subtract max logit for numerical stability
    exp_values = np.exp(shifted_z)
    probabilities = exp_values / np.sum(exp_values)
    return probabilities</pre>
      </div>
      <div class="continue-button" onclick="showNextSection(9)">Continue</div>
  </section>

  <section id="section9">
      <h2>Softmax Coder Extraordinaire!</h2>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A character receiving a 'NumPy Ninja' badge with a Softmax graph emblem on it. Python code for Softmax is in the background.">
      </div>
      <p>Excellent! You've successfully implemented the Softmax function in Python using NumPy, even making it numerically stable.</p>
      <p>You now know how to:</p>
      <ul>
          <li>Translate the Softmax mathematical formula into code.</li>
          <li>Use NumPy functions like $$np.exp()$$, $$np.sum()$$, and $$np.max()$$ effectively.</li>
          <li>Implement a common numerical stability trick (subtracting the max logit).</li>
          <li>Wrap your logic into a reusable Python function.</li>
      </ul>
      <p>This is a really important function in the deep learning toolkit, especially for multiclass classification problems.</p>
      <div class="faq-section">
          <h3>Frequently Asked Questions</h3>
          <h4>Why is the $$np.max(z)$$ subtraction trick for numerical stability so important for Softmax?</h4>
          <p>The exponential function $$e^x$$ grows extremely rapidly. If you have a logit $$z$$ that's even moderately large (e.g., 100, which can happen during training), $$e^{100}$$ is an enormous number (around $$2.68 \times 10^{43}$$). If $$z$$ is, say, 1000, $$e^{1000}$$ is too large for standard floating-point numbers to represent, leading to an 'overflow' error (often represented as $$inf$$).</p>
          <p>By subtracting $$max(z)$$ from all $$z_j$$ <em>before</em> exponentiating, the largest new logit becomes $$max(z) - max(z) = 0$$. So the largest term you'll exponentiate is $$e^0 = 1$$. All other terms will be $$e^{negative\_number}$$, which are between 0 and 1. This keeps all the numbers in $$exp(z\_shifted)$$ in a manageable range, preventing overflow while giving the exact same final probabilities due to the $$e^C$$ factor canceling out.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(10)">Continue</div>
  </section>

  <section id="section10">
      <h2>What's Next in Our Neural Adventure?</h2>
      <p>You're building up a fantastic practical and theoretical understanding of neural network components!</p>
      <p>We've looked at how individual neurons process information and how Softmax handles multiclass outputs. Next, we're going to take a slight detour to see how even very simple networks, composed of just a few neurons, can be used to model fundamental <strong>logical operations</strong> like AND, OR, and the challenging XOR. This will give us more intuition about what neural networks can <em>represent</em> and the importance of non-linearity. Get ready for some logic puzzles, AI style!</p>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A path leading from the 'Softmax Code' area towards a new area with glowing AND, OR, XOR logic gate symbols. Caption: 'Next Up: Logic Gates & The XOR Puzzle!'">
      </div>
  </section>

  <script>
      // Show the first section initially
      document.getElementById("section1").style.display = "block";
      document.getElementById("section1").style.opacity = "1";

      function showNextSection(nextSectionId) {
          const currentButton = event.target;
          const nextSection = document.getElementById("section" + nextSectionId);
          
          currentButton.style.display = "none";
          
          nextSection.style.display = "block";
          setTimeout(() => {
              nextSection.style.opacity = "1";
          }, 10);

          setTimeout(() => {
              nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }, 500);
      }

      function revealAnswer(id) {
          const revealText = document.getElementById(id);
          const revealButton = event.target;
          
          revealText.style.display = "block";
          revealButton.style.display = "none";
      }

      function revealSolution(id) {
          const solution = document.getElementById(id);
          const revealButton = event.target;
          
          solution.style.display = "block";
          revealButton.style.display = "none";
      }
  </script>
</body>
</html>