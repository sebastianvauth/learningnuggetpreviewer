<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Beyond ReLU: Leaky ReLU and The Importance of Derivatives</title>
  <style>
      body {
          font-family: Arial, sans-serif;
          max-width: 800px;
          margin: 0 auto;
          padding: 20px;
          background-color: #ffffff;
          font-size: 150%;
      }
      section {
          margin-bottom: 20px;
          padding: 20px;
          background-color: #ffffff;
          display: none;
          opacity: 0;
          transition: opacity 0.5s ease-in;
      }
      h1, h2, h3, h4 {
          color: #333;
          margin-top: 20px;
      }
      p, li {
          line-height: 1.6;
          color: #444;
          margin-bottom: 20px;
      }
      ul {
          padding-left: 20px;
      }
      .image-placeholder, .interactive-placeholder, .continue-button, .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          text-align: left;
      }
      .image-placeholder img, .interactive-placeholder img {
          max-width: 100%;
          height: auto;
          border-radius: 5px;
      }
      .vocab-section, .why-it-matters, .test-your-knowledge, .faq-section, .stop-and-think {
          padding: 20px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .vocab-section {
          background-color: #f0f8ff;
      }
      .vocab-section h3 {
          color: #1e90ff;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .vocab-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .vocab-term {
          font-weight: bold;
          color: #1e90ff;
      }
      .why-it-matters {
          background-color: #ffe6f0;
      }
      .why-it-matters h3 {
          color: #d81b60;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .stop-and-think {
          background-color: #e6e6ff;
      }
      .stop-and-think h3 {
          color: #4b0082;
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .continue-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #007bff;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .reveal-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #4b0082;
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
      }
      .test-your-knowledge {
          background-color: #e6ffe6; /* Light green background */
      }
      .test-your-knowledge h3 {
          color: #28a745; /* Dark green heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .test-your-knowledge h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .test-your-knowledge p {
          margin-bottom: 15px;
      }
      .check-button {
          display: inline-block;
          padding: 10px 20px;
          margin-top: 15px;
          color: #ffffff;
          background-color: #28a745; /* Green background */
          border-radius: 5px;
          text-decoration: none;
          cursor: pointer;
          border: none;
          font-size: 1em;
      }
      .faq-section {
          background-color: #fffbea; /* Light yellow background */
      }
      .faq-section h3 {
          color: #ffcc00; /* Bright yellow heading */
          font-size: 0.75em;
          margin-bottom: 5px;
          margin-top: 5px;
      }
      .faq-section h4 {
          color: #000;
          font-size: 0.9em;
          margin-top: 10px;
          margin-bottom: 8px;
      }
      .option {
          margin-bottom: 10px;
          padding: 10px;
          border: 1px solid #ddd;
          border-radius: 5px;
          cursor: pointer;
      }
      .option:hover {
          background-color: #f5f5f5;
      }
      .option.selected {
          background-color: #e6ffe6;
          border-color: #28a745;
      }
      .option-feedback {
          display: none;
          margin-top: 5px;
          padding: 10px;
          border-radius: 5px;
      }
      .option-feedback.correct {
          background-color: #d4edda;
          color: #155724;
      }
      .option-feedback.incorrect {
          background-color: #f8d7da;
          color: #721c24;
      }
      .mathematical-steps {
          background-color: #f8f9fa;
          padding: 15px;
          border-radius: 8px;
          margin-top: 20px;
      }
      .mathematical-steps h3 {
          color: #333;
          font-size: 1em;
          margin-bottom: 15px;
      }
      .step {
          margin-bottom: 15px;
          padding-bottom: 15px;
          border-bottom: 1px solid #eee;
      }
      .step:last-child {
          border-bottom: none;
      }
      .step-title {
          font-weight: bold;
          margin-bottom: 5px;
      }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <section id="section1">
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A ReLU neuron is shown looking a bit 'stuck' or 'asleep' on the negative side of its input. Another neuron, labeled 'Leaky ReLU', has a tiny, persistent 'drip' or 'leak' of activation even on its negative side, looking more alert.">
      </div>
      <h1>Lesson 13: Beyond ReLU: Leaky ReLU and The Importance of Derivatives</h1>
      <h2>Waking Up 'Dying' Neurons</h2>
      <p>Hey everyone! In our last visit to the Activation Function Zoo, we met the popular ReLU. We celebrated its speed and how it helps with vanishing gradients for positive inputs. But we also encountered its little quirk: the <strong>'Dying ReLU' problem</strong>, where neurons can get stuck outputting zero if their inputs are always negative, effectively stopping them from learning.</p>
      <p>Today, we're going to look at a clever modification designed to address this: <strong>Leaky ReLU</strong>. And then, we're going to shift gears and talk about something absolutely fundamental to how neural networks learn: the <strong>derivatives</strong> of these activation functions. It's time to connect activation functions to the learning process itself! (This lesson covers content from Slides 21 & 22).</p>
      <div class="continue-button" onclick="showNextSection(2)">Continue</div>
  </section>

  <section id="section2">
      <h2>Introducing Leaky ReLU: A Small Fix, A Big Difference</h2>
      <p>So, how do we stop our ReLU neurons from potentially 'dying' if they only ever see negative net inputs $$z$$? The idea behind Leaky ReLU is elegantly simple. (Referencing Slide 21).</p>
      <div class="interactive-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A side-by-side comparison of two graphs: 1. Left Graph: ReLU Function. Shows the familiar max(0,z) shape (flat at 0 for z<0, then z for z>0). 2. Right Graph: Leaky ReLU Function. Shows a very similar shape, but for z<0, instead of being flat at φ(z)=0, it has a slight positive slope. This slope is labeled δ (delta).">
      </div>
      <p>Instead of outputting a hard zero when the net input $$z$$ is negative, Leaky ReLU allows a small, non-zero, positive <strong>slope</strong> for negative inputs.</p>
      <p><strong>Equation for Leaky ReLU:</strong><br>
      $$\phi(z) = z$$ if $$z > 0$$<br>
      $$\phi(z) = \delta z$$ if $$z \leq 0$$</p>
      <p>Here, $$\delta$$ (delta) is a small positive constant, typically a value like 0.01 or 0.001. The 'forr $$\delta > 0$$ (klein)' on the slide just means 'for $$\delta > 0$$ (small)'.</p>
      <p><strong>What does this 'leak' achieve?</strong></p>
      <ul>
          <li><strong>Solves (or Mitigates) Dying ReLU:</strong> Because there's now a small, <em>non-zero</em> slope ($$\delta$$) even for negative inputs, the derivative for negative inputs will also be non-zero ($$\delta$$ instead of 0). This means that even if a neuron consistently receives negative net inputs, its weights can still be updated (albeit by a smaller amount). The neuron is less likely to get completely stuck and 'die'.</li>
          <li><strong>Allows Some Information Flow:</strong> A small amount of information (gradient) can still flow through the neuron even if its input is negative.</li>
          <li><strong>Range:</strong> The output range of Leaky ReLU is $$(-\infty, +\infty)$$, just like the Identity function, although for practical purposes with small $$\delta$$, the negative outputs will be small.</li>
      </ul>
      <div class="vocab-section">
          <h3>Build Your Vocab</h3>
          <h4 class="vocab-term">Hyperparameter</h4>
          <p>A parameter whose value is used to control the learning process. It is set before the learning process begins (unlike model parameters like weights and biases, which are learned). In Leaky ReLU, $$\delta$$ is a hyperparameter. Other examples include learning rate, number of hidden layers, etc.</p>
      </div>
      <div class="interactive-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="An interactive graph showing Leaky ReLU. A slider allows the student to change the input z from -5 to 5. Another slider allows them to change δ (from 0.01 to 0.3). The graph updates, and the output φ(z) is displayed numerically.">
      </div>
      <div class="continue-button" onclick="showNextSection(3)">Continue</div>
  </section>

  <section id="section3">
      <h2>Why Gradients Matter: The Key to Learning</h2>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A simple cartoon: A character is blindfolded on a hilly terrain (representing the 'cost function landscape'). They are trying to find the lowest point (minimum cost). Their only tool is a stick (representing the gradient/derivative) that tells them the slope of the ground where they are standing, so they know which way is 'downhill'.">
      </div>
      <p>Okay, we've mentioned 'gradients' and 'derivatives' quite a bit, especially when talking about vanishing gradients or Leaky ReLU keeping gradients alive. But <em>why</em> are these mathematical concepts so central to neural networks? (Referencing Slide 22 for the derivative formulas).</p>
      <p>Think of training a neural network as trying to find the best set of weights and biases that make the network's <strong>Cost Function</strong> (its overall error) as small as possible. This Cost Function can be imagined as a complex, hilly landscape. Our goal is to find the bottom of the deepest valley in this landscape.</p>
      <p>How do we navigate this landscape? We use an algorithm called <strong>Gradient Descent</strong> (or variants of it). And the 'gradient' in Gradient Descent is essentially a collection of <strong>derivatives</strong>!</p>
      <ul>
          <li>The <strong>derivative</strong> of the Cost Function with respect to a particular weight tells us how much the Cost (error) will change if we make a tiny change to that weight.</li>
          <li>More importantly, it tells us the <strong>direction</strong> in which that weight should be adjusted to <em>decrease</em> the Cost.</li>
      </ul>
      <p>This process of calculating these gradients and using them to update the weights, working backward from the output layer to the input layer, is called <strong>Backpropagation</strong>. And for backpropagation to work, it needs to calculate how changes in one layer's activations affect the next, which involves the derivatives of the activation functions themselves!</p>
      <div class="why-it-matters">
          <h3>Why It Matters</h3>
          <p>Without being able to calculate these derivatives (or if the derivatives are always zero for some neurons), the network simply can't learn effectively. The derivatives are the signals that guide the learning process, telling each weight how to adjust itself to contribute to reducing the overall error.</p>
      </div>
      <div class="continue-button" onclick="showNextSection(4)">Continue</div>
  </section>

  <section id="section4">
      <h2>The Derivatives' Gallery: A Quick Look</h2>
      <p>So, let's briefly list the derivatives for the activation functions we've spent the most time with. You don't need to memorize how to derive them from first principles for this course, but recognizing their forms is useful. (Referencing Slide 22).</p>
      <div class="mathematical-steps">
          <h3>Derivatives of Common Activation Functions (dφ/dz)</h3>
          <div class="step">
              <div class="step-title">1. Sigmoid: φ(z) = 1 / (1 + e^(-z))</div>
              <p>The derivative of the Sigmoid function can be conveniently expressed using its own output:</p>
              <p>\[\frac{d}{dz}\phi(z) = \phi(z)(1 - \phi(z))\]</p>
          </div>
          <div class="step">
              <div class="step-title">2. Tanh: φ(z) = tanh(z)</div>
              <p>Similarly, the Tanh derivative also uses its own output:</p>
              <p>\[\frac{d}{dz}\phi(z) = 1 - \phi(z)^2 = 1 - \tanh^2(z)\]</p>
          </div>
          <div class="step">
              <div class="step-title">3. ReLU: φ(z) = max(0, z)</div>
              <p>ReLU's derivative is piecewise constant:</p>
              <p>\[\frac{d}{dz}\phi(z) = \begin{cases} 1 & \text{if } z > 0 \\ 0 & \text{if } z < 0 \\ \text{undefined} & \text{if } z = 0 \end{cases}\]</p>
              <p>(The slide correctly notes: 'There is no derivative for z = 0'. In practice, when implementing backpropagation, the derivative at $$z=0$$ is usually assigned to be 0 or 1. This doesn't typically cause problems because hitting $$z=0$$ exactly is rare with floating-point numbers, and even if it happens, the assignment is consistent.)</p>
          </div>
          <div class="step">
              <div class="step-title">4. Leaky ReLU: φ(z) = z if z>0 else δz</div>
              <p>Leaky ReLU's derivative is also piecewise constant, but with the 'leak' slope:</p>
              <p>\[\frac{d}{dz}\phi(z) = \begin{cases} 1 & \text{if } z > 0 \\ \delta & \text{if } z < 0 \\ \text{undefined} & \text{if } z = 0 \end{cases}\]</p>
              <p>(Similar to ReLU, the derivative at $$z=0$$ is handled pragmatically in implementations, often by assigning $$\delta$$ or $$1$$.)</p>
          </div>
      </div>
      <div class="interactive-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A four-panel display. Each panel shows the graph of an activation function (Sigmoid, Tanh, ReLU, Leaky ReLU) on the left, and directly next to it, the graph of ITS DERIVATIVE function on the right.">
      </div>
      <div class="continue-button" onclick="showNextSection(5)">Continue</div>
  </section>

  <section id="section5">
      <h2>Vanishing Gradients Revisited</h2>
      <p>Now we can better understand the 'vanishing gradient' problem with Sigmoid and Tanh.</p>
      <div class="interactive-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A dynamic graph of the Sigmoid function φ(z) and its derivative φ(z)(1-φ(z)) plotted together (derivative on a secondary y-axis or different color).">
      </div>
      <p>When the input $$z$$ to a Sigmoid or Tanh neuron is very large (positive or negative), the neuron is said to be 'saturated'. Its output is very close to 0/1 (for Sigmoid) or -1/1 (for Tanh), and the <em>slope</em> of the activation function at that point is almost zero. Since the derivative <em>is</em> the slope, the gradient signal passed backward during training becomes tiny.</p>
      <p>If you have many such saturated neurons in a deep network, these tiny gradients get multiplied together, and the overall gradient signal reaching the early layers can become vanishingly small, effectively stopping those layers from learning. ReLU and Leaky ReLU help with this because for $$z > 0$$, their derivative is a constant 1 (or $$\delta$$ for Leaky ReLU when $$z < 0$$), which doesn't diminish.</p>
      <div class="test-your-knowledge">
          <h3>Test Your Knowledge</h3>
          <h4>What is the primary benefit of Leaky ReLU over standard ReLU regarding derivatives?</h4>
          <div class="option" onclick="selectOption(this, false)">
              <p>Leaky ReLU's derivative is always 1.</p>
              <div class="option-feedback incorrect">Not quite. Its derivative is 1 for $$z>0$$, but $$\delta$$ for $$z<0$$.</div>
          </div>
          <div class="option" onclick="selectOption(this, false)">
              <p>Leaky ReLU's derivative is defined at z=0, unlike standard ReLU.</p>
              <div class="option-feedback incorrect">Technically, both are undefined at z=0, though handled pragmatically.</div>
          </div>
          <div class="option" onclick="selectOption(this, true)">
              <p>Leaky ReLU has a non-zero derivative for negative inputs ($$z<0$$), helping to prevent 'dying' neurons.</p>
              <div class="option-feedback correct">Exactly! This small, non-zero gradient ($$\delta$$) for negative inputs allows learning to continue even if a neuron consistently receives negative net inputs.</div>
          </div>
          <div class="option" onclick="selectOption(this, false)">
              <p>Leaky ReLU's derivative is always larger than standard ReLU's derivative.</p>
              <div class="option-feedback incorrect">Not necessarily. For $$z>0$$, both are 1. For $$z<0$$, Leaky ReLU's is $$\delta$$ (small positive) while ReLU's is 0.</div>
          </div>
      </div>
      <div class="continue-button" onclick="showNextSection(6)">Continue</div>
  </section>

  <section id="section6">
      <h2>A World of Activations</h2>
      <div class="image-placeholder">
          <img src="/placeholder.svg?height=300&width=600" alt="A collage or a 'galaxy' view of many different activation function graph shapes, with the ones we studied (Sigmoid, Tanh, ReLU, Leaky ReLU) slightly more prominent. Caption: 'An Ever-Expanding Universe of Activations!'">
      </div>
      <p>While we've focused on these few, remember from our previous 'zoo' visit (Slides 23 & 24) that there's a whole universe of other activation functions out there (ELU, SELU, Swish, GELU, etc.), each with its own mathematical form, derivative, and set of trade-offs.</p>
      <p>The key takeaway is that activation functions and their derivatives are at the heart of a neural network's ability to both represent complex functions (due to non-linearity) and to <em>learn</em> from data (via gradient-based optimization).</p>
      <p>We've spent a lot of time on individual neurons and their activation functions. Next up, we're going to look at how the <em>output layer</em> of a network is specially designed when we need to classify inputs into <em>more than two</em> categories. This will introduce us to another very important function: Softmax!</p>
      <div class="faq-section">
          <h3>Frequently Asked Questions</h3>
          <h4>If the derivative of ReLU/Leaky ReLU is undefined at z=0, doesn't that break the math for backpropagation?</h4>
          <p>That's a very sharp observation! Mathematically, yes, there's a 'kink' where the derivative isn't strictly defined. However, in practice for deep learning:</p>
          <ol>
              <li>It's extremely rare for the net input $$z$$ to be <em>exactly</em> zero due to floating-point arithmetic.</li>
              <li>Even if it were, software implementations simply assign a value for the gradient at that point (commonly 0 or 1 for ReLU, or $$\delta$$ or 1 for Leaky ReLU). This is called a 'subgradient'.</li>
              <li>Empirically, this simplification works very well and doesn't hinder the learning process. The benefits of these functions far outweigh this minor theoretical discontinuity.</li>
          </ol>
      </div>
  </section>

  <script>
      // Show the first section initially
      document.getElementById("section1").style.display = "block";
      document.getElementById("section1").style.opacity = "1";

      function showNextSection(nextSectionId) {
          const currentButton = event.target;
          const nextSection = document.getElementById("section" + nextSectionId);
          
          currentButton.style.display = "none";
          
          nextSection.style.display = "block";
          setTimeout(() => {
              nextSection.style.opacity = "1";
          }, 10);

          setTimeout(() => {
              nextSection.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }, 500);
      }

      function revealAnswer(id) {
          const revealText = document.getElementById(id);
          const revealButton = event.target;
          
          revealText.style.display = "block";
          revealButton.style.display = "none";
      }

      function selectOption(option, isCorrect) {
          // Remove selected class from all options
          const options = document.querySelectorAll('.option');
          options.forEach(opt => {
              opt.classList.remove('selected');
              opt.querySelector('.option-feedback').style.display = 'none';
          });
          
          // Add selected class to clicked option
          option.classList.add('selected');
          
          // Show feedback for the selected option
          const feedback = option.querySelector('.option-feedback');
          feedback.style.display = 'block';
      }
  </script>
</body>
</html>